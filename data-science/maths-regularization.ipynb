{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régularisation\n",
    "Dans une perspective fonctionnelle où le choix du modèle se ramène à la recherche d'une fonction minimisant une fonction objectif, la régularisation correspond au fait d'intégrer une mesure de la complexité $\\Omega(f)$ de la fonction choisie à l'objectif. Ce faisant on introduit du biais dans l'espoir d'améliorer les performance en généralisation (diminution de la variance).\n",
    "\n",
    "Exemple de mesures de complexité: nombre de variables, profondeur (arbres de décision), degré du polynome (modèle polynomiaux), pour un modèle linéaire : nombre de coefficients non nuls ($\\mathcal{l}_0$), somme des valeurs absolues des coefficients ($\\mathcal{l}_1$), somme des carrés des coefficients ($\\mathcal{l}_2$), etc.\n",
    "\n",
    "Remarque : Pour le modèle linéaire, on dit souvent qu'on recherche le vecteur de paramètres $\\hat{w}$ minimisant la fonction objectif mais on recherche en fait de façon équivalente, $\\hat{f}_w \\in \\{ f: \\mathbb{R}^p \\rightarrow \\mathbb{R} \\;|\\; f(x)=w^{\\top}x \\; avec \\; w \\in \\mathbb{R}^p \\}$ \n",
    "\n",
    "Dans le cas général où on minimise sur un espace de fonction, $\\Omega(f)$ sera une mesure de régularité (*smoothness*): on ne veut pas que notre fonction varie trop vite sur son espace de définition. Dans les cas où il est équivalent de résoudre le problème dans un espace type $\\mathbb{R}^p$, la mesure de complexité va en général être une norme de vecteur dont le but sera d'imposer un *shrinkage* sur les coefficients: on impose à aux coefficients de notre solution de prendre de faibles valeurs. Certaines mesures comme par exemple la norme 1 de $\\mathbb{R}^n$ peuvent même favoriser des solutions dites *sparses* et ainsi être associée à un comportement désirable de sélection de *features*. \n",
    "\n",
    "## La régularisation vue comme un problème d'optimisation sous contraintes\n",
    "Pour un *hypothesis space* $\\mathcal{H}$ (espace de fonctions) et une mesure de complexité $\\Omega$ de $\\mathcal{H}$ dans $\\mathbb{R}^+$, régulariser c'est chercher la meilleure fonction $\\hat{f}$ de $\\mathcal{H}$ dotée d'une complexité d'au plus $r$ pour la mesure qu'on se donne, le paramètre $r$ étant un hyperparamètre choisi par *cross-validation*. Cette démarche peut se traduire par le problème d'optimisation suivant:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{min_{f \\in \\mathcal{H}}\\frac{1}{n}\\sum_{i=0}^{n}\\mathcal{l}(f(x_{i}), y_{i})} & {} \\\\ {\\text { subject to }} & {\\Omega(f) \\leq r}\\end{array}\n",
    "$$\n",
    "\n",
    "Cette forme est appelée *constrained empirical risk minimization*. On trouve toutefois les problèmes régularisés sous une autre forme appelée *penalized empirical risk minimization*: \n",
    "\n",
    "$$min_{f \\in \\mathcal{H}}\\frac{1}{n}\\sum_{i=0}^{n}\\mathcal{l}(f(x_{i}), y_{i}) + \\lambda\\Omega(f)$$\n",
    "\n",
    "Où le paramètre $\\lambda$ est un hyperparamètre choisi par cross-validation.\n",
    "\n",
    "**Ces deux formes sont en fait équivalentes.** Pour que les deux formes pénalisée et contrainte soient équivalentes, il faut que pour tout $r$, il existe un $\\lambda$ tel que les solutions des deux problèmes coincident et inversement. Ecrivons les conditions nécessaires du premier ordre pour chacun des problèmes:\n",
    "* Le problème pénalisé est un problème d'optimisation non contraint, on a donc une unique condition du premier ordre: \n",
    "    * $\\nabla_{f} \\mathcal{L}(f^{*}) = 0$\n",
    "* Pour le problème contraint, les conditions du premier ordre correspondent aux KKT: \n",
    "    * $\\nabla_{f} \\mathcal{L}(f^{*}, \\alpha^{*}) = 0$ (colinéarité des gradients à l'optimum)\n",
    "    * $\\nabla_{\\alpha} \\mathcal{L}(f^{*}, \\alpha^{*}) = 0$ (respect des contraintes - primal)\n",
    "    * $\\alpha \\geq 0$ (faisabilité du dual)\n",
    "    * $\\alpha(\\Omega(f) - r) = 0$ (*complementary slackness*)\n",
    "\n",
    "On remarque que les premières conditions de chaque cas sont équivalentes et donc résoudre un problème est équivalent à résoudre l'autre si et seulement si $\\lambda = \\alpha$. A chaque $r$ correspond donc un unique $\\lambda$ tel que les deux problèmes sont équivalents. De plus, les KKT du problème contraint précisent que $\\lambda \\geq 0$ (ce qui est rassurant) et que la solution réalisant l'optimum $f^{*}$ vérifie $\\Omega(f^{*})=r$.\n",
    "\n",
    "## Cas de la régression linéaire: régularisations LASSO et Ridge\n",
    "On se situe dans un cas où notre *loss function* $l$ est la MSE (les iso-fonction de coût présentant la forme d'une ellipse en dimension 2) et où le problème d'optimisation régularisé présenté ci-dessus peut se résoudre de façon équivalente dans $\\mathbb{R}^p$ et où les régularisation LASSO et Ridge sont associées aux mesures de complexité correspondant aux normes $\\|.\\|_1$ et $\\|.\\|_2$ de $\\mathbb{R}^p$ respectivement.\n",
    "\n",
    "Géométriquement, le jeu de paramètres $\\hat{w}$ solution du problème correspond au point où la fonction de coût vient tangenter la surface des contraintes. Ce raisonnement géométrique permet de rendre compte de différentes propriétés:\n",
    "* La \"taille\" de la surface des contraintes est contrôlée par le paramètre $r$ et donc $\\lambda$ (plus $\\lambda$ est élevé, plus le domaine de faisabilité des contraintes est restreint et donc la régularisation forte (et on comprend au passage l'idée derrière le terme de *shrinkage*).\n",
    "* La forme du domaine des contraintes dépend de la norme $q$ choisie. Noter que ce domaine n'est pas convexe pour $q \\lt 1$.\n",
    "* Pour la norme L1, le domaine des contraintes a la particularité de présenter un domaine d'admissibilité des contraintes polyédrique (convexe) avec des sommets au niveau des axes. Or si l'optimum est atteint au niveau des sommets, on en déduit que la solution optimale sera *sparse*: certains de ses coefficients seront nuls. A l'inverse, pour les normes $q \\gt 1$, les solutions ne se trouvent qu'exceptionnellement au niveau des axes, les coefficients obtenus peuvent être très faibles mais non nuls. On perd la propriété de \"sélection de *features*\" de la norme L1.\n",
    "\n",
    "Cf. The Elements of Statistical Learning, Figures 3.11 et 3.12.\n",
    "\n",
    "Remarques:\n",
    "* Dans le cas des moindres carrés, la régularisation L1 (LASSO) ne mène pas systématiquement à une *sparse solution*. Il existe des cas où la fonction de coût vient tangenter la surface des contraintes au niveau d'une surface. En fait, tout dépend de la localisation de l'optimum non contraint par rapport au domaine des contraintes et de la forme de l'ellipse. Il reste que pour une forme d'ellipse donnée, il existe de larges régions de l'espace des paramètres qui donneront lieu à une *sparse solution* pour le problème contraint si la solution au problème non contraint s'y trouve.\n",
    "* Pour $q \\leq 1$, on doit avoir des problèmes de différentiabilité des contraintes au niveau des \"sommets\" du domaine d'admissibilité. La problème d'optimisation n'est notamment pas régulier dans le cas du LASSO. Toutefois, on peut astucieusement reparamétrer le problème (avec éventuellement l'introduction de nouvelles contraintes) permettant d'aboutir à un nouveau problème présentant toutes les conditions de régularité requises. Voir notamment [ce passage de cette vidéo de Bloomberg](https://youtu.be/d6XDOS4btck?t=3110).\n",
    "\n",
    "### Sur la capacité de sélection de *features* du LASSO\n",
    "Par sa capacité à pouvoir générer des solutions *sparses*, le LASSO a *de facto* une capacité de sélection de *features*. On parle même parfois de *emmbedded feature selection method* puisque la partie sélection de *features* est incluse dans le modèle et non effectuée séparément.  \n",
    "\n",
    "Les *sparses solutions* sont particulièrement recherchées notamment pour les raisons suivantes: \n",
    "* Gains de performance: calculs plus rapides, moins de *features* à stocker, modèle à déployer moins lourds, etc.\n",
    "* Interprétabilité: Identifie les *features* importantes.\n",
    "* *Feature selection*: On peut utiliser le LASSO comme outil de sélection de *features* avant d'entrainer un modèle non linéaire plus lourd.\n",
    "\n",
    "### Sur les noms des régularisations LASSO et Ridge\n",
    "LASSO est en fait l'acronyme de *Least Absolute Shrinkage and Selection Operator* dont on déduit immédiatement qu'il s'agit de la régularisation L1 et de sa capacité de sélection de *features*.\n",
    "\n",
    "Le terme de Ridge fait par déduction référence à la régularisation L2. Le terme de Ridge semble provenir du cas d'une régression linéaire affectée de multicolinéarité parmi les *features*. Dans ce cas, la fonction de coût (MSE) n'a pas la forme d'une parabole dotée d'un unique minimum mais possède au moins une arrête (*ridge* en anglais) dont tous les points sont des minimums (pas étonnant, si deux variables sont colinéaires/apportent la même information, il peut exister une infinité de couples de paramètres permettant d'avoir la même valeur de fonction objectif: (0.4, 0.1), (0.3, 0.2), etc.). Il n'y a pas d'unicité du minimum. L'introduction du terme de régularisation L2 a pour effet de \"rendre\" à la fonction de coût une forme dotée d'un unique minimum global. L'idée se retrouve analytiquement: en cas de colinéarité, $X^{\\top}X$ n'est pas inversible mais en présence de régularisation L2, la matrice à inverser n'est plus $X^{\\top}X$ mais $X^{\\top}X + \\lambda I$ qui elle peut être inversible, inversibilité dont on déduit l'existence et l'unicité d'un vecteur de paramètres réalisant l'optimum. \n",
    "\n",
    "### LASSO et Ridge en présence de features corrélées, régularisation ElasticNet\n",
    "Il a été mis en évidence que pour un groupe de *features* fortement corrélées, c'est à dire apportant la même information: \n",
    "* La régularisation LASSO tend à ne sélectionner arbitrairement qu'une seule *feature* du groupe, ce qui peut conduire à une relative instabilité et incohérence des résultats (de petites différences dans le *training* set peuvent conduirent à une sélection de *features* différentes),\n",
    "* La régularisation Rigde va à l'inverse répartir les poids de façon relativement équilibrée entre les *features* d'un même groupe (*grouping effect*).\n",
    "\n",
    "Par exemple: Prenons l'exemple de deux features redondantes dans un modèle linéaire $w_{1}x{1} + w_{2}x{2}$, dans un modèle ne comprenant que $x_1$, le meilleur modèle est celui avec $4x_1$. Dans le cas du modèle avec variable redondante, on a donc une infinité de façons de répartir les poids $w_1$ et $w_2$ tels que $w_1 + w_2 = 4$. La régularisation LASSO va tendre à choisir de mettre arbitrairement tout le poids sur l'un des deux, alors que la régularisation Ridge va mettre un poids de $2$ sur chacune des variables.  \n",
    "\n",
    "Une façon d'obtenir le meilleur des deux mondes est d'opter pour la régularisation ElasticNet qui hybride les régularisations LASSO et Rigde. Le problème d'optimisation est alors le suivant: \n",
    "\n",
    "La régularisation ElasticNet permet de conserver une capacité de sélection de *features* du LASSO tout en étant plus robustes aux situations de *features* corrélées (il a été montré que les poids donnés à deux *features* seront d'autant plus proches que celles-ci sont corrélées et que la part accordée à la régularisation Rigde est importante) mais au prix de l'ajout d'un hyperparamètre.\n",
    "\n",
    "Remarque: Un avantage de la régularisation est aussi d'empêcher les coefficients d'exploser en présence de forte colinéarité (cf. forme contrainte du problème régularisé).\n",
    "\n",
    "### Résumé\n",
    "#### LASSO\n",
    "Avantages:\n",
    "* Capacité de sélection de *features* \n",
    "\n",
    "Inconvénients:\n",
    "* Instable en présence de *features* fortement corrélées\n",
    "* Si nombre de *features* $p > n$ le nombre d'observations, ne va sélectionner qu'au plus $n$ features.\n",
    "* Non régulier, optimisation plus complexe\n",
    "\n",
    "#### Ridge\n",
    "Avantages:\n",
    "* Optimisation aisée, voire possibilité de *closed-form solutions*\n",
    "* N'est pas impacté si $p > n$\n",
    "\n",
    "Inconvénients:\n",
    "* N'a pas de capacité de sélection de *features*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
