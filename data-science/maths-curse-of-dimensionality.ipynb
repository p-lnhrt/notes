{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Curse of dimensionality*\n",
    "## Définition générale\n",
    "Concept introduit pour la première fois par Bellman en 1961 et désigne le fait que pour un niveau d'erreur donné, le nombre d'observations nécessaire à l'estimation d'une fonction arbitraire augmente exponentiellement avec son nombre de variables (la dimensionnalité).\n",
    "\n",
    "Ce phénomène provient du fait que lorsque la dimensionnalité du problème augmente, plus la dimension de l'espace des *inputs* augmente, plus la densité des observations disponibles dans ce même espace diminue (*data sparsity*). Cette *sparsity* pose problème pour toute méthode reposant sur de la *statistical significance*.\n",
    "\n",
    "Cela explique par exemple pourquoi l'estimation de densité en haute dimension avec des données limitées est particulièrement ardu et qu'en classification par exemple, on se retrouve plus souvent à rechercher une frontière de décision qu'à chercher à estimer une densité. En haute dimension, retenir que les algorithmes *density-based* sont pénalisés.\n",
    "\n",
    "Exemple classique: les pixels d'une image. A ce sujet, il semble que les NN arrivent particulièrement bien à s'accomoder de la *curse of dimensionality* pour différentes raisons dont leur capacité à extraire et combiner des *features* de faible dimension.\n",
    "\n",
    "## *Curse of dimensionality* en *machine learning*\n",
    "On donne souvent à titre d'illustration de la *curse of dimensionality* des phénomènes étranges qui apparaissent en haute dimension et qui dans le contexte du *machine learning* viennent notamment perturber les méthodes se basant sur des scores (pour mesurer une similarité par exemple) dérivés de distances entre les observations (Ex: *nearest neighbors*):\n",
    "* En haute dimension, ce qu'on pense être un voisinage local peut en fait s'avérer très large: le plus proche voisin est en fait très \"loin\". Dans le cas des *nearest neighbors* ce phénomène contribue à induire une variance élevée (d'autant plus élevée que $k$ est faible). Cf. notamment *The Elements of Statistical Learning* p.22 et suivantes.\n",
    "* On montre que pour une observation de référence et un groupe d'observations pris au hasard, la différence entre la distance minimale et maximale de l'observation de référence à ce groupe tend vers zéro quand la dimension tend vers l'infini: en haute dimension, les scores de similarité varient très peu, tous les points peuvent de leur point de vue apparaître très semblables.\n",
    "\n",
    "Dans le contexte du *machine learning* dans lequel on cherche le plus souvent à apprendre un \"*state-of-nature*\" à l'aide d'un nombre fini d'observations. On peut reformuler la définition de Bellman donnée plus haut comme le fait que le nombre d'observations nécessaires à une même performance en généralisation augmente exponentiellement avec la dimensionnalité. Contrairement à la formulation de Bellman, il faut toutefois se souvenir qu'en *machine learning*, on ne vient pas directement approcher la fonction. On postule le plus souvent que la fonction à approcher appartient à une certaine classe de fonctions et on cherche la fonction de cette classe minimisant l'erreur sur le *training set*. Mais même en postulant une forme pour la fonction à approcher, son estimation n'échappe pas à la *curse of dimensionality*. \n",
    "\n",
    "Par exemple, supposons une distribution gaussienne pour nos observations. A nombre d'observations constants (par exemple 100), on intuite que l'estimation d'une gaussienne multivariée en 3 dimensions sera moins précise que pour une gaussienne univariée en dimension 1 (à cause de la *sparsity* des données).\n",
    "\n",
    "L'erreur de généralisation est classiquement décomposée en deux termes:\n",
    "* L'erreur due au fait d'avoir postulé que la fonction à approcher appartient à une classe (limitée) de fonctions (le biais).\n",
    "* L'erreur due au fait que l'algorithme d'entrainement minimise l'erreur uniquement sur le *training set* et non sur l'ensemble des données possibles ce qui induit une fluctuation aléatoire dépendant du choix du *training set* et de la complexité de la classe de fonctions postulée pour la fonction à approcher (la variance).\n",
    "\n",
    "La dimensionnalité du problème (le nombre de *features* $p$) intervient dans chacun des deux termes notamment via le choix de la classe de fonctions. Pour certaines fonctions d'erreurs et classes de fonctions, on peut même obtenir une expression analytique de l'erreur en fonction de $p$ et d'autres paramètres du problème (le nombre d'observations $N$, la dispersion des résidus $\\sigma$, etc.) qui permet de voir quels éléments viennent atténuer les effets de l'augmentation de la dimension sur l'erreur de généralisation. \n",
    "\n",
    "Remarques: \n",
    "* Pour une classe de fonctions donnée (ex: les modèles linéaires), la performance en généralisation de fait pas que décroître avec la dimensionnalité, elle augmente d'abord (on *underfit* de moins en moins en donnant la possibilité au modèle d'extraire plus de complexité) puis diminue sous les effets de la *curse of dimensionality* (on a plus assez d'observations pour estimer avec précision la meilleure fonction avec le nombre de variables d'entrée qu'on se donne). On désigne ce phénomène sous le nom de *Hughes phenomenon* ou de *peaking phenomena*.\n",
    "* Peut-on voir l'*overfitting* comme une manifestation de la *curse of dimensionality* ?: n'ayant pas assez d'observations pour le nombre de *features* qu'on se donne, on \"choisit\" au sein de la classe de fonction une fonction assez éloignée de la meilleure fonction possible pour cette classe de fonctions (dont on se serait rapproché si on avait eut plus de données), l'erreur commise étant alors assez largement dépendante des observations du *training set*. Cf. par exemple plus haut le problème pour les k-NN (qui reviennent à approcher notre fonction à l'aide de fonctions en escalier) des voisinages qui ne sont plus vraiment locaux.\n",
    "* Se souvenir par exemple que pour les modèles linéaires estimées par moindre carrés, l'erreur d'estimation des coefficients est inversement proportionnelle au nombre de degrés de liberté $N-p$.\n",
    "\n",
    "Au delà de l'augmentation de l'erreur de généralisation à nombre d'observations constant, citons comme autres effets défavorables pouvant apparaître avec l'augmentation de la dimensionnalité:\n",
    "* L'augmentation du temps de calcul: suivant les algorithmes et leurs implémentations, le nombre de features $p$ peut apparaître assez défavorablement dans l'expression de la complexité.\n",
    "* L'augmentation du volume nécessaire au stockage des données.\n",
    "* Plus on ajoute des *features*, plus il est probable que certaines d'entre elles soient plus ou moins redondantes (colinéarité). La qualité de l'estimation de certains algorithmes y est particulièrement sensible (ex: régression linéaire).\n",
    "* Existence d'un extrémum de la fonction de coût: plus on a de dimensions, moins il est problable qu'on ait un \"bol\" dans $\\mathbb{R}^{p}$ (Hessienne strictement positive/négative à l'extrémum).\n",
    "\n",
    "## Réduction de dimension\n",
    "On comprend dès lors l'intérêt de se restreindre au nombre de dimensions strictement nécessaire, d'où une prolifération de méthodes visant à diminuer la dimensionnalité du problème tout en conservant le maximum d'information apporté par les données (voir par exemple le lemme de Johnson-Lindenstrauss). On peut distinguer deux grandes approches: \n",
    "* Parmi toutes les *features* qu'on se donne, on va chercher le sous-ensemble de celles-ci contenant le plus d'information sans plus de transformations. A cette approche appartiennent les algorithmes de sélection de *features*. Citons par exemple les algorithmes donnant à chaque *feature* un score et ne conservant que les $k$ *features* au score le plus élevé. On trouve aussi les algorithmes *greedy* éliminant ou ajoutant récursivement des *features* au modèle suivant les valeurs prises par un critère d'information.\n",
    "* L'autre approche repose sur ce qu'on appelle la *manifold hypothesis*: pour un espace de *features* de dimension $p$, on fait l'hypothèse que les données se regroupent dans une variété de dimension très inférieure. Ex: Données  en dimension 3 se distribuant globalement suivant une surface, variété de dimension 2. Le nombre de dimensions \"utiles\" où se rassemble l'information est beaucoup plus faible. On peut voir alors cet espace de plus faible dimension comme ayant le \"vrai\" nombre de dimension. Le \"vrai\" nombre de dimensions se comprenant comme le nombre de dimensions permettant de bien \"généraliser\". Les méthodes de cette famille vont alors chercher à approcher cette variété si elle existe. La partie \"réduction de dimension\" de la PCA par exemple, fait l'hypothèse que cette variété est linéaire/est un sous espace affine de l'espace des features. D'autres méthodes non-linéaires visent à approcher au mieux les variétés \"courbées\". \n",
    "\n",
    "Remarque: La régularisation L1 est à ranger dans la première catégorie.\n",
    "\n",
    "TODO: Supervised vs unsupervised dimension reduction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
