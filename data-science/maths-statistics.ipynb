{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of content \n",
    "\n",
    "### Convergences et théorèmes de convergence\n",
    "* Convergence preque sûre\n",
    "* Convergence en probabilité\n",
    "* Convergence en loi (inclut théorème centrale-limite)\n",
    "* Convergence en moyenne d'ordre $q$\n",
    "* Liens entre les différents types de convergence\n",
    "* Loi des grands nombres\n",
    "\n",
    "### Test de Student \n",
    "* Importance de la loi de Student en statistiques \n",
    "* Exemple : Test d’espérance pour un échantillon gaussien\n",
    "* Exemple : Test d’égalité des moyennes de deux échantillons supposés gaussiens \n",
    "* Exemple : Test de significativité des coefficients estimés d’une régression linéaire\n",
    "\n",
    "### Tests du $\\chi^{2}$\n",
    "* Exemple : Test de variance pour un échantillon gaussien\n",
    "* Tests modélisant le problème comme l'étude de la population d'intervalles disjoints\n",
    "* Exemple : Test d’ajustement (à une loi) appelé aussi test d’adéquation\n",
    "* Exemple : Test d’homogénéité \n",
    "* Exemple : Test d’indépendance\n",
    "\n",
    "### Test de Fisher \n",
    "* Exemple : Test d’égalité des variances de deux échantillons supposés gaussiens\n",
    "* Exemple : Test de significativité d’une régression linéaire \n",
    "* Exemple : ANOVA à 1 facteur - Test d’égalité des moyennes pour $k$ échantillons gaussiens\n",
    "\n",
    "### Sur le théorème de Cochran et deux de ses applications classiques\n",
    "* Théorème de Cochran \n",
    "* Application au problème de la régression linéaire\n",
    "* Application aux lois suivies par les estimateurs $\\overline{X}$ et $S^{2}$ dans le cas gaussien\n",
    "\n",
    "### Estimation\n",
    "* Rappels de base de la théorie de l'estimation\n",
    "* Moyenne et fréquence empirique\n",
    "* Variance empirique \n",
    "\n",
    "### Variance, covariance, matrice de covariance, d'auto-covariance, de cross-covariance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergences et théorèmes de convergence\n",
    "\n",
    "## Convergence presque sûre\n",
    "Soit $X_1,X_2,\\dots,X_n$ une suite de variables aléatoires, on définit sa convergence presque sûre vers une variable aléatoire $X$ comme suit:\n",
    "\n",
    "$$X_n \\xrightarrow[n \\to +\\infty]{\\text{c.p.s.}}X \\Leftrightarrow \\mathbb{P}(\\lim\\limits_{n \\to +\\infty}X_n=X)=1$$\n",
    "\n",
    "## Convergence en probabilité\n",
    "Soit $X_1,X_2,\\dots,X_n$ une suite de variables aléatoires, on définit sa convergence en probabilité vers une variable aléatoire $X$ comme suit:\n",
    "\n",
    "$$X_n \\xrightarrow[n \\to +\\infty]{\\text{c.p.}}X \\Leftrightarrow \\forall\\epsilon>0 \\lim\\limits_{n \\to +\\infty}\\mathbb{P}(|X_n-X|\\geq\\epsilon)=0$$\n",
    "\n",
    "## Convergence en loi\n",
    "Soit $X_1,X_2,\\dots,X_n$ une suite de variables aléatoires de fonctions de répartition $F_1,F_2,\\dots,F_n$, on définit sa convergence en loi vers une variable aléatoire $X$ de fonction de répartition $F$ comme suit:\n",
    "\n",
    "$$X_n \\xrightarrow[n \\to +\\infty]{\\text{c.v.l}}X \\Leftrightarrow \\lim\\limits_{n \\to +\\infty}F_n(a)=F(a)$$\n",
    "\n",
    "en tout point $a$ où $F$ est continue.\n",
    "\n",
    "### Théorème centrale-limite\n",
    "L'exemple le plus connu de convergence en loi est sans doute le théorème centrale-limite.\n",
    "\n",
    "Soit $X_1,X_2,\\dots,X_n$ une suite de variables aléatoires i.i.d. d'espérance $\\mu$ et de variance $\\sigma$ finie (admettant un moment d'ordre 2) et $\\bar{X}_n=\\frac{1}{n}\\sum_{i}X_i$, on a alors:\n",
    "\n",
    "$$\\frac{\\bar{X}_n-\\mu}{\\sigma/\\sqrt{n}}\\xrightarrow[n \\to +\\infty]{\\text{c.v.l}}\\mathcal{N}(0,1)$$\n",
    "\n",
    "### Convergence en loi de la loi de Student\n",
    "Un autre exemple connu de la convergence en loi est celui de la convergence vers une $\\mathcal{N}(0,1)$ d'une loi de Student à $k$ degrés de liberté quand $k \\rightarrow +\\infty$.\n",
    "\n",
    "## Convergence en moyenne d'ordre $q$\n",
    "Soit $X_1,X_2,\\dots,X_n$ une suite de variables aléatoires, on définit sa convergence en moyenne quadratique d'ordre $q>0$ vers une variable aléatoire $X$ comme suit:\n",
    "\n",
    "$$X_n \\xrightarrow[n \\to +\\infty]{\\text{c.v.m.q}}X \\Leftrightarrow \\mathbb{E}(|X_n|^q)<+\\infty \\:et\\: \\lim\\limits_{n \\to +\\infty}\\mathbb{E}(|X_n-X|^q)=0$$\n",
    "\n",
    "Remarques:\n",
    "* La première condition traduit l'exigence que $X_n$ soit intégrable d'ordre $q$.\n",
    "* Le cas $q=2$ correspond au cas de la convergence en moyenne quadratique.\n",
    "* Si $s<r$, alors $CVM_r \\Rightarrow CVM_s$\n",
    "* La vitesse de convergence d'autant plus élevée que $r$ est élevé.\n",
    "\n",
    "## Liens entre les différents types de convergence\n",
    "* $CVPS \\Rightarrow CVP \\Rightarrow CVL$\n",
    "* $CVM_r \\Rightarrow CVP$\n",
    "* Si $s<r$, alors $CVM_r \\Rightarrow CVM_s$\n",
    "\n",
    "## Loi des grands nombres\n",
    "La loi des grands nombres s'intéresse à la convergence de la moyenne empirique $\\bar{X}_n=\\frac{1}{n}\\sum_{i}X_i$ d'une suite de variables aléatoires $X_1,X_2,\\dots,X_n$ i.i.d. et d'espérance finie. Elle connait deux énoncés qui correspondant chacun à un type de convergence différent: \n",
    "* Soit $X_1,X_2,\\dots,X_n$ une suite de variables aléatoires i.i.d. et d'espérance $\\mu$ finie, alors $\\bar{X}_n \\xrightarrow[n \\to +\\infty]{\\text{c.v.p}} \\mu$\n",
    "* Soit $X_1,X_2,\\dots,X_n$ une suite de variables aléatoires i.i.d. et intégrable (i.e. $\\mathbb{E}(|X_1|)<+\\infty$), alors $\\bar{X}_n \\xrightarrow[n \\to +\\infty]{\\text{c.v.p.s.}} \\mu$\n",
    "\n",
    "Le second énoncé impose des hypothèses plus fortes (l'intégrabilité de la v.a. plutôt que simplement l'existence de l'espérance) mais nous assure également d'où forme plus forte de convergence (la convergence presque sûre contre la convergence en probabilité). Le premier énoncé correspond donc à la loi faible des grands nombres, le second à la loi forte des grands nombres. Suivre la loi forte implique de suivre la loi faible pour deux raisons: \n",
    "* Les hypothèses de la loi forte impliquent celles de la loi faible\n",
    "* La convergence presque sûre de la loi forte implique la convergence en probabilités de la loi faible.\n",
    "\n",
    "Remarques: \n",
    "* Il existe des lois de probabilité n'admettant pas d'espérance et pour lesquelles la loi de grands nombre ne peut être appliquée. Ex: loi de Cauchy.\n",
    "* On peut voir le théorème centrale limite comme un raffinement de la loi des grands nombres dans le sens où il apporte des précisions sur la vitesse de convergence.\n",
    "* On montre que la moyenne empirique d'une suite de v.a. i.i.d est l'ESBVM de leur espérance, la loi des grands nombres donne des résultats sur la convergence de cet estimateur. La loi faible (resp. forte) des grands nombres démontre que la moyenne empirique est un estimateur convergent (resp. fortement convergent) de l'espérance. \n",
    "* Interprétation de la loi faible des grands nombres/de la convergence en probabilité: plus la taille de l'échantillon augmente, plus les echantillons pour lesquels la moyenne empirique s'éloigne de l'espérance sont rares.\n",
    "\n",
    "La loi des grands nombres apporte le résultat fondamental de la convergence de la moyenne empirique d'un échantillon vers l'espérance lorsque la taille de l'échantillon tend vers l'infini. Elle permet entre autres de justifier l'approximation et l'interprétation d'une probabilité comme une fréquence de réalisation, justifiant du même coup le principe des sondages. \n",
    "\n",
    "Un exemple appliqué au *machine learning*: La fonction qu'on cherche à approcher en ML est souvent la solution d'un problème d'optimisation fonctionnel. En théorie on cherche $f^{*}=argmin_{f\\in\\mathcal{H}}\\mathbb{E}_{X,Y}[\\mathcal{L}(y,f(x))]$. Le calcul de cette espérance présuppose la connaissance de la distribution de $(X,Y)$ à laquelle on a rarement accès. L'espérance de la *loss* est alors approximée par sa moyenne empirique sur le *training set* appelée risque empirique (*empirical risk*). On cherche alors $f^{*}=argmin_{f\\in\\mathcal{H}}\\frac{1}{n}\\sum_{(x_i,y_i)\\in\\mathcal{D}_n}\\mathcal{L}(y_i,f(x_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions classiques\n",
    "## Distributions discrètes \n",
    "Les principales distributions discrètes se séparent en deux cétagories: \n",
    "* celles pouvant être vues comme modélisant la distribution du nombre de succès (et donc à support fini) lors de tirages successifs réalisé dans une urne contenant deux (cas univarié) ou plusieurs (cas multivarié) type de boules \n",
    "* et leur symétrique modélisant la distribution du nombre d'échecs avant l'obtention d'un $k^{e}$ succès lors de tirages successifs de ces mêmes boules (à support potentiellement infini). \n",
    "\n",
    "Remarque : dans le contexte de tirage dans une urne contenant deux ou plus type de boules, on appelle \"succès\" le tirage d'une boule de la catégorie (arbitraire) d'intérêt. \n",
    "\n",
    "### Distribution du nombre de succès lors de tirages avec remise (indépendants) : Lois de Bernoulli, catégorielle, binomiale et multinomiale\n",
    "La loi de Bernoulli correspond à la modélisation d'un unique tirage dans une urne contenant deux types de boules. Sa version multivariée parfois rencontrée s'appelle loi catégorielle (*categorical distribution*). La binomiale correspond à la répétition d'épreuves de Bernoulli dans le cas de l'urne à deux types de boules. Sa version multivariée s'appelle loi multinomiale. \n",
    "\n",
    "Remarque : dans le cas univarié, les moments d'intérêt (espérance et variance) sont des scalaires. Dans le cas multivarié, l'espérance est un vecteur, la variance correspond à la matrice (d'auto)covariance.\n",
    "\n",
    "#### Loi de Bernoulli $Ber(p)$\n",
    "* Paramètres: $p \\in [0, 1]$ et $q = 1-p$ (deux paramètres, un seul d'indépendant)\n",
    "* Support : $\\{0, 1\\}$\n",
    "* Fonction de masse : $\\left\\{\\begin{array}{l}{\\mathbb{P}(X=1)=p} \\\\ {\\mathbb{P}(X=0)=q=1-p}\\end{array}\\right.$\n",
    "* Espérance : $\\mathbb{E}(X)=p$\n",
    "* Variance : $\\mathbb{V}(X)=pq = p(1-p)$\n",
    "\n",
    "#### Loi catégorielle\n",
    "* Paramètres : $p_{i}  \\in [0, 1]$ et $\\sum_{i}p_{i}=1$ ($k>2$ paramètres dont seuls $k-1$ sont indépendants)\n",
    "* Support : $\\{0, 1\\}^{k}$\n",
    "* Fonction de masse : $\\mathbb{P}(X_{1}=0,\\dots, X_{i}=1, \\dots,X_{k}=0)=p_{i}$\n",
    "* Espérance : $\\mathbb{E}(X_{i})=p_{i}$\n",
    "* Variance : $\\mathbb{V}(X_{i})=p_{i}(1-p_{i})$ et $cov(X_{i}, X_{j})=-p_{i}p_{j}$\n",
    "\n",
    "#### Loi binomiale $\\mathcal{B}(n, p)$\n",
    "La loi binomiale correspond à la distribution du nombre de succès lors de $n$ répétitions d'une même épreuve de Bernoulli de paramètre p (il existe par ailleurs une version généralisée de la loi binomiale dans le cas où ces épreuves ne seraient plus les mêmes), la loi de Bernoulli correspondant elle même à un cas particulier de la loi binomiale (une $\\mathcal{B}(1, p)$). \n",
    "* Paramètres : $n \\in \\mathbb{N}^{*}$, $p \\in [0, 1]$ et $q = 1-p$ (trois paramètres dont deux d'indépendants)\n",
    "* Support : $[\\![0, n]\\!]$\n",
    "* Fonction de masse : $\\mathbb{P}(X=k)=\\binom{n}{k}p^{k}q^{n-k}$\n",
    "* Espérance : $\\mathbb{E}(X)=np$\n",
    "* Variance : $\\mathbb{V}(X)=npq=np(1-p)$\n",
    "\n",
    "La loi binomiale correspondant à une somme de $n$ épreuves de Bernoulli identiques de paramètre $p$, on peut montrer à l'aide du théorème centrale limite qu'elle converge en loi (et donc admet comme approximation asymptotique) vers une loi normale $\\mathcal{N}(np, npq)$.\n",
    "\n",
    "D'où vient la forme de la fonction de masse ? d'où vient le coefficient binomial ? Il suffit juste de dénombrer. La probabilité d'obtenir $k$ succès après $n$ répétitions d'une loi de Bernoulli identique s'écrit trivialement $p^{k}q^{n-k}$. $\\mathbb{P}(X=k)$ sera à égal à cette probabilité fois le nombre de séquences à $k$ succès. Or, il vient immédiatement que l'ensemble des séquences de longueur $n$ contenant $k$ succès est égal à $\\binom{n}{k}$ (par définition). On en déduit la forme de la fonction de masse.\n",
    "\n",
    "#### Loi multinomiale\n",
    "De façon analogue à son équivalent univarié, la loi multinomiale correspond à la répétition de $n$ épreuves de Bernoulli multivariée (catégorielle) identiques. De la même manière que le théorème centrale limite permet de montrer qu'une binomiale converge en loi vers une loi normale univariée, on montre qu'une multinomiale converge en loi vers une loi normale multivariée de même espérance et matrice de covariance.\n",
    "* Paramètres : $n \\in \\mathbb{N}^{*}$, $p_{i}  \\in [0, 1]$ et $\\sum_{i}p_{i}=1$ ($k+1>3$ paramètres dont seuls $k$ sont indépendants)\n",
    "* Support : $[\\![0, n]\\!]^{k}$\n",
    "* Fonction de masse : $\\mathbb{P}(X_{1}=x_{1},\\dots,X_{k}=x_{k})=\\frac{n!}{x_{1}! \\dots x_{k}!}p_{1}^{x_{1}} \\dots p_{k}^{x_{k}}$\n",
    "* Espérance : $\\mathbb{E}(X_{i})=np_{i}$\n",
    "* Variance : $\\mathbb{V}(X_{i})=np_{i}(1-p_{i})$ et $cov(X_{i}, X_{j})=-np_{i}p_{j}$\n",
    "\n",
    "Remarque : pour être parfaitement explicite et clair sur le cas multivarié, $\\mathbb{P}(X_{1}=x_{1},\\dots,X_{k}=x_{k})$ désigne la probabilité après $n$ tirages avec remise dans une urne contenant $k$ catégories de boules différentes, d'avoir obtenu $x_{1}$ boules de la catégorie 1, $x_{2}$ boules de la catégorie 2, etc. Cette formulation s'applique aussi au cas univarié (dont on se rend compte qu'il est en fait bivarié mais à un seul degré de liberté : si $x_{1}=k$ alors $x_{2}=n-k$).\n",
    "\n",
    "### Distribution du nombre de succès lors de tirages sans remise (non indépendants) : Loi hypergéométrique\n",
    "La loi hypergéométrique correspond à la même situation de tirages successifs dans une urne contenant deux catégories de boules (cas univarié) mais où le tirage se fait sans remise : la probabilité de tirer une même boule n'est donc plus indépendante des résultats des tirages précédents. \n",
    "* Paramètres : $N \\in \\mathbb{N}^{*}$ la population totale, $K \\in [\\![0, N]\\!]$ la population de la classe d'intérêt et $n \\in [\\![0, N]\\!]$ le nombre de tirages\n",
    "* Support : $k \\in [\\![max(0, n+K-N), min(n,K)]\\!]$\n",
    "* Fonction de masse : $\\mathbb{P}(X=k)=\\frac{\\binom{K}{k}.\\binom{N-K}{n-k}}{\\binom{N}{n}}$\n",
    "* Espérance : $\\mathbb{E}(X)=n\\frac{K}{N}$\n",
    "* Variance : N'a pas de forme simple\n",
    "\n",
    "Remarque : l'expression compliquée pour le support correspond: \n",
    "* pour son terme de gauche : si on a réalisé plus de succès que la population de la boule d'intérêt, alors on l'a forcément tirée une ou plusieurs fois.\n",
    "* pour son terme de droite : on ne peut pas obtenir plus de succès que la population d'intérêt.\n",
    "\n",
    "On dispose pour la loi hypergéométrique d'approximation asymptotiques intuitives : \n",
    "* Si $n<<K$ et $n<<N$ alors la loi hypergéométrique $\\mathcal{H}(n, N, K)$ s'approche d'une loi binomiale $\\mathcal{B}(n, K/N)$.\n",
    "* De même que pour la loi binomiale, si on se trouve sous l'approximation binomiale et que $n$ est suffisamment grand, la loi hypergéométrique s'approche par une loi normale $\\mathcal{N}(np, np(1-p))$ avec $p=K/N$.\n",
    "\n",
    "De même que pour la loi binomiale, il existe pour la loi hypergéométrique une version multivariée dont il n'est pas utile d'aborder le détail.\n",
    "### Distribution du nombre d'échecs avant le $k^{e}$ succès lors de tirages avec remise (indépendants) : Loi binomiale négative\n",
    "La loi binomiale négative modélise une expérience proche de la loi binomiale dans le sens où : \n",
    "* on procède à des tirages avec remise successifs équivalents à une répétition d'épreuves de Bernoulli identiques\n",
    "* la loi prend deux paramètre $n$ et $p$, $p$ correspondant au paramètre de l'épreuve de Bernoulli. \n",
    "Mais on ne s'intéresse pas à la même chose que pour la loi binomiale: la loi négative binomiale décrit la distribution du nombre d'échecs $k$ survenus avant d'avoir obtenu $n$ succès après $n+k$ répétitions d'une épreuve de Bernoulli de paramètre $p$.  \n",
    "* Paramètres : $n \\in \\mathbb{N}$, $p \\in [0, 1]$ et $q = 1-p$ (trois paramètres dont deux d'indépendants)\n",
    "* Support : $k \\in \\mathbb{N}$, on remarque que le support de $k$ est ici infini.\n",
    "* Fonction de masse : $\\mathbb{P}(X=k)=\\binom{n+k-1}{k}p^{n}q^{k}$\n",
    "* Espérance : $\\mathbb{E}(X)=n(1-p)/p$\n",
    "* Variance : $\\mathbb{V}(X)=n(1-p)/p^{2}$\n",
    "\n",
    "D'où vient la forme de la fonction de masse ? Comme pour la loi binomiale, on montre immédiatement que la probabilité d'obtenir $k$ échecs lors de $n+k$ répétition d'une épreuve de Bernoulli est $p^{n}q^{k}$. Reste à connaitre le nombre de séquences à $n+k$ éléments comportant k échecs/éléments d'intérêt. Or, on sait que le dernier élément de la séquence est un succès par définition. On recherche donc en fait le nombre de séquences à $n+k-1$ élements comportant $k$ échecs/éléments d'intérêt. Ce nombre étant par définition $\\binom{n+k-1}{k}$, on en déduit la forme de la fonction de masse. \n",
    "\n",
    "Il existe une version multivariée de la binomiale négative, mais elle semble peu utilisée. \n",
    "\n",
    "### Distribution du nombre d'échecs avant le $k^{e}$ succès lors de tirages avec remise (dépendants) : Loi hypergéométrique négative\n",
    "Il existe un équivalent négatif (compliqué) pour la loi hypergéométrique mais il n'est que peu utilisé.\n",
    "\n",
    "### Loi de Poisson $\\mathcal{P}(\\lambda)$\n",
    "Considérant un événement survenant en moyenne un nombre $\\lambda$ de fois par intervalle (par exemple de temps ou d'espace), cette fréquence restant constante et la survenue d'un événement n'ayant pas d'impact sur la probabilité de survenue du suivant (processus sans mémoire), alors la distribution du nombre $k$ d'événements survenant dans un intervalle donné suit une loi de Poisson.\n",
    "* Paramètres : $\\lambda \\in \\mathbb{R}^{+*}$\n",
    "* Support : $k \\in \\mathbb{N}$, on remarque que le support de $k$ est ici infini.\n",
    "* Fonction de masse : $\\mathbb{P}(X=k)=\\frac{\\lambda^{k}e^{-\\lambda}}{k!}$\n",
    "* Espérance : $\\mathbb{E}(X)=\\lambda$ (par définition).\n",
    "* Variance : $\\mathbb{V}(X)=\\lambda$\n",
    "\n",
    "On peut montrer que la loi de Poisson de paramètre $\\lambda=np$ est correspond à la loi limite d'une binomiale $\\mathcal{B}(n, p)$ pour $n$ tendant vers l'infini et $p$ faible (pour une fois ce n'est pas le théorème centrale limite, il suffit d'étudier le comportement asymptotique de la fonction de masse de la binomiale). Ce résultat permet une approximation asymptotique commode (et classique) de la binomiale (la loi de Poisson étant plus simple à manipuler) sous les conditions $p$ \"sufisamment faible\" et $n$ \"suffisamment grand\".   \n",
    "\n",
    "Une autre approximation asymptotique classique pour la loi de Poisson vient de sa convergence en loi vers une $\\mathcal{N}(\\lambda, \\lambda)$ quand $\\lambda$ tend vers l'infini / pour $\\lambda$ \"suffisamment grand\". Là encore, ce résultat ne provient pas du théorème centrale limite mais s'appuie sur l'étude du comportement asymptotique de la fonction caractérisque de la loi de Poisson. \n",
    "\n",
    "Remarque : la loi de Poisson est stable par addition (pas étonnant vu sa proximité avec la loi exponentielle): la somme de deux lois de Poisson de paramètres $\\lambda_1$ et $\\lambda_2$ est une loi de Poisson de paramètre $\\lambda_1 + \\lambda_2$. \n",
    "\n",
    "Remarque : on montre que la moyenne empirique est l'estimateur du maximum de vraisemblance pour $\\lambda$.\n",
    "\n",
    "## Distributions continues\n",
    "Sauf mention contraire, on se place toujours dans le cas univarié.\n",
    "\n",
    "### Loi uniforme\n",
    "* Paramètres: $(a, b) \\in \\mathbb{R}^{2}$\n",
    "* Support: $[a, b]$ (support fini)\n",
    "* Fonction de répartion : $F_{X}(t)=\\frac{(t-a)}{(t-a)}$\n",
    "* Densité de probabilité : $f_{X}(t)=\\frac{1}{(b-a)}$\n",
    "* Espérance : $\\mathbb{E}(X) = \\frac{1}{2}(a + b)$\n",
    "* Variance : $\\mathbb{V}(X) = \\frac{1}{12}(b - a)^{2}$\n",
    "\n",
    "### Lois normale et log-normale\n",
    "#### Loi normale $\\mathcal{N}(\\mu, \\sigma^{2})$\n",
    "* Paramètres: $\\mu \\in \\mathbb{R}, \\sigma > 0$\n",
    "* Support:  $\\mathbb{R}$ \n",
    "* Fonction de répartion : $F_{X}(t)=\\int_{-\\infty}^{t} f_{X}(u) \\mathrm{d} u$\n",
    "* Densité de probabilité : $f_{X}(t)=\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp(-\\frac{(t-\\mu)^{2}}{2\\sigma^{2}})$\n",
    "* Espérance : $\\mathbb{E}(X) = \\mu$\n",
    "* Variance : $\\mathbb{V}(X) = \\sigma^{2}$\n",
    "\n",
    "La loi normale est stable par addition : soient une famille de variables aléatoires indépendantes normalement ditribuées $X_{i}$ suivant une $\\mathcal{N}(\\mu_{i}, \\sigma_{i}^{2})$ respectivement, alors $\\sum_{i}X_{i}$ suit une $\\mathcal{N}(\\sum_{i}\\mu_{i}, \\sum_{i}\\sigma_{i}^{2})$.\n",
    "\n",
    "#### Loi log-normale\n",
    "On dit que $U$ est log-normale de paramètres $\\mu \\in \\mathbb{R}$ et $\\sigma \\in \\mathbb{R}^{+*}$ si $X=ln(U-\\theta)$ normale de paramètres $\\mu$ et $\\sigma$.\n",
    "* Paramètres: $\\mu \\in \\mathbb{R}, \\sigma > 0$\n",
    "* Support: $]\\theta, +\\infty[$ avec $\\theta \\in \\mathbb{R}$, en général $\\theta$ est souvent pris implicitement égal à zéro. \n",
    "* Espérance : $\\mathbb{E}(X) = e^{\\mu}$\n",
    "* Variance : Pas de forme simple.\n",
    "\n",
    "L'utilité de la loi log-normale apparait notamment pour des produits de variables aléatoires. Prenons par exemple $T=\\prod_{i} X_{i}$ avec les $X_{i}$ i.i.d. à valeurs dans $\\mathbb{R}^{+*}$, $log(T)=\\sum_{i} log(X_{i})$ cette variable aléatoire convergeant en loi vers une loi normale d'après le théorème centrale limite. On en déduit alors que T converge asymptotiquement en loi vers une loi log-normale. La loi log-normale est donc notamment la loi limite des produits de variables aléatoires i.i.d..\n",
    "\n",
    "#### Loi normale multivariée\n",
    "Il s'agit d'une généralisation au cas de vecteurs gaussiens (vecteurs aléatoires donc chaque composante suit une loi normale, ces composantes n'étant pas forcément idépendantes entres elles, c'est le cas uniquement si la matrice de covariance $\\Sigma$ est diagonale).\n",
    "\n",
    "* Paramètres: $\\mu \\in \\mathbb{R}^{n}, \\Sigma \\in \\mathcal{M}_{n}(\\mathbb{R})$ et $\\Sigma$ semi-définie positive (cas non dégénéré).\n",
    "* Support:  $\\mathbb{R}^{n}$\n",
    "* Densité de probabilité : $f_{\\mathbf{X}}(\\mathbf{x})=\\frac{1}{\\sqrt{(2 \\pi)^{k} det(\\mathbf{\\Sigma})}}\\exp (-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^{\\mathrm{T}} \\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}))$\n",
    "\n",
    "* Espérance : $\\mathbb{E}(X) = \\mu$\n",
    "* Variance : $\\mathbb{V}(X) = \\Sigma$\n",
    "\n",
    "La loi normale multivariée présente aussi la stabilité par addition : l'espérance de la somme de $n$ gaussiennes multivariées correspondra à la somme des espérances, idem pour la matrice de covariance. Plus généralement, la gaussienne multivariée est stable par transformation affine : pour $X$ suivant une $\\mathcal{N}(\\boldsymbol{\\mu}, \\Sigma)$, $b \\in \\mathbb{R}^{m}$ et $A \\in \\mathcal{M}_{m, n}(\\mathbb{R})$, alors $Y=A.X+b$ suit une  $\\mathcal{N}(A.\\boldsymbol{\\mu}+b, A.\\Sigma.A^{\\top})$.\n",
    "\n",
    "### Lois gamma, du $\\chi^{2}$  et exponentielle\n",
    "#### Loi gamma\n",
    "* Paramètres: $k>0$ (facteur de forme) et $\\theta > 0$ (facteur d'échelle)\n",
    "* Support: $\\mathbb{R}^{+}$\n",
    "* Fonction de répartion : $F_{X}(t)=\\int_{-\\infty}^{t} f_{X}(u) \\mathrm{d} u$\n",
    "* Densité de probabilité : $f_{X}(t)=\\frac{1}{\\Gamma(k)\\theta}.(\\frac{x}{\\theta})^{k-1}.e^{-\\frac{x}{\\theta}}$ \n",
    "* Espérance : $\\mathbb{E}(X) = k\\theta$\n",
    "* Variance : $\\mathbb{V}(X) = k\\theta^{2}$\n",
    "\n",
    "$\\Gamma(k)$ permet à la densité de sommer sur 1 (constante de normalisation). Si $k \\in \\mathbb{N}$, alors $\\Gamma(k)=(k-1)!$. Une propriété importante de la loi gamma et de ses dérivées est l'additivité : si $X_{i}$ ensemble de variables aléatoires indépendantes chacune distribuée suivant une $\\gamma(k_{i}, \\theta)$, alors $\\sum_{i}X_{i}$ suit une $\\gamma(\\sum_{i}k_{i}, \\theta)$.\n",
    "\n",
    "#### Loi exponentielle\n",
    "La loi exponentielle de paramètre $\\lambda$ est un cas particulier de loi gamma puisque qu'elle correspond à une \n",
    "$\\gamma(1, 1/\\lambda)$. On en déduit immédiatement: \n",
    "* Espérance : $\\mathbb{E}(X) = 1/\\lambda$\n",
    "* Variance : $\\mathbb{V}(X) = 1/\\lambda^{2}$\n",
    "\n",
    "Etant une loi gamma, la loi exponentielle présente aussi la propriété d'additivité.\n",
    "\n",
    "#### Loi du $\\chi^{2}$\n",
    "Le $\\chi^{2}$ à $n$ degrés de libertés est souvent défini comme la loi suivie par la somme des carrés de $n$ variables aléatoires i.i.d. (ce qui permet d'affirmer l'existance d'un loi limite par le théorème centrale limite quand n croit à l'infini) suivant une $\\mathcal{N}(0, 1)$. Or il se trouve qu'un $\\chi_{n}^{2}$ est aussi une $\\gamma(n/2, 2)$. On en déduit immédiatement: \n",
    "* Espérance : $\\mathbb{E}(X) = n$\n",
    "* Variance : $\\mathbb{V}(X) = 2n$\n",
    "\n",
    "Les lois du $\\chi^{2}$ étant des cas particuliers de lois gamma, elles possèdent également la propriété d'additivité.\n",
    "\n",
    "### Loi de Student\n",
    "Soient U et V deux variables aléatoires indépendantes suivant respectivement une $\\mathcal{N}(0, 1)$ et un $\\chi_{n}^{2}$, alors la quantité $\\frac{U}{\\sqrt{V/n}}$ suivra par définition une loi de Student à $n$ degrés de liberté. La loi de Student tend vers une $\\mathcal{N}(0, 1)$ quand n croit à l'infini.  \n",
    "\n",
    "* Paramètres: $n \\in \\mathbb{N^{*}}$ appelé nombre de degrés de liberté. \n",
    "* Support: $\\mathbb{R}$\n",
    "* Espérance $\\mathbb{E}(X) = 0$\n",
    "* Variance : $\\mathbb{V}(X) = \\frac{n}{n-2}$\n",
    "\n",
    "Les fonctions de répartition et de densité n'ont pas de forme simple. \n",
    "\n",
    "### Loi de Fisher\n",
    "Soient U et V deux variables aléatoires indépendantes suivant chacune un $\\chi^{2}$ à respectivement $n_{1}$ et $n_{2}$ degrés de liberté, alors la quantité $\\frac{U/n_{1}}{V/n_{2}}$ suivra par définition une loi de Fisher à $(n_{1}, n_{2})$ degrés de liberté. La loi de Student tend vers une $\\mathcal{N}(0, 1)$ quand n croit à l'infini. Les lois de Fisher sont à support dans $\\mathbb{R}^{+*}$ et sont disymétriques (contrairement aux lois de Student). Si $X$ suit une loi de Student à $n$ degrés de libertés, alors $X^{2}$ suit par définition une loi de Fisher à $(1, n)$ degrés de liberté. Les fonctions de répartition, de densité ainsi que les premiers moments n'ont pas de forme simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Différence entre probabilité et vraisemblance\n",
    "Pour être synthétique et en prenant l'exemple de variables continues:\n",
    "* Une probabilité est une aire sous la courbe d'une distribution donnée. Cette aire nous donne la probabilité d'obtenir certaines données étant donnée la distribution choisie: $P(data|distribution)$\n",
    "* La vraisemblance d'une observation correspond à la valeur prise par une distribution en ce point. Etant données des observations, calculer la vraisemblance pour différentes distributions sur ce jeu de données nous permet de juger de la plausibilité de chacune d'entre elles à avoir généré les données. Dans tout calcul de vraisemblance, les données/observations sont fixes, données, et les distributions variables: $L(distribution|data)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test de Student\n",
    "Peut se dire de tout test dont la statistique de test suit une loi de Student sous l’hypothèse nulle. \n",
    "\n",
    "## Importance de la loi de Student en statistiques \n",
    "Pour une suite de variables aléatoires i.i.d. suivant une $\\mathcal{N}(\\mu, \\sigma^{2})$, on montre que le quotient des variables aléatoires $(\\frac{\\overline{X}-\\mu}{\\sigma/\\sqrt{n}})$ et $\\frac{S \\sqrt{n}}{\\sigma}$ forme une statistique indépendante du paramètre $\\sigma$ et suivant une loi de Student, ces deux quantités étant dans le cas gaussien indépendantes et suivant respectivement une $\\mathcal{N}(0, 1)$ et un $\\chi_{n-1}^{2}$. On peut donc utiliser cette statistique pour des tests (sur la valeur de l’espérance par exemple) sans se soucier de connaître la valeur de la variance.\n",
    "Les tests de Student sont de plus réputés relativement robuste lorsqu’on s’écarte des hypothèses de normalité.\n",
    "\n",
    "## Exemple : Test d’espérance pour un échantillon gaussien\n",
    "On dispose d’un échantillon hasard gaussien de loi parente $\\mathcal{N}(\\mu, \\sigma^{2})$ d’espérance $\\mu$ inconnue. On souhaite tester si cette espérance prend une valeur particulière. L’espérance peut donc apparaître dans la statistique de test puisque sa valeur sera par définition connue sous l’hypothèse nulle.\n",
    "\n",
    "* Si la variance $\\sigma^{2}$ de la loi parente est connue, on va simplement se servir de la moyenne empirique $\\overline{X}$ comme statistique de test et dont la loi sous l'hypothèse nulle est une gaussienne parfaitement déterminée. Ce type de test sur la valeur de la moyenne est aussi souvent appelé z-test.\n",
    "* Si la variance de la loi parente est inconnue on va former à l’aide de $\\overline{X}$ et $S$ une statistique indépendante de $\\sigma$ suivant une loi de Student. Ce type de test sur la valeur de la moyenne est aussi souvent appelé t-test.\n",
    "\n",
    "## Exemple : Test d’égalité des moyennes de deux échantillons supposés gaussiens \n",
    "On dispose de deux échantillons correspondant chacun à des réalisations de deux variables aléatoires X et Y et qui sont supposées indépendantes et identiquement distribuées suivant les lois normales $\\mathcal{N}(\\mu_{X}, \\sigma^{2})$ et $\\mathcal{N}(\\mu_{Y}, \\sigma^{2})$ respectivement. Les variances sont supposées égales (mais pas forcément connues). On va tester l’égalité des espérances (inconnues), situation correspondant à l’hypothèse nulle.\n",
    "\n",
    "La statistique de test consiste en la studentisation de la variable aléatoire $\\overline{X}-\\overline{Y}$. La studentisation impose l’hypothèse supplémentaire de l’indépendance des échantillons afin que le $S^{2}$ de $\\overline{X}-\\overline{Y}$ puisse simplement consister en la somme de $S^{2}_{X}$ et $S^{2}_{Y}$ et suivre une loi du $\\chi^{2}$. Sous la double hypothèse d’égalité des variances et des espérances, cette dernière correspondant à l’hypothèse nulle, la statistique obtenue ne dépend plus des paramètres inconnus des lois suivies par $X$ et $Y$.\n",
    "\n",
    "## Exemple : Test de significativité des coefficients estimés d’une régression linéaire\n",
    "Pour la régression linéaire, il est systématique de tester la significativité des coefficients estimés : peut-on affirmer que chaque coefficient estimé est différent de zéro pour le risque de première espèce qu’on se donne ?\n",
    "\n",
    "\n",
    "Pour un coefficient donné, le test consiste à studentiser l’estimateur de ce coefficient. Les hypothèses du modèle linéaire permettent ensuite de justifier que la statistique formée suit une loi de Student : \n",
    "* La normalité des résidus implique que l’estimateur de chaque coefficient suit une loi normale (pas aussi simple à montrer qu’il n’y parait).\n",
    "* La forme linéaire du modèle (la réponse modélisée comme combinaison linéaire des variables dépendantes) et la normalité des résidus permettent via une utilisation appropriée du théorème de Cochran de démontrer la validité des hypothèses nécessaires à ce que la statistique de test suive une loi de Student : \n",
    "    * Indépendance du numérateur et du dénominateur.\n",
    "    * Dénominateur suivant une loi du $\\chi^{2}$.\n",
    "\n",
    "Ce test est prolongé par l’ANOVA à 1 facteur qui revient à tester l’égalité des espérances pour $k>2$ échantillons gaussiens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests du $\\chi^{2}$ \n",
    "Peut se dire de tout test dont la statistique de test suit une loi du $\\chi^{2}$ sous l’hypothèse nulle.\n",
    "\n",
    "## Exemple : Test de variance pour un échantillon gaussien\n",
    "On dispose d’un échantillon hasard gaussien de loi parente de variance inconnue. On souhaite tester si cette variance prend une valeur particulière. La variance peut donc apparaître dans la statistique de test puisque sa valeur sera par définition connue sous l’hypothèse nulle :\n",
    "\n",
    "* Si l’espérance de la loi parente $\\mu$ est connue, on va simplement former la somme des carrés des écarts à la moyenne $\\sum_{i}(y_{i}-\\mu)^{2}/\\sigma^{2}$, somme de $n$ gaussiennes centrées indépendantes, cette statistique suit un $\\chi_{n}^{2}$. \n",
    "* Si l’espérance de la loi parente $\\mu$ est inconnue, on va l’estimer sur l’échantillon à l’aide de la moyenne empirique introduisant du même coup une relation de dépendance linéaire entre les éléments de l’échantillon. On utilise ainsi $S^{2}$ comme statistique de test. Cette statistique suit un $\\chi_{n-1}^{2}$.\n",
    "\n",
    "## Tests modélisant le problème comme l'étude de la population d'intervalles disjoints\n",
    "On va s’intéresser maintenant à une famille de tests du $\\chi^{2}$ partageant une approche commune. La modélisation du problème va en effet consister à chaque fois à se ramener à un ensemble d’intervalles disjoints (niveaux d'une variable qualitative, segmentation d'une variable quantitative, etc.) et d’étudier pour chacun d’entre eux l’écart entre la population observée et une population théorique. L’hypothèse nulle est à chaque fois une hypothèse sur la population théorique.\n",
    "\n",
    "## Exemple : Test d’ajustement (à une loi) appelé aussi test d’adéquation\n",
    "On veut tester si notre n-échantillon hasard suit une loi particulière, l’idée du test est alors de comparer les histogrammes réalisé et théorique de l’échantillon. Dans le cas où l’échantillon correspondrait bien à des tirages indépendants de la loi postulé, l’écart mesuré entre les deux histogrammes devrait être faible. Le test commence par postuler une loi pour l’échantillon, sous l’hypothèse nulle chaque élément de l’échantillon correspond à une réalisation de cette loi. \n",
    "\n",
    "Pour construire les histogrammes, il faut diviser l’espace dans lequel l’échantillon prend ses valeurs (on aura aussi choisi sa loi de manière à ce que son support coïncide avec ce domaine de définition) en $k$ intervalles disjoints (bins). La probabilité théorique de tomber dans chaque intervalle se déduit de la densité de probabilité de la loi postulée. \n",
    "\n",
    "Sous l’hypothèse nulle, l’histogramme d’une réalisation de l’échantillon peut se modéliser comme une réalisation d’une loi multinomiale de paramètres $n$ et $(p_{1}, \\dots, p_{k})$. On va ensuite supposer que la taille de l’échantillon est assez élevée (avec la restriction supplémentaire que les effectifs de chaque intervalle soient tous suffisamment élevés) pour pouvoir approximer la loi multinomiale suivie par le vecteur aléatoire des populations de chaque intervalle $(n_{1}, \\dots, n_{k})$ par une loi normale multivariée (théorème centrale limite). On s’intéresse ensuite à la version centrée et réduite du vecteur gaussien à valeurs dans $\\mathbb{R}^{k}$ obtenu. La statistique du test correspond à la (2-)norme de ce vecteur qu’on attend proche de zéro sous l’hypothèse nulle. Le théorème de Cochran (entre autres preuves possibles) permet alors de montrer que la statistique de test suit un $\\chi_{k-1}^{2}$. Il existe en effet une relation linéaire introduisant une dépendance entre les composantes du vecteur aléatoire (la somme des effectifs de chaque intervalle devant être égale à l’effectif total de l’échantillon) et diminuant d’autant le nombre de degrés de liberté.\n",
    "\n",
    "On voit que ce type de test est très général : il n’impose pas de restriction particulière à la loi qu’on postule pour la population, on parle de test libre. Cette propriété explique sa prépondérance en statistique.\n",
    "\n",
    "L’idée générale du test est donc de comparer les effectifs d’intervalles à un effectif théorique, ce dernier étant calculé comme le produit de l’effectif total et de la probabilité d’appartenance à l’intervalle. Le vecteur aléatoire des effectifs de chaque classe est modélisé d’abord par une multinomiale finalement approximée asymptotiquement par une gaussienne multivariée. Ce vecteur gaussien est finalement centré et réduit et sa norme sert de statistique de test qui suit un $\\chi^{2}$ dont le nombre de degrés de liberté dépend du problème. L’hypothèse nulle du test va à chaque fois concerner la probabilité (théorique) d’appartenance à l’intervalle : hypothèse de loi dans le test d'adéquation, d'indépendance dans le test d'indépendante, etc.\n",
    "\n",
    "Remarque : Il existe évidemment de nombreux autres tests d’ajustement. On peut citer par exemple les tests de Kolomogorov ou de Cramer von Mises qui tous deux se reposent sur l’écart entre fonctions de répartition théorique et empirique. Ils divergent notamment sur la statistique de test utilisée et la loi (asymptotique) associée.\n",
    "\n",
    "## Exemple : Test d’homogénéité \n",
    "Soit une variable aléatoire $X$ qualitative/catégorielle à $R$ modalités dont on possède $K$ $n_{k}$-échantillons hasard. Le test d’homogénéité vise à tester si tous ces échantillons proviennent de la même population, c’est-à-dire qu’ils sont tous des échantillons de même loi parente $X$. Sous cette hypothèse (qui sera l’hypothèse nulle), la probabilité de présenter une même modalité sera la même dans chaque échantillon. On va appliquer la même méthode que pour le test d’ajustement, chaque intervalle correspondant à une modalité pour un échantillon donné ($K.R$ intervalles au total).\n",
    "\n",
    "Le nombre de degrés de libertés du $\\chi^{2}$ suivi par la statistique de test sera donc de $K.R – K = K.(R-1)$, chaque échantillon introduisant en effet une relation de dépendance (pour chaque échantillon, la somme des effectifs de chaque classe devant être égal à l’effectif total de l’échantillon) entre les $K.R$ composantes du vecteur aléatoire des populations. On distingue ensuite deux cas : \n",
    "* Soit les probabilités d’appartenance à chaque classe sont connues et le nombre de degrés de liberté du $\\chi^{2}$ suivi par la statistique de test reste de $K.(R-1)$. Ce cas correspond en fait à la réalisation simultanée d’un test d’homogénéité (répondant à la question \"Tous les échantillons sont-ils issus d’une même population ?\") et un test d’ajustement (répondant à la question \"Les distributions de chaque échantillon sont-elles compatibles avec la distribution de la loi postulée ?\").\n",
    "* Soit on est obligé d'estimer les probabilités d'appartenance à chaque classe sur l’ensemble de la population ce qui introduit $R$ relations de dépendance supplémentaires mais dont seules $R-1$ sont indépendantes (les probabilités estimées devant sommer sur $1$). Le nombre de degrés de libertés est alors de $K.R – K – (R – 1) = (K – 1).(R – 1)$.\n",
    "\n",
    "Ce test fonctionne aussi si la loi qu’on pense partagée par nos différents échantillons n’est pas discrète. Il suffit de faire comme dans le test d’adéquation et de diviser le support de la loi en $R$ intervalles qui joueront le rôle des différentes modalités. On peut alors se trouver dans la 2e situation où il est nécessaire d'estimer un (ou plusieurs) paramètre de la loi de densité de probabilité postulée. On choisira alors l'estimateur adapté et réduira le nombre de degré de liberté comme il convient.\n",
    "\n",
    "## Exemple : Test d’indépendance\n",
    "On s’intéresse à deux caractères que chaque individu de la population étudié peut présenter. On suppose que chacun de ces caractères peut prendre des valeurs discrètes, le cas des valeurs continues pouvant comme précédemment se ramener au cas discret par subdivision du domaine de définition en un nombre fini d’intervalles disjoints. Chaque individu est ainsi caractérisé par deux caractères A et B à $K$ et $R$ modalités respectivement.\n",
    "\n",
    "Comme dans les tests précédents, on va former le vecteur aléatoire (de taille $K.R$) des populations de chaque intervalle, un intervalle correspondant ici à un couple particulier de modalités pour les caractères A et B.  \n",
    "L’hypothèse nulle consiste à postuler que les caractères A et B sont indépendants. La probabilité théorique d’appartenance à un intervalle sera alors égale au produit des probabilités de présenter chacune des modalités a priori.\n",
    "\n",
    "Le nombre de degrés de libertés du $\\chi^{2}$ suivi par la statistique de test sera de $K.R-1$, la contrainte que les effectifs de chaque classe devant sommer sur l’effectif total introduisant une relation linéaire de dépendance entre les composantes du vecteur aléatoire des populations.\n",
    "\n",
    "Le nombre de degrés de libertés se réduit encore si on doit estimer sur la population les probabilités d’occurrence de chaque modalité. Les estimations de probabilités introduisent ainsi pour les caractères A et B, $K – 1$ et $R – 1$ relations linéaires indépendantes entre les composantes du vecteur aléatoire des populations respectivement. Un rapide calcul montre que le nombre total de degrés de liberté du $\\chi^{2}$ suivi par la statistique de test est alors de $(K-1).(R-1)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test de Fisher\n",
    "Peut se dire de tout test dont la statistique de test suit une loi de Fisher sous l’hypothèse nulle. \n",
    "\n",
    "## Exemple : Test d’égalité des variances de deux échantillons supposés gaussiens\n",
    "On prend deux échantillons hasard gaussiens supposés indépendants pour lesquels variance et espérance sont inconnues. On teste l’hypothèse d’une égalité des variances (hypothèse nulle) des lois parentes des deux échantillons. On forme pour chaque échantillon la somme des carrés des composantes qui par définitions suivent chacune des lois du $\\chi^{2}$. Les deux échantillons étant supposés indépendants, les deux $\\chi^{2}$ le sont aussi. On forme alors le quotient des deux quantités pour obtenir la statistique de test qui quit une loi de Fisher. Sous l’hypothèse nulle, les variances disparaissent de la statistique de test qui ne dépend plus que des échantillons. On remarque que ce test est également indépendant de la connaissance des espérances des lois parentes. \n",
    "\n",
    "## Exemple : Test de significativité d’une régression linéaire \n",
    "On teste l’hypothèse qu’il n’existe pas de coefficient de la régression (hors constante) non-nul (hypothèse nulle). On utilise la formule d'analyse de la variance qui décompose la variance totale en somme de la variance expliquée et de la variance résiduelle (égalité toujours vraies dans le contexte des moindres carrés car correspond à une simple utilisation du théorème de Pythagore). Le théorème de Cochran permet de montrer en s'appuyant sur les hypothèses du modèle linéaire que ces différentes contributions sont indépendantes et suivent chacune une loi du $\\chi^{2}$. \n",
    "\n",
    "On forme alors le quotient de la variance expliquée sur la variance résiduelle qui suit ainsi une loi de Fisher. Sous l’hypothèse nulle, le modèle se réduit à la constante, l’espace sur lequel on projette est donc de dimension 1. On en déduit immédiatement les degrés de liberté des différents $\\chi^{2}$ : la variance expliquée suit alors un $\\chi_{1}^{2}$ et la variance résiduelle un $\\chi_{n-2}^{2}$. La statistique de test suit alors une loi de Fisher $\\mathcal{F}(1, n-2)$.\n",
    "\n",
    "La même technique est utilisée pour comparer différents modèles emboités. Les tests de Fisher correspondants sont alors associés à un crtière d'information (AIC, BIC, etc.) pour sélectionner le meilleur modèle.\n",
    "\n",
    "## Exemple : ANOVA à 1 facteur - Test d’égalité des moyennes pour $k$ échantillons gaussiens\n",
    "On suppose qu’on dispose de $k$ échantillons hasards gaussiens indépendants soumis à un unique facteur de variation qualitatif et qu’on suppose n’agir que sur l'espérance (ex: la classe d'origine des élèves dans des notes à un examen). Les variances des lois parentes des échantillons sont toutes supposées égales. On veut tester l’hypothèse de l’influence du facteur considéré sur les espérances des lois parentes : l’hypothèse nulle correspond au cas où le facteur est sans influence, toutes les lois parentes sont de même espérance.\n",
    "\n",
    "L’idée va consister à décomposer la variance totale en la somme d’une variance inter-échantillon/inter-classe (variance expliquée) et d’une variance intra-échantillon/intra-classe (variance résiduelle). Cette décomposition générale est parfois appelée formule d’analyse de la variance.\n",
    "\n",
    "On peut montrer que la variance résiduelle suit un $\\chi_{n-k}^{2}$. Sous l’hypothèse nulle, on montre que les variances totales et expliquées suivent des $\\chi_{n-1}^{2}$ et $\\chi_{k-1}^{2}$. On en déduit l’indépendance des variances expliquée et résiduelle sous l’hypothèse nulle.\n",
    "\n",
    "On forme alors la statistique de test consistant du quotient de la variance expliquée (inter-classe) sur la variance résiduelle (intra-classe) qui sous l'hypothèse nulle suit une loi de Fisher $\\mathcal{F}(k-1, n-k)$.\n",
    "L’ANOVA est réputé assez robuste si les échantillons de sont pas strictement gaussiens dès que les effectifs sont suffisants. En revanche, elle est signalée beaucoup plus sensible aux violations de l’hypothèse d’homoscédasticité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sur le théorème de Cochran et deux de ses applications classiques\n",
    "\n",
    "## Théorème de Cochran \n",
    "Pour $k$ matrices $B_{k}$ de $\\mathcal{M}_{n}(\\mathbb{R})$ de rangs respectifs $r_{k}$ telles que $\\sum_{k}B_{k}=I_{n}$, si une des proposition suivante est vraie, alors les deux autres sont vraies: \n",
    "* $\\sum_{k}r_{k}=n$\n",
    "* Les $k$ variables aléatoires correspondant aux formes quadratiques $x^{\\top}B_{k}x$ sont indépendantes\n",
    "* Les $k$ variables aléatoires correspondant aux formes quadratiques $x^{\\top}B_{k}x$ suivent chacune un $\\chi_{r_{k}}^{2}$\n",
    "\n",
    "Ce théorème est très souvent présenté dans le cas particulier ou les matrices $B_{k}$ sont deux matrices de projection orthogonales. Ce cas correspond à celui des deux applications classiques du théorème présentées ci-dessous. \n",
    "\n",
    "## Application au problème de la régression linéaire\n",
    "On rappelle que pour $\\Pi_{F}$ un projecteur orthogonal sur un sous-espace vectoriel $F$, on a par le théorème de Pythagore pour tout vecteur $y$ de $\\mathbb{R}^{n}$ :\n",
    "\n",
    "$$\\|y\\|^{2} = \\| \\Pi_{F}(y) \\|^{2} + \\| \\Pi_{F \\perp}(y) \\|^{2}$$\n",
    "\n",
    "Le problème de la régression linéraire dans le contexte des moindres carrés se ramenant au problème de la projection orthogonale du vecteur aléatoire $Y$ à valeurs dans $\\mathbb{R}^{n}$ sur le sous-espace vectoriel engendré par les colonnes de $X$ (noté $Vect(X)$, $Vect(X)=F$), on peut écrire :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\|Y-\\overline{Y}\\|^{2} &= \\| \\Pi_{F}(Y-\\overline{Y}) \\|^{2} + \\| \\Pi_{F \\perp}(Y-\\overline{Y}) \\|^{2} \\\\\n",
    "&= \\| \\hat{Y} - \\Pi_{F}(\\overline{Y}) \\|^{2} + \\| (I_n - \\Pi_{F})(Y-\\overline{Y}) \\|^{2} \\\\\n",
    "&=\\| \\hat{Y} - \\overline{Y} \\|^{2} + \\| Y-\\overline{Y} - \\hat{Y}-\\overline{Y} \\|^{2} \\\\\n",
    "&= \\| \\hat{Y} - \\overline{Y} \\|^{2} + \\| Y - \\hat{Y} \\|^{2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Remarque : on rappelle que $\\overline{Y} \\in Vect(\\mathbb{1}_n) \\subset Vect(X)$\n",
    "\n",
    "La dernière formule correspond exactement à la formule d'analyse de la variance (décomposition de la variance totale en somme de la variance expliquée par le modèle et variance résiduelle). Conséquence de cette configuration géométrique particulière, les vecteurs aléatoires gaussiens $\\hat{Y} - \\overline{Y}$ et $Y - \\hat{Y}$ appartiennent à des sous-espaces vectoriels orthogonaux et sont donc indépendantes.\n",
    "\n",
    "On rappelle que $\\Pi_{F}$ est une projection orthogonale, or une matrice de projection orthogonale étant symétrique ($P^{\\top}=P$) et idempotente ($P^{2}=P$), on a pour tout vecteur $x$ de $\\mathbb{R}^{n}$ : \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x^{\\top}Px &= x^{\\top}P^{2}{x} \\\\\n",
    "&= x^{\\top}P^{\\top}P{x} \\\\\n",
    "&= (Px)^{\\top}Px \\\\\n",
    "&= \\|Px\\|^{2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Les espaces $F$ et $F^{\\perp}$ étant en somme directe on en déduit que la somme des rangs de $P_{F}$ et $P_{F \\perp}$ matrices canoniquement associées à $\\Pi_{F}$ et $\\Pi_{F \\perp}$ respectivement, est égale à $n$. On déduit alors du théorème de Cochran des propriétés sur les formes quadratiques $x^{\\top}Px$ : \n",
    "* Les variables aléatoires $x^{\\top}P_{F}x$ et $x^{\\top}P_{F^{\\perp}}x$ sont indépendantes.\n",
    "* $x^{\\top}P_{F}x \\sim \\chi_{dim(F)}^{2}$\n",
    "* $x^{\\top}P_{F^{\\perp}}x \\sim \\chi_{dim(F^{\\perp})}^{2}$\n",
    "\n",
    "Or dans le cas particulier où les matrices $P$ sont aussi des matrices de projection orthogonales, ces résultats se reformulent:\n",
    "* Les variables aléatoires $\\|P_{F}x\\|^{2}$ et $\\|P_{F^{\\perp}}x\\|^{2}$ sont indépendantes.\n",
    "* $\\|P_{F}x\\|^{2} \\sim \\chi_{dim(F)}^{2}$\n",
    "* $\\|P_{F^{\\perp}}x\\|^{2} \\sim \\chi_{dim(F^{\\perp})}^{2}$\n",
    "\n",
    "Plus concrètement, dans le cas d'une régression linéaire à $p$ *features* (sans colinéarité), $dim(F)=p+1$ (on a ici inclut la constante à l'ensemble des *features* via l'hypothèse $\\mathbb{1}_n \\in Vect(X)$) dont on déduit $dim(F^{\\perp})=n-(p+1)$.\n",
    "\n",
    "On en déduit immédiatement les lois suivies par les variables aléatoires $\\| \\hat{Y} - \\overline{Y} \\|^{2}$ et $\\| Y - \\hat{Y} \\|^{2}$ (respectivement $\\chi_{dim(F)}^{2} \\sim \\chi_{p+1}^{2}$ et $\\chi_{dim(F^{\\perp})}^{2} \\sim \\chi_{n-p-1}^{2}$) représentant les parts de variance expliquée et résiduelle du modèle. Ces dernières étant de plus indépendantes, on va pouvoir en formant leur quotient, obtenir une nouvelle variable aléatoire suivant une loi de Fisher. On a également du même coup obtenu la loi suivie par l'estimateur sans biais de la variance des résidus $\\sigma_{\\epsilon}$: $S^{2}=\\frac{1}{n-(p+1)}\\| Y - \\hat{Y} \\|^{2}$ qui est notamment utile pour les tests de significativité des coefficients du modèle.\n",
    "\n",
    "Remarques: \n",
    "* Le théorème de Cochran fait l'hypothèse d'un vecteur gaussien standard, pour être parfaitement rigoureux, il faut réduire, normaliser les quantités $\\| \\hat{Y} - \\overline{Y} \\|^{2}$ et $\\| Y - \\hat{Y} \\|^{2}$ avec $\\sigma^{2}$: c'est donc $\\frac{\\| \\hat{Y} - \\overline{Y} \\|^{2}}{\\sigma_{\\epsilon}^2}$ par exemple qui suit un $\\chi_{p+1}^{2}$ et non $\\| \\hat{Y} - \\overline{Y} \\|^{2}$.\n",
    "* Le fait qu'on travaille avec un vecteur gaussien standard provient d'une partie des hypothèses de la régression linéaire, à savoir des résidus supposés normalement distribués et de matrice de covariance proportionnelle à l'identité (pas de corrélation entre les résidus + homoscédasticité).\n",
    "\n",
    "## Application aux lois suivies par les estimateurs $\\overline{X}$ et $S^{2}$ dans le cas gaussien\n",
    "On considère le vecteur gaussien $X \\sim \\mathcal{N}(\\mu, \\sigma^{2} I_{n})$ et sa version centrée réduite $Y$. On remarque qu'en reprenant les résultats de la projection orthogonale appliquée à $F=Vect(\\mathbb{1}_n)$, on peut obtenir des résultats importants sur les estimateurs $\\overline{X}$ et $S^{2}$ (on rappelle que $\\overline{X}$ est le projeté orthogonal de $X$ sur $F=Vect(\\mathbb{1}_n)$). En reprenant le théorème de Pythagore, on a : \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\|Y\\|^{2} &= \\| \\Pi_{F}(Y) \\|^{2} + \\| \\Pi_{F \\perp}(Y) \\|^{2} \\\\\n",
    "&= \\| \\hat{Y}\\|^{2} + \\| (I_n - \\Pi_{F})(Y) \\|^{2} \\\\\n",
    "&= \\| \\hat{Y}\\|^{2} + \\| Y - \\hat{Y} \\|^{2} \\\\\n",
    "&= \\| \\overline{Y}\\|^{2} + \\| Y - \\overline{Y} \\|^{2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Cette dernière équation est équivalente à:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sum_{i} y_{i}^{2}&=n \\overline{y}^{2}+\\sum_{i}\\left(y_{i}-\\overline{y}\\right)^{2} \\\\\n",
    "\\Leftrightarrow \\sum_{i}\\left(\\frac{x_{i}-\\mu}{\\sigma}\\right)^{2}&=n\\left(\\frac{\\overline{x}-\\mu}{\\sigma}\\right)^{2}+\\sum_{i}\\left(\\frac{x_{i}-\\overline{x}}{\\sigma}\\right)^{2} \\\\\n",
    "\\Leftrightarrow \\sum_{i}\\left(\\frac{x_{i}-\\mu}{\\sigma}\\right)^{2}&=\\left(\\frac{\\overline{x}-\\mu}{\\sigma/\\sqrt{n}}\\right)^{2}+\\sum_{i}\\left(\\frac{x_{i}-\\overline{x}}{\\sigma}\\right)^{2} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "L'orthogalité des projections donne le premier résultat qui établit l'indépendance entre $(\\frac{\\overline{X}-\\mu}{\\sigma/\\sqrt{n}})$ et $\\frac{S \\sqrt{n}}{\\sigma}$. De cette propriété, on déduit que le rapport de ces deux variables aléatoires suit une loi de Student. \n",
    "\n",
    "Du théorème de Cochran on déduit le second résultat d'importance ($\\sum_{i} y_{i}^{2}$ étant une somme de carrés de $\\mathcal{N}(0,1)$, cette quantité suit un $\\chi_{n}^{2}$) : \n",
    "$$\\sum_{i}(\\frac{x-\\overline{x}}{\\sigma})^{2}=\\frac{n S^{2}}{\\sigma^{2}} \\sim \\chi_{n-1}^{2}$$\n",
    "\n",
    "L'espérance d'un khi-deux étant égale à son nombre de degrés de libertés, on retrouve immédiatement le résultat (indépendant de la loi suivie par $X$):\n",
    "$$\\mathbb{E}(S^{2})=\\frac{(n-1)}{n} \\sigma^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation\n",
    "##  Rappels de base de la théorie de l'estimation\n",
    "Soit $(X_1,\\dots,X_n)$ un $n$-échantillon de $X$ variable aléatoire, $(x_1,\\dots,x_n)$ étant une réalisation de l'échantillon. On appelle statistique relative à l'échantillon toute variable aléatoire $T = \\Phi(X_1,\\dots,X_n)$. La moyenne et la variance empirique de même que que les inf, sup et quantiles sont par exemple des statistiques pour un $n$-échantillon.\n",
    "\n",
    "### Statistique vs. estimateur\n",
    "Soit $X$ variable aléatoire de densité de probabilité $f(x, \\theta)$, $\\theta$ inconnu et dont le domaine de définition ne dépend pas de $\\theta$. Soit $\\Phi(X_1,\\dots,X_n)$ une statistique relative à un échantillon indépendante de $\\theta$ et à valeurs dans le domaine de définition de $\\theta$. $T = \\Phi(X_1,\\dots,X_n)$ est un estimateur pour $\\theta$. \n",
    "\n",
    "Cette définition est finalement extrêmement large : un nombre très important de statistiques peuvent ainsi prétendre à être des estimateurs pour $\\theta$, mais lesquels sont du meilleur intérêt lorsqu'il s'agira d'approximer $\\theta$ par la valeur prise par l'estimateur qu'on aura choisi, sur une réalisation de notre échantillon ? On recherche donc en général parmi tous les estimateurs possibles pour $\\theta$:\n",
    "* Un estimateur sans biais: $\\mathbb{E}(T) = \\theta$\n",
    "* Un estimateur de variance $Var(T)$ minimale. Les valeurs prises par l'estimateur nous sembleront d'autant plus fiables qu'on les sait être les moins dispersées possibles.\n",
    "\n",
    "On va donc chercher à construire pour $\\theta$, un estimateur $T$ sans biais et de variance minimale (ESBVM). On peut montrer que la variance d'un ESB connait une borne inférieure qui ne peut être atteinte que si la loi de l'échantillon prend une forme particulière (inégalité Fréchet-Darmois-Cramer-Rao - FDCR). Si l'ESBVM existe, alors il est unique.\n",
    "\n",
    "On a pas mentionné jusqu'à présent la taille $n$ de l'échantillon alors qu'il peut paraître intéressant que la précision d'un estimateur s'accroisse quand la taille de l'échantillon augmente. On parle ainsi :\n",
    "* D'estimateur consistant ou convergent lorsque $T_n$ converge en probabilité vers $\\theta$ lorsque $n \\to +\\infty$.\n",
    "* D'estimateur fortement convergent lorsque $T_n$ converge presque sûrement vers $\\theta$ lorsque $n \\to +\\infty$.\n",
    "\n",
    "Remarque : Si $Var(T_n) \\xrightarrow[n \\to +\\infty]{} 0$ alors $T_n$ converge en moyenne quadratique et donc en probabilité vers $\\theta$.\n",
    "\n",
    "Toutefois cette propriété de convergence nous intéresse assez peu : la convergence peut être lente et augmenter la taille de l'échantillon peut être couteux. On travaille souvent à taille d'échantillon constante. Ce qui nous intéresse est de travailler avec le meilleur estimateur à notre disposition pour une taille d'échantillon donnée même s'il est rassurant de savoir que la précision de notre estimateur sera d'autant plus faible que la taille de notre échantillon sera grande.\n",
    "\n",
    "## Moyenne et fréquence empirique\n",
    "Si on suppose que la loi suivie par $X$ admet une espérance $\\mu$ et une variance $\\sigma^2$, alors la moyenne empirique $\\bar{X}_n=\\frac{1}{n}\\sum_{i}X_i$ comme estimateur de l'espérance est:\n",
    "  - Sans biais\n",
    "  - De variance $\\sigma^2/n$, la précision de l'estimateur ne s'accroit qu'en $\\sqrt{n}$.\n",
    "\n",
    "Remarques:\n",
    "* Ces résultats ne sont valables que pour un échantillon constitué par tirage avec remise dans une population infinie, les v.a. $X_i$ constituant l'échantillon étant alors i.i.d.. Dans un tirage sans remise dans une population finie, les $X_i$ ne sont plus indépendantes. On montre que les correction apportées sont d'autant plus sensibles que la taille d'ensemble $N$ de la population est faible et/ou que le taux de sondage $n/N$ est élevé avec $n$ la taille de l'échantillon.\n",
    "* On peut montrer que la moyenne empirique est l'ESBVM pour l'espérance pour un grand nombre de lois suivies par $X$ (cf. inégalité FCDR).\n",
    "\n",
    "### Fréquence empirique\n",
    "La fréquence empirique $F$ correspond à la moyenne empirique des résultats de $n$ réalisations indépendantes d’une épreuve de Bernoulli de paramètre $p$. $F$ est un estimateur du paramètre $p$ : \n",
    "* $nF$ est exactement modélisé par une $\\mathcal{B}(n,p)$. On en déduit immédiatement que l’espérance de $F$ est égale à $p$ : $F$ est un estimateur sans biais de $p$. La variance de l’estimateur est égale à $p(1-p)/n$. \n",
    "Comme pour la moyenne empirique, la loi des grands nombres nous assure de la consistance de l’estimateur $F$ qui ici converge en probabilité vers $p$. \n",
    "* On peut facilement montrer que la fréquence empirique $F$ est aussi l’estimateur du maximum de vraisemblance pour $p$. \n",
    "* $nF$ suivant une loi binomiale, on profite également de ses propriétés asymptotiques comme la convergence en loi vers une loi normale $\\mathcal{N}(np, \\sqrt{npq})$.\n",
    "\n",
    "## Variance empirique\n",
    "Si on suppose que la loi suivie par $X$ admet une espérance $\\mu$ et une variance $\\sigma^2$, alors la variance empirique $\\frac{1}{n}\\sum_{i}(X_i-\\bar{X}_n)^2$ comme estimateur de la variance est:\n",
    "  - Biaisé : $E(T) = \\frac{n-1}{n}\\sigma^2$\n",
    "  - La variance de l'estimateur est de forme plus complexe mais approximable pas une quantité en $O(1/n)$ quand $n$ devient grand.\n",
    "  - La loi forte des grands nombres permet de montrer facilement que bien que biaisée, la variance empirique converge presque sûrement vers la variance $\\sigma^2$ de $X$ quand la taille de l'échantillon tend vers l'infini.\n",
    "\n",
    "Remarque : De même que pour la moyenne empirique, si le tirage de l'échantillon se fait dans une population finie et sans remise, des corrections similaires viennent s'appliquer à l'espérance de l'estimateur de la variance (entre autres)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance, covariance, matrice de covariance, d'auto-covariance, de cross-covariance \n",
    "La matrice de variance-covariance appelée aussi indifféremment matrice de covariance d'un vecteur aléatoire $X$ de $\\mathbb{R}^{n}$ est une matrice carrée de $\\mathcal{M}_{n}(\\mathbb{R})$ rassemblant les $n$ variances de chaque composantes et les $n(n-1)/2$ covariances des composantes de $X$. On la note $\\Sigma$ (parfois $\\Sigma_{XX}$).  \n",
    "\n",
    "Remarque : La matrice de corrélation correspond à la matrice de covariance des mêmes variables/composantes mais centrées (normalisées). Attention, les deux matrices n'ont cependant pas dans le cas général les mêmes valeurs propres.\n",
    "\n",
    "La matrice de covariance se définit simplement comme la matrice de coefficients $\\sigma_{i, j}=Cov(x_i, x_j)$ avec $x_i$ et $x_j$ variables aléatoires réelles respectivement $i^{e}$ et $j^{e}$ composantes de $X$. De cette définition, on déduit immédiatement que $\\Sigma$ est une matrice carrée de $\\mathcal{M}_{n}(\\mathbb{R})$. Plus généralement, on peut définir pour $X$ et $Y$ deux vecteurs aléatoires de $\\mathbb{R}^{n}$ et $\\mathbb{R}^{m}$ respectivement la matrice de coefficients $\\sigma_{i, j}=Cov(x_i, y_j)$, définition dont on déduit qu'elle est une matrice de $\\mathcal{M}_{n, m}(\\mathbb{R})$. On parle pour la première de matrice d'auto-covariance $\\Sigma_{XX}$ qui est toujours carrée, et pour la seconde de matrice de cross-covariance $\\Sigma_{XY}$ qui est rectangulaire dans le cas général. On peut donner deux définitions équivalentes des matrices d'auto- et de cross-covariance: \n",
    "* $\\Sigma_{XX} = \\mathbb{E}(((X -\\mathbb{E}(X))(X -\\mathbb{E}(X))^{\\top}) = \\mathbb{E}(X.X^{\\top}) -\\mathbb{E}(X).\\mathbb{E}(X)^{\\top}$\n",
    "* $\\Sigma_{XY} = \\mathbb{E}(((X -\\mathbb{E}(X))(Y -\\mathbb{E}(Y))^{\\top}) = \\mathbb{E}(X.Y^{\\top}) -\\mathbb{E}(X).\\mathbb{E}(Y)^{\\top}$\n",
    "\n",
    "On déduit immédiatemment de la définition de $\\Sigma_{XX}$ qu'elle a pour propriété d'être symétrique définie semi positive réelle. Cela a notamment pour conséquences : \n",
    "* Des valeurs propres dans $\\mathbb{R}^{+}$\n",
    "* $\\forall x \\in \\mathbb{R}^{n}, x^{\\top}\\Sigma x \\geq 0$ (voir plus bas).\n",
    "* $\\exists N \\in \\mathcal{M}_{n}(\\mathbb{R}), \\Sigma=N^{\\top}N$ (conséquence du théorème spectral et de la diagonalisabilité en base orthonormée).\n",
    "\n",
    "Toute matrice symétrique réelle pouvant être canoniquement associée à une forme quadratique, on en déduit qu'on peut donner une interprétation de la matrice de covariance en termes de forme quadratique. \n",
    "\n",
    "Soit $c$ un vecteur constant de $\\mathbb{R}^{n}$, on montre facilement que la variance de la variable aléatoire $c^{\\top}X$ (combinaison linéaire des composantes de $X$) à valeurs dans $\\mathbb{R}$ est donnée par $c^{\\top}\\Sigma_{XX}c$. On en déduit que la forme quadratique canoniquement associée à $\\Sigma_{XX}$, $u \\mapsto u^{\\top}\\Sigma_{XX}u$ est l'application qui à un vecteur $u$ de $\\mathbb{R}^{n}$ associe la variance de la variable aléatoire réelle $u^{\\top}X$.\n",
    "\n",
    "Remarque: On montre facilement que $\\mathbb{E}(c^{\\top}X) = c^{\\top}\\mathbb{E}(X)$. Plus généralement, pour $X$ matrice aléatoire à valeurs dans $\\mathcal{M}_{m,n}(\\mathbb{R})$, $a \\in \\mathcal{M}_{p,m}(\\mathbb{R})$ et $b \\in \\mathcal{M}_{n,q}(\\mathbb{R})$ constants, on a: $\\mathbb{E}(aX) = a\\mathbb{E}(X)$ et $\\mathbb{E}(Xb) = \\mathbb{E}(X)b$.\n",
    "\n",
    "Remarque : $\\Sigma_{XX}$ étant diagonalisable en base orthonormée et à valeurs propres positives, on montre facilement qu'il existe une matrice de changement de base orthogonale $P$ et $D$ diagonale de $\\mathcal{M}_{n}(\\mathbb{R})$ telles que $\\Sigma_{XX} = P^{\\top}N^{\\top}NP$. D'où $c^{\\top}\\Sigma_{XX}c = c^{\\top}P^{\\top}N^{\\top}NPc = \\|NPc\\|^{2}$. On peut donc ainsi élaborer une interprétation de la variance en termes de mesure, de norme d'un vecteur. \n",
    "\n",
    "$\\Sigma_{XX}$ peut aussi s'interpréter comme la matrice d'une plus générale forme bilinéaire symétrique $(u, v) \\mapsto u^{\\top}\\Sigma_{XX}v$ qui à un couple de vecteurs (constants) $(u,v)$ de $\\mathbb{R}^{n}$ associe la covariance entre les variable aléatoires rélles $u^{\\top}X$ et $v^{\\top}X$. Cette forme bilinéaire étant par les propriétés de la variance définie positive, elle définit également un produit scalaire sur $\\mathbb{R}^{n}$. Deux vecteur $u$ et $v$ (constants) de $\\mathbb{R}^{n}$ sont ainsi définis comme orthogonaux au sens de ce produit scalaire si et seulement si la covariance/corrélation entre les variables aléatoires réelles $u^{\\top}X$ et $v^{\\top}X$ est nulle. \n",
    "\n",
    "Remarque: $\\Sigma_{XX}$ étant une matrice de forme bilinéaire, elle en possède la propriété suivante: $\\forall (u, v) \\in (\\mathbb{R}^{n})^{2}, u^{\\top}\\Sigma_{XX}v = v^{\\top}\\Sigma_{XX}u$. Cette propriété découle du fait qu'une forme bilinéaire est à valeurs dans $\\mathbb{R}$ et que la transposée d'un scalaire est égale à ce même scalaire.\n",
    "\n",
    "On déduit de ce qui précède les propriétés classiques et générales de la variance et de la covariance:\n",
    "* $Cov(\\sum_{i}a_{i}X_{i}, \\sum_{j}b_{j}Y_{j}) = Cov(a^{\\top}X, b^{\\top}Y) = \\sum_{i}\\sum_{j}a_{i}b_{j}Cov(X_{i}, Y_{j})$\n",
    "* $Var(\\sum_{i}a_{i}X_{i}) = Var(a^{\\top}X) = Cov(a^{\\top}X, a^{\\top}Y) = \\sum_{i}a_{i}^{2}Var(X_{i}) + 2\\sum_{i<j}a_{i}a_{j}Cov(X_{i}, X_{j})$\n",
    "\n",
    "On rappelle à toutes fins utiles la définition et propriété suivante pour deux variables aléatoire réelles (scalaires) $X$ et $Y$:\n",
    "* $cov(X, Y)=\\mathbb{E}((X -\\mathbb{E}(X))(Y -\\mathbb{E}(Y))) = \\mathbb{E}(X.Y) -\\mathbb{E}(X).\\mathbb{E}(Y)$ \n",
    "* L'indépendance de $X$ et $Y$ implique la nullité de leur covariance, la réciproque étant fausse.\n",
    "\n",
    "Dans la suite de cette partie, les vecteurs aléatoires seront notés en gras (par exemple: $\\mathbf{x}$) afin de ne pas créer d'ambiguités entre vecteurs et matrices.\n",
    "\n",
    "Plus généralement, prenons $A \\in \\mathcal{M}_{m, n}(\\mathbb{R})$ et $B \\in \\mathcal{M}_{l, n}(\\mathbb{R})$ et considérons les vecteurs aléatoires $A\\mathbf{x}$ et $B\\mathbf{x}$ de $\\mathbb{R}^{m}$ et $\\mathbb{R}^{l}$ respectivement, résultats de la transformation linéaire de $\\mathbf{x}$ vecteur aléatoire de $\\mathbb{R}^{n}$ par $A$ et $B$ respectivement.\n",
    "\n",
    "On peut montrer que la matrice de cross-covariance entre $A\\mathbf{x}$ et $B\\mathbf{x}$ est donnée par $A\\Sigma_{xx}B^{\\top} = \\Sigma_{AxBx}$. Cette matrice appartient à $\\mathcal{M}_{m, l}(\\mathbb{R})$. En particulier, la matrice d'auto-covariance d'une transformation linéaire $A\\mathbf{x}$ de $\\mathbf{x}$ avec $A \\in \\mathcal{M}_{m, n}(\\mathbb{R})$ est égale à $A\\Sigma_{xx}A^{\\top} = \\Sigma_{AxAx}$. Cette matrice appartient à $\\mathcal{M}_{m}(\\mathbb{R})$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
