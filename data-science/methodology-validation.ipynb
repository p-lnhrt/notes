{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection : Evaluer la performance de plusieurs modèles (différents modèles d'une même famille, différents types de modèles, différentes techniques de régularisation pour un même modèle, etc.) dans le but de choisir le meilleur d'entre eux.\n",
    "\n",
    "Model assessment : Estimer la test error / generalization error / out-of-sample error du meilleur modèle sur un jeu de données indépendant lors de l'étape de model selection.\n",
    "\n",
    "\n",
    "On va en général découper son jeu de données en trois sous-ensembles disjoints:\n",
    "* Training set servant à l'estimation des paramètres de chaque modèle.\n",
    "* Validation set servant à l'estimation d'un critère de sélection lors de l'étape de model selection.\n",
    "* Test set servant à l'estimation de la generalization error du meilleur modèle choisi.\n",
    "\n",
    "Dans les cas où on a peu de données à disposition, on ne peut pas se permettre un tel découpage et des techniques (relevant de l'échantillonnage) permettent de faire une utilisation optimale des données à notre disposition pour chacune de ces étapes (cross-validation).\n",
    " \n",
    "Dans l'idéal, on souhaiterait pour un modèle estimé $\\hat{g}$ sur un training set $D_{\\text {train}}$, pouvoir estimer l'erreur pour l'ensemble des points $(x, y)$ de la distribution $P$ des valeurs possibles et en dériver une estimation de la test error pour $\\hat{g}$: <br>\n",
    "\n",
    "$E_{\\text {train}}\\left(D_{\\text {train}}, n_{\\text {train}}\\right)=\\mathbb{E}_{P}\\left[L\\left(y, \\hat{g}\\left(x, D_{\\text {train}}\\right)\\right)\\right]$\n",
    "\n",
    "On remarque que cette estimation à une dépendance:\n",
    "\n",
    "* Envers le training set $D_{\\text {train}}$ via le jeu de paramètres estimés. On peut s'en abstraire en moyennant sur l'ensemble des trainings sets (ce qui peut aussi se voir comme le choix d'un modèle moyen correspondant à la moyenne des modèles estimés sur chacun des training sets).\n",
    "\n",
    "* Envers la taille des training sets : une estimation du modèle sur des trainings sets de petite taille aboutira typiquement à une surestimaiton de la test error (d'où l'utilisation de learning curves).\n",
    "\n",
    "* On remarque aussi qu'on a moyenné par rapport à $P$ qu'on ne connait pas forcément. Cela peut être d'autant plus impactant que nos datasets sont de taille finie. D'où l'importance des méthodes permettant d'approcher $P$ afin d'estimer l'erreur de généralisation (cross-validation).\n",
    "\n",
    "On appelle l'erreur moyennée sur l'ensemble des training sets $E_{\\text {test}}\\left(n_{\\text {train}}\\right)=\\mathbb{E}_{D_{\\text {train}}} \\mathbb{E}_{p}\\left[L\\left(y, \\hat{g}\\left(x, D_{\\text {train}}\\right)\\right)\\right]$ expected generalisation error.\n",
    "\n",
    "On peut montrer que l'erreur (de test) peut se décomposer en trois termes correspondant à:\n",
    "\n",
    "* L'écart entre la réponse et son espérance conditionnelle (qui est en fait la fonction à approcher et qu'on peut aussi appeler \"vrai\" modèle (true model), souvent inconnue), il s'agit de variabilité interne aux données, aucune référence à un modèle n'apparait dans ce terme. On parle pour ce terme de bruit ou d'erreur irréductible (irreductible error) puisqu'elle ne dépend d'aucun paramètre sous notre contrôle (modèle, training set).\n",
    "\n",
    "* L'écart entre notre modèle (moyenné sur tous les training sets) et l'espérance conditionnelle (correspondant à la fonction cible à approcher). Il s'agit de l'écart de notre modèle au \"vrai\" modèle appelé aussi biais.\n",
    "\n",
    "* L'écart entre un modèle entrainé sur un training set particulier et le modèle moyenné sur l'ensemble des training sets. Ce terme quantifie la variabilité des modèles face aux changements de training sets. On parle aussi de variance.\n",
    "\n",
    "Remarque : la démonstration de cette erreur se base souvent sur l'hypothèse qu'on travaille avec une MSE. Les différents termes peuvent ne pas aussi bien se séparer avec d'autres types d'erreur mais l'idée reste valable.\n",
    "\n",
    "Remarque importante : en pratique la training/test error est calculée comme une estimation sur un échantillon et non pour la population (moyenne empirique vs espérance). Ces erreurs sont donc toujours évaluées par rapport à un dataset donné. C'est aussi pour cette raison qu'on aime avoir sur les graphiques les représentant (complexity et learning curves) les intervalles de confiance correspondant.\n",
    "\n",
    "Remarque : la test error est aussi appelée out-of-sample error, terme venant insister sur le fait que l'estimation se fait sur un échantillon distinct de celui utilisé pour l'estimation des paramètres (la training error étant aussi appelée in-sample error: on évalue l'erreur faite sur le même échantillon que celui utilisé pour l'estimation).\n",
    "\n",
    "Ecart entre test (out-of-sample) et train (in-sample) error : provient en partie du fait que les points ne seront pas exactement les même. Dans chaque cas, la distribution des observations dans les test et train sets peuvent être différentes de la distribution théorique $P$. On parle aussi de in-sample / out-sample data pour les train/test sets.\n",
    "\n",
    "Quand à training set size donnée, si la test error de notre modèle est :\n",
    "\n",
    "* Plus élevée que celle d'un modèle moins complexe, on dit que notre modèle overfit.\n",
    "\n",
    "* Plus élevée que celle d'un modèle plus complexe, on dit que notre modèle underfit.\n",
    "\n",
    "On choisit pour modèle optimal, celui qui minimise la test error.\n",
    "\n",
    "Afin de diagnostiquer ces phénomènes et de choisir le meilleur modèle, on peut tracer à training set size donnée les courbes de complexité donnant $E_{\\text {train}}$ et $E_{\\text {test}}$ en fonction de la complexité du modèle (ou plus largement d'un hyper-paramètre qu'on souhaite ajuster).\n",
    "\n",
    "Il faut ici distinguer la capacité de généralisation du modèle à complexité donnée (même pour la complexité minimale, le modèle peut mal généraliser) et qui correspond à l'écart entre $E_{\\text {train}}$ et $E_{\\text {test}}$ qu'on souhaite le plus faible possible. Une des difficultés rencontrées en pratique est de s'entendre sur la valeur maximale de l'écart qu'on considère comme raisonnable. Remarque : cet écart tend théoriquement vers zéro quand on fait tendre $n_{\\text {train}}$ et $n_{\\text {test}}$ vers l'infini.\n",
    "\n",
    "L'expected generalization error est la mesure donnant le plus d'information quant à la capacité de généralisation d'un modèle et apparaît comme la plus indiquée pour procéder à la sélection de modèles. Cependant, la calculer n'est pas toujours faisable. Des approches alternatives visent alors à estimer des grandeurs différentes de l'expected generalization error mais aboutissant à une classement des modèles similaire. Les phases de model selection et de model assessment peuvent reposent ainsi en général sur des approches différentes (tout comme elles peuvent toutes les deux reposer sur expected generalization error).\n",
    "\n",
    "Par exemple en régression, on peut par exemple utiliser la part de variance expliquée (appelée aussi coefficient de détermination) $R^{2}$ pour la hiérarchisation des modèles. Le problème du $R^{2}$ est qu'un modèle plus complexe aura toujours un meilleur score qu'un modèle moins complexe. On préfère alors utiliser un $R^{2}$ corrigé de la taille du dataset et de la complexité du modèle : $R_{a d j}^{2}=1-\\frac{\\operatorname{SSE}(n-1)}{\\operatorname{SST}(n-p)}$.\n",
    "\n",
    "D'autres approches ($C_p$ de Mallow, AIC, BIC) partent d'une décomposition de la test error en la somme de la training error et d'un terme positif appelé optimisme (qui augmente avec la complexité du modèle et le bruit mais diminue avec la taille du dataset). Cette décomposition n'est pas forcément aisément calculable pour tous les modèles (il existe par exemple une forme exacte pour la régression linéaire). AIC et BIC semblent privilégier la negative log-likelihood pour l'erreur. Attention: les critères tels que les AIC/BIC n'ont vraisemblablement de sens que pour certaines familles de modèles, à utiliser avec prudence.\n",
    "\n",
    "Les différents termes composant ces critères traduisent aussi un compromis entre goodness of fit (MSE, negative log-likelihood) et complexité du modèle qui permettent en général à la courbe de complexité de passer par un minimum.\n",
    "\n",
    "Exemple d'algorithmes de sélection de modèle (meilleur compromis complexité-performance):\n",
    "* Forward stepwise selection : Pour un maximum de p paramètres (features) commencer par fitter les p modèles à un paramètre, on choisit le meilleur suivant le critère retenu (AIC, BIC, expected test error, etc.). On fit ensuite les p-1 modèles à deux paramètres où on a ajouté au modèle précédant un des paramètres non-choisis à l'étape précédente. On choisit le meilleur, etc. Des p modèles retenus, on choisit le meilleur suivant le critère adopté (on peut aussi arrêter l'algorithme si on voit que le critère ne fait que remonter au fur et à mesure qu'on ajoute des paramètres).\n",
    "* Backward stepwise selection : Même principe mais en partant du modèle à p paramètres à partir duquel on calcule la performance de tous les modèles à p-1 paramètres etc.\n",
    "\n",
    "Remarque :\n",
    "* La notion de paramètres correspond ici au nombre de features, la démarche présentée ci-dessus ne concerne pas l'optimisation d'hyper-paramètres. Ces algorithmes sont d'ailleurs plus souvent présentés aux cotés d'autres techniques de sélection de features (ex: régularisation).\n",
    "\n",
    "* Les deux algorithmes précédents sont gloutons (greedy) : ils choisissent à chaque étape un minimum local, démarche qui ne garantit pas d'aboutir à un mimimum global.\n",
    "\n",
    "Cross-validation\n",
    "\n",
    "A l'avantage de s'appliquer à n'importe quel type de modèle (qu'il soit paramétrique ou non par exemple). Conceptuellement, il s'agit d'une méthode de re-échantillonage qu'on applique uniquement au training set (le test set servant au model assessment restant isolé). Permet sans doute de diminuer la variance dans l'estimation des différentes erreurs (attention cependant à ce que chacune des folds ne soit pas trop petites), on calcule en effet une training et validation error moyennée sur l'ensemble des folds.\n",
    "\n",
    "Avantages:\n",
    "* Donne une estimation directe de l'expected test error (contrairement aux AIC, BIC, etc.).\n",
    "* Aucune hypothèse sur le modèle (contrairement aux AIC, BIC, etc.).\n",
    "\n",
    "Inconvénients:\n",
    "* Choix du nombre de folds (que se passe-t-il si trop grand ? si trop petit ?\n",
    "* Temps de calcul, puisque le modèle doit être fitté k fois.\n",
    "\n",
    "LOOCV pour les cas avec peu de données ? Est-elle vraiment utilisée car elle est souvent mentionnée..\n",
    "\n",
    "Estimation de la test error : on prend le jeu de paramètre jugé le meilleur à la validation, fit les k modèles correspondant et l'expected test error correspond à la moyenne des erreurs de chacun des k modèles sur le test set (?).\n",
    "\n",
    "Learning curves :\n",
    "\n",
    "Utiles au diagnostic et à chaque diagnostic correspondent de potentielles solutions. On se place à complexité constante et faisons varier la taille du training set (comme on l'a vu plus haut, les tailles d'échantillon étant finies, nos estimations de l'erreur en dépendent). Ces courbes visent :\n",
    "\n",
    "* A établir un diagnostic quand à la décomposition en bias/variance de l'erreur. On retient souvent que pour les cas :\n",
    "    * High bias/low variance : Les erreurs sont stabilisées à un niveau élevé. Augmenter la taille de l'échantillon ne sert à rien. Mieux vaut essayer d'augmenter la complexité du modèle (ajout de features, boosting, par exemple).\n",
    "    * Low  bias/high variance : La training error est très faible mais l'écart avec la test error est élevé. Augmenter la taille de l'échantillon peut aider (le biais étant faible, on finira par converger vers un modèle qui performe de façon satisfaisante, pour rappel l'écart tend vers zéro (OU PLUTOT LE BIAIS ?) quand n tend vers l'infini). On peut aussi chercher à réduire la complexité du modèle (régularisation, bagging).\n",
    "* A estimer la taille minimale du dataset à partir de laquelle le biais dans l'estimation des erreurs est minimal. Ainsi, si on constate que les erreurs ne sont pas stabilisées pour notre taille de dataset, on tirera des bénéfices à augmenter la taille de celui-ci si cela est possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda Python 3.6.12",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
