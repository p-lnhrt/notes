{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "Les *support vector machines* (SVM) sont d'abord des classicateurs (même s'ils peuvent être adaptés à la régression) faisant partie de la famille des classificateurs à vaste marge (*large-margin classifiers*). Un SVM va ainsi chercher à apprendre une fonction $f: \\mathcal{X} \\rightarrow \\mathbb{R}$ avec $\\mathcal{X}$ l'espace des *features*. La frontière de décision séparant l'espace en deux correspond au lieu des points $x \\in \\mathcal{X}$ tels que $f(x)=0$, cette frontière étant complétée par deux marges $f(x)=\\pm1$ (par convention), l'objectif étant que cette marge soit la plus large possible et qu'aucun point ne s'y trouve. Le critère de décision est $sgn(f(x))$, un point étant alors classé avec d'autant plus de certitude que $f(x)$ est positive (ou négative).\n",
    "\n",
    "Remarque: Aux problèmes de la famille des *large margin classifiers* correspond une famille spécifique de *loss functions* fonction d'une quantité appelée la marge et égale à $y.f(x)$. Dans le cas du SVM en classification, les *loss function* correspondant au problème sont appelées *hinge loss* et *squared hinge loss*. \n",
    "\n",
    "Dans le cas le plus simple (*sans kernel*), les SVM font l'hypothèse que la frontière de décision est un hyperplan affine de $\\mathcal{X}=\\mathbb{R}^{n}$. Il peut toutefois exister une infinité d'hyperplans satisfaisant cette exigence, un SVM va donc chercher à trouver l'hyperplan séparateur le plus éloigné des observations données à l'entrainement, l'hyperplan de marge maximale. Cet hyperplan sera a priori celui de tous les hyperplans séparateurs qui aura la meilleure performance en généralisation.\n",
    "\n",
    "## Cas linéaire séparable (*hard-margin classifier*)\n",
    "Afin de pouvoir poser le problème, on se place dans le cas le plus simple et sans doute le plus improbable en pratique: il existe au moins un hyperplan affine permettant de parfaitement séparer nos données.\n",
    "\n",
    "Soit un hyperplan affine $\\mathcal{H}$ passant par $p \\in \\mathbb{R}^{n}$ et de normale $w \\in \\mathbb{R}^{n}$. Cet hyperplan peut se définir comme le lieu des points $x$ de $\\mathbb{R}^{n}$ satisfaisant: \n",
    "\n",
    "$$(x-p)^{\\top}.w=0$$\n",
    "\n",
    "La distance d'un point quelconque $a \\in \\mathbb{R}^{n}$ à $\\mathcal{H}$ s'écrit:\n",
    "\n",
    "$$d(a, \\mathcal{H}) = \\frac{|(a-p)^{\\top}.w|}{\\|w\\|}$$\n",
    "\n",
    "Ou pour retrouver des notations plus fréquentes, on pose $b = -p^{\\top}.w \\in \\mathbb{R}$ et la distance s'exprime: \n",
    "\n",
    "$$d(a, \\mathcal{H}) = \\frac{|a^{\\top}.w + b|}{\\|w\\|}$$\n",
    "\n",
    "Remarque: L'unité dans laquelle s'exprime cette distance est $\\frac{1}{\\|w\\|}$.\n",
    "\n",
    "En prenant nos labels $y_{i}$ égaux à $1$ ou $-1$, on peut écrire la contrainte de séparabilité (tous les points d'un même label doivent se trouver du même côté de l'hyperplan séparateur) pour chaque point $x_{i}$ sous la forme suivante: \n",
    "\n",
    "$$y_{i}.(x_{i}^{\\top}.w + b) \\geq 0$$\n",
    "\n",
    "L'hyperplan affine d'équation $x^{\\top}.w + b = 0$ divise en effet l'espace en deux régions caractérisées par les inéquations $x^{\\top}.w + b \\lt 0$ et $x^{\\top}.w + b \\gt 0$.\n",
    "\n",
    "La recherche de l'hyperplan séparateur de marge maximale correspond donc à la résolution du problème d'optimisation:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\max_{w, b} \\min_{i}} & {\\frac{|x_{i}^{\\top}.w + b|}{\\|w\\|}} \\\\ {\\text { subject to }} & {y_{i}.(x_{i}^{\\top}.w + b) \\geq 0} \\end{array}\n",
    "$$\n",
    "\n",
    "L'inconvénient de cette formulation est que ce problème est très difficile à résoudre tel quel (d'ailleurs d'autres formulations naives du problème arrivent aux mêmes problèmes insolubles). Deux observations amènent à considérablement simplifier le problème: \n",
    "* L'hyperplan séparateur de marge maximale se situe à égale distance des points de la classe positive les plus proches du plan et des points de la classe négative les plus proches du plan.\n",
    "* On choisit $w$ de sorte que ces points des plus proches de l'hyperplan séparateur (appelés vecteurs de support) se placent sur l'hyperplan affine d'équation $x^{\\top}.w + b = 1$ pour ceux de la classe positive, et sur celui d'équation $x^{\\top}.w + b = -1$ pour ceux de la classe négative. La distance de chacun des ces hyperplans à l'hyperplan séparateur est donc $\\frac{1}{\\|w\\|}$ et la marge vaut $\\frac{2}{\\|w\\|}$. L'ensemble des membres de la classe positive respectivement négative doivent alors de trouver dans le demi-espace $x^{\\top}.w + b \\geq 1$ respectivement $x^{\\top}.w + b \\leq -1$.\n",
    "\n",
    "On voit aussi mieux la double influence du choix de $w$ sur le problème: \n",
    "* Choisir $w$ c'est choisir la normale à l'hyperplan séparateur et donc la direction de celui-ci.\n",
    "* Pour une direction donnée, faire varier la norme de $w$, c'est faire varier la marge: elle est d'autant plus élevée que la norme de $w$ est petite. $w$ n'est donc pas pris unitaire, mais sa norme est choisie de façon à ce que la marge soit égale à  $\\frac{2}{\\|w\\|}$.\n",
    "\n",
    "Le problème d'optimisation peut ainsi se reformuler:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\max_{w, b}} & {\\frac{2}{\\|w\\|}} \\\\ {\\text { subject to }} & {y_{i}.(x_{i}^{\\top}.w + b) \\geq 1}\\end{array}\n",
    "$$\n",
    "\n",
    "Remarques : \n",
    "* **Noter que les contraintes ont légèrement changé**. Toutefois, la fonction de décision ne change pas: on attribue une classe en fonctions de sa position par rapport à l'hyperplan $x^{\\top}.w + b = 0$.\n",
    "* La distance d'un point à $x^{\\top}.w + b = 0$ nous donne une idée de la \"certitude\" avec laquelle un point a été classifié (typiquement retourné par méthode `decision_function` dans `sklearn`).\n",
    "\n",
    "Ou de façon équivalente :\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\min_{w, b}} & {\\frac{\\|w\\|}{2}} \\\\ {\\text { subject to }} & {y_{i}.(x_{i}^{\\top}.w + b) \\geq 1}\\end{array}\n",
    "$$\n",
    "\n",
    "Ou encore pour une résolution plus aisée: \n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\min_{w, b}} & {\\frac{\\|w\\|^{2}}{2}=\\frac{1}{2}w^{\\top}w} \\\\ {\\text { subject to }} & {y_{i}.(x_{i}^{\\top}.w + b) \\geq 1}\\end{array}\n",
    "$$\n",
    "\n",
    "On retombe donc sur un simple problème d'optimisation sous contraintes, la fonction à optimiser étant convexe et les contraintes affines.\n",
    "\n",
    "## Cas linéaire non séparable (*soft-margin classifier*)\n",
    "Il est fortement problable qu'il ne soit pas possible de trouver un hyperplan séparateur pour les données à disposition (présence d'*outliers*, erreurs de labellisation, etc.). On va donc introduire de nouvelles variables (parfois appelées *slack variables*) permettant à un point de ne pas se trouver du bon côté de l'hyperplan $x_{i}^{\\top}.w+b = \\pm 1$ tout en pénalisant cette situation. **Attention**, se situer du mauvais côté de l'hyperplan délimitant la marge n'implique pas forcément une erreur de classification: on peut encore être classifié correctement mais se trouver à l'intérieur de la marge. \n",
    "\n",
    "Remarque : On passe d'un *hard* à un *soft classifier*.\n",
    "\n",
    "Cette possibilité va avoir un effet régularisant, on va pouvoir trouver un hyperplan séparateur (qui aurait pu ne pas exister si le problème n'était pas séparable) mais la solution obtenue sera aussi plus robuste (généralisera mieux). On peut en effet facilement intuiter que le choix de l'hyperplan séparateur est très sensible à la présence d'*outliers*: un outlier mal placé peut conduire à une direction potentiellement très différente et/ou une marge réduite(et donc possiblement de faibles performances en généralisation). \n",
    "\n",
    "Remarque: La sensiblité aux *outliers* décrite ci-dessus correspond en fait à une situation d'*overfitting*.\n",
    "\n",
    "$x_{i}^{\\top}.w+b$ correspond à la distance (arithmétique) de $x_{i}$ à l'hyperplan central, son signe donnant le côté duquel $x_i$ se trouve. Pour qu'un point soit considéré comme bien classé (et en dehors de la marge), on doit avoir: \n",
    "* $|x_{i}^{\\top}.w+b|\\geq1$\n",
    "* La distance $x_{i}^{\\top}.w+b$ et $y_i$ de même signe. \n",
    "\n",
    "Un point mal classifié ou à l'intérieur de la marge correspond ainsi aux cas où $y_{i}.(x_{i}^{\\top}.w+b)\\leq1$. On introduit alors une variable $\\xi_i$ correspondant à l'écart entre $y_{i}.(x_{i}^{\\top}.w+b)$ et $1$. $\\xi_{i}$ est appelée *slack variable* et est telle que:\n",
    "* $\\xi_{i}$ est positive lorsqu'un point n'est pas du bon côté de l'hyperplan délimitant la marge pour sa classe. On a alors $\\xi_{i}=1-y_{i}.(x_{i}^{\\top}.w+b) \\gt 0$\n",
    "* $\\xi_{i}$ est nulle lorsque le point est bien classé.\n",
    "\n",
    "L'introduction de $\\xi_{i}\\geq0$ autorise que des points $x_i$ puissent se trouver du mauvais côté de leut hyperplan, c'est-à-dire dans la marge ou mal classifiés. **Noter ainsi qu'une erreur de classification ne correspond pas à $\\xi_{i} > 0$ mais à $\\xi_{i} > 1$**. Le cas où $\\xi_{i}$ compris entre $0$ et $1$ correspond à un point encore correctement classifié mais situé à l'intérieur de la marge.\n",
    "\n",
    "En fixant une borne supérieure $K$ à $\\sum_{i}\\xi_{i}$ (on parle parfois de $K$ comme d'un budget), on s'assure ainsi qu'on aura au plus $K$ point mal classifiés. On peut alors réécrire le problème d'optimisation:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\min_{w, b}} & {\\frac{1}{2}w^{\\top}w} \\\\ {\\text { subject to }} & {y_{i}.(x_{i}^{\\top}.w + b) \\geq 1-\\xi_{i}}  \\\\ {} & {\\xi_{i} \\geq 0} \\\\ {} & {\\sum_{i}\\xi_{i} \\leq K} \\end{array}\n",
    "$$\n",
    "\n",
    "Remarque: $\\xi_{i}$ est nul si le point se situe du bon côté de l'hyperplan $x_{i}^{\\top}.w+b = \\pm 1$ (suivant sa classe). $\\xi_{i}$ sera d'autant plus élevé que $x_{i}$ est profondément du mauvais côté.\n",
    "\n",
    "Ce problème est le plus souvent donné dans la forme équivalente:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\min_{w, b, \\xi}} & {\\frac{1}{2}w^{\\top}w + C\\sum_{i}\\xi_{i}} \\\\ {\\text { subject to }} & {y_{i}.(x_{i}^{\\top}.w + b) \\geq 1-\\xi_{i}}  \\\\ {} & {\\xi_{i} \\geq 0} \\end{array}\n",
    "$$\n",
    "\n",
    "Sous cette forme, on vient ajouter à la fonction objectif un terme pénalisant les $\\xi_{i}$ positifs. Le terme $C$ (appelé simplement *cost*) contrôle ainsi le compromis entre maximisation de la marge (minimisation de la norme de $w$) et tolérance aux erreurs de classification. Ce paramètre de régularisation est très souvent associé à la variable $C$ comme dans la librairie `sklearn.svm`.\n",
    "\n",
    "### Hinge *loss*\n",
    "On remarque que $\\xi_{i} = max(0, 1-y_{i}.(x_{i}^{\\top}.w + b)) \\geq 0$. Le problème d'optimisation sous contraintes ci-dessus peut donc se réécrire de façon équivalente sous la forme du problème non contraint: \n",
    "\n",
    "$$\\min_{w, b} \\frac{1}{2}w^{\\top}w + C\\sum_{i}max(0, 1-y_{i}.(x_{i}^{\\top}.w + b))$$\n",
    "\n",
    "Cette forme n'est pas facilement soluble du fait de la forte non linéarité de la fonction objectif mais elle a l'avantage de faire apparaître la fonction objectif à minimiser comme la somme de deux termes: \n",
    "* Un terme de pénalité $\\frac{1}{2}w^{\\top}w$ qui désavantage les normes élevées pour $w$ et qui de façon équivalente favorise les marges larges. Ce terme ne dépend ni de $y_{i}$ ni de $ŷ_{i}=sgn(x_{i}^{\\top}.w + b)$, ce n'est donc pas une *loss* / un *empirical risk*. Ce terme peut se voir comme un terme de régularisation: il favorise les solutions avec une bonne performance en généralisation.\n",
    "* Un terme de pertes (*loss*) $\\sum_{i}L(y_{i},ŷ_{i})$ avec $L(y_{i},ŷ_{i})=max(0, 1-y_{i}.(x_{i}^{\\top}.w + b))$ appelée la *hinge loss* (nom lié à la forme en coude de la fonction $x \\mapsto max(0, 1-x)$, *hinge* signifiant \"charnière\").\n",
    "\n",
    "Remarque: La *hinge loss* a l'avantage d'être convexe et contribue à l'objectif de maximisation de la marge du fait qu'elle pénalise d'autant plus un point qu'il est éloigné de sa marge.\n",
    "\n",
    "On rencontre aussi la *squared hinge loss* correspondant simplement à $L(y_{i},ŷ_{i})=max(0, 1-y_{i}.(x_{i}^{\\top}.w + b))^2$. Comme lors des passages des normes 1 à 2, passer de la *hinge loss* à la *squared hinge loss* a pour effet de davantage pénaliser les points très mal classifiés avec comme corrolaire d'être plus sensible aux *outlier*.\n",
    "\n",
    "Le problème d'optimisation sous contraintes (facilement soluble) associé à la *squared hinge loss* est simplement:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\min_{w, b, \\xi}} & {\\frac{1}{2}w^{\\top}w + C\\sum_{i}\\xi_{i}}^{2} \\\\ {\\text { subject to }} & {y_{i}.(x_{i}^{\\top}.w + b) \\geq 1-\\xi_{i}}  \\\\ {} & {\\xi_{i} \\geq 0} \\end{array}\n",
    "$$\n",
    "\n",
    "### Interprétation du paramètre de régularisation $C$\n",
    "Attention à l'interprétation du paramètre de régularisation : plus il est faible, plus l'effet régularisant est élevé (inversement aux paramètres $\\lambda$ utilisés dans d'autres problèmes):\n",
    "* Si $C$ est très élevé, ne pas se trouver du bon côté de son hyperplan ($\\xi_{i}$ positif) voire être mal classifié ($\\xi_{i} \\gt 1$) est très pénalisée et à l'extrême, tous les $\\xi_{i}$ sont mis à zéro ce qui nous ramène au problème séparable (cas asymptotique $C=\\infty$).\n",
    "* Si $C$ est très faible, l'impact de larges erreurs ($\\xi_{i}$ élevés) sur la fonction objectif sera d'autant plus faible.\n",
    "\n",
    "Dans les autres problèmes régularisés où le terme contrôlant la régularisation est souvent noté $\\lambda$ on cherche le plus souvent à contraindre la norme d'un vecteur de coefficients $\\beta$ à prendre de faibles valeurs dans le même cadre d'un problème de minimisation (d'une fonction de coûts). Une valeur de $\\lambda$ élevée va alors contraindre à un vecteur de coefficients le plus *sparse* possible et à l'extrême, nul. A des valeurs de $\\lambda$ élevées est donc associée une forte régularisation. A l'inverse, un $\\lambda$ ne met que peu de contraintes sur le vecteur des coefficients et régularise peu. Le cas d'un $\\lambda$ nul nous ramène au cas non-régularisé.\n",
    "\n",
    "Dans le cas du SVM et du coefficient $C$ c'est l'inverse: c'est à une valeur élevée des $\\xi_{i}$ qu'est associé un fort effet régularisant (alors que c'est une valeur faible des $\\beta_{i}$ pour $\\lambda$). Ces $\\xi_{i}$ pourront prendre des valeurs d'autant plus élevées que l'impact sur la fonction de coût sera modéré et donc que $C$ est faible. A l'inverse, un $C$ trop élevé pénalise trop fortement les $\\xi_{i}$ positifs qui à l'extrême seront tous nuls. L'effet régularisant est alors faible ou nul.\n",
    "\n",
    "Dans `sklearn`, le paramètre classiquement noté $\\lambda$ dans les problèmes est appelé `alpha` (sans doute parce que `lambda` est un nom réservé par Python). Suivant les problèmes, la librairie appelle son paramètre de régularisation `alpha` où l'effet est d'autant plus fort que le paramètre est élevé ou `C` en référence aux SVM (le cas pour les modèles de `sklearn.svm` mais aussi pour `sklearn.linear_model.LogisticRegression` par exemple) et où l'effet est d'autant plus fort que le paramètre est faible.\n",
    "\n",
    "### $\\nu$-SVM\n",
    "La constante $C$ a le désavantage d'être d'autant plus difficile à optimiser qu'elle peut prendre une large gamme de valeurs (elle est à valeurs dans $\\mathbb{R}^{+}$). Il a alors été proposé une formulation alternative du problème du SVM correspondant au programme d'optimisation:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\min_{w, b, \\xi, \\rho}} & {\\frac{1}{2}w^{\\top}w - \\nu.\\rho + \\frac{1}{n}\\sum_{i}\\xi_{i}} \\\\ {\\text { subject to }} & {y_{i}.(x_{i}^{\\top}.w + b) \\geq \\rho-\\xi_{i}}  \\\\ {} & {\\xi_{i} \\geq 0} \\\\ {} & {\\rho \\geq 0} \\end{array}\n",
    "$$\n",
    "\n",
    "Avec $\\nu \\in [0, 1]$ et $n$ le nombre d'observations.\n",
    "\n",
    "La forme du problème n'a pas l'air d'être très simple à interpréter mais ce qui compte le plus est qu'elle est dans les cas qui nous intéressent, équivalente à la formutaion classique du problème appelée $C$-SVM. On peut toutefois préciser que dans cette formulation, on peut montrer que: \n",
    "* La largeur de la marge est $\\frac{2\\rho}{\\|w\\|}$ quand $\\xi = 0$.\n",
    "* Le coefficient $\\nu$ correspond à la fraction maximale de points *out of margin* (dans la marge ou mal classifiés) et à la fraction minimale de points également vecteurs de support.\n",
    "* Si la résolution du problème précédent aboutit à $\\rho \\gt 0$, alors les formulations correspondant au $\\nu$-SVM et au $C$-SVM sont équivalentes (le $\\nu$-SVM n'étant alors qu'une reparamétrisation du $C$-SVM) avec $C=\\frac{1}{\\nu.n}$. Cette formulation a ainsi l'avantage d'améliorer l'interprétabilité du coefficient $C$ (au prix semble-t-il d'une résolution plus difficile du problème).\n",
    "\n",
    "Remarque: $\\nu$ correspond aussi asymptotiquement à la fraction de prédictions mal classifiées, à l'erreur.\n",
    "\n",
    "La librairie `sklearn.svm` fournit une implémentation du $\\nu$-SVM pour la classification (`sklearn.svm.NuSVC`) et la régression (`sklearn.svm.NuSVR`).\n",
    "\n",
    "Pour plus de détails, voir notamment : \n",
    "* [A tutorial on $\\nu$-Support Vector Machines](http://is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/pdf3353.pdf). \n",
    "* [Schölkopf et al. (2000), page 1223 et suivantes](https://alex.smola.org/papers/2000/SchSmoWilBar00.pdf).\n",
    "\n",
    "## Résolution des problèmes d'optimisation\n",
    "Reprenons le problème (primal) associé au cas linéairement séparable: \n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\min_{w, b}} & {\\frac{1}{2}w^{\\top}w} \\\\ {\\text { subject to }} & {y_{i}.(x_{i}^{\\top}.w + b) \\geq 1} \\end{array}\n",
    "$$\n",
    "\n",
    "Dont le lagrangien associé est:\n",
    "\n",
    "$$\\mathcal{L}(w, b, \\mu) = \\frac{1}{2}w^{\\top}w + \\sum_{i}\\mu_{i}.(1-y_{i}.(x_{i}^{\\top}.w + b))$$\n",
    "\n",
    "On se situe dans le cas favorable d'une fonction objectif convexe et de contraintes affines: \n",
    "* Les conditions de Slater nous assurent que la résolution du primal et du dual sont équivalentes (dualité forte).\n",
    "* Satisfaire les conditions KKT sont une condition suffisante pour être solution optimale du problème contraint.\n",
    "\n",
    "On montre notamment après résolution des KKT que la fonction de décision peut s'exprimer sous la forme suivante: \n",
    "\n",
    "$$x^{\\top}.w + b = \\sum_{i}\\mu_{i}y_{i}\\langle x, x_{i}\\rangle + b$$\n",
    "\n",
    "Remarque : \n",
    "* L'allure de la fonction de décision est la même dans le cas du *soft-margin classifier* où on introduit des *slack variables* $\\xi_{i}$.\n",
    "* L'allure de la fonction de décision est la même pour le $\\nu$-SVM, quoique que les multiplicateurs de Lagrange $\\mu_{i}$ peuvent sans doute en théorie avoir des valeurs différentes que dans le cas du $C$-SVM quoique les deux approches semble coincider la plupart du temps.\n",
    "\n",
    "### Vecteurs de support\n",
    "Par *complementary slackness*, un certain nombre de $\\mu_{i}$ seront nuls. Calculer la variable de décision ne demande alors de calculer que les produits scalaires de $x$ avec les vecteurs $x_{i}$ dont le multiplicateur de Lagrange est non nul. Les points pour lesquels le multiplicateur de Lagrange $\\mu_{i}$ est non nul sont appelés **vecteurs de support** (*support vectors*):\n",
    "* Dans le cas séparable (*hard-margin classifier*), on montre que les point à multiplicateur nul sont situés strictement au delà des hyperplans délimitant la marge $x^{\\top}.w+b = \\pm 1$, les *support vectors* sont situés sur l'un des deux hyperplans qui prennent donc \"appui sur eux\" d'où le nom. Par définition, il n'y a pas dans ce cas de points situés à l'intérieur de la marge\n",
    "* Dans le cas non séparable (*soft-margin classifiers*), le développent des KKT permet de montrer que les multplicateurs de Lagrange sont bornés $0 \\leq \\mu_{i} \\leq C$ Les *support vectors* correspondent dans ce cas à l'ensemble des points situés à l'intérieur de la marge ou sur les hyperplans séparateurs la délimitant: \n",
    "    * Aux multiplicateurs nuls correspondent les points situés strictement au delà des hyperplan délimitant la marge.\n",
    "    * A $0 \\lt \\mu_{i} \\lt C$ correspondent les points situés sur les hyperplans délimitant la marge.\n",
    "    * A $\\mu_{i} = C$ correspondent les points situés strictement à l'intérieur de la marge.\n",
    "\n",
    "Cette propriété remarquable est aussi intéressante du point de vue de la performance: pour classifier un nouveau point, il suffit de calculer les produits scalaires de celui-ci avec les *support vectors*, ce qui peut être relativement rapide. Seule une fraction des points du *training set* contribue à la fonction de décision, la solution est ainsi dite *sparse* ce qui conduit parfois à désigner les SVM comme des *sparse kernel machines*.\n",
    "\n",
    "### Problèmes duals\n",
    "On montre que le problème dual du cas séparable peut se mettre sous la forme:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\max_{\\mu}} & {\\sum_{i}\\mu_{i} - \\frac{1}{2}\\sum_{i}\\sum_{j}\\mu_{i}\\mu_{j}y_{i}y_{j}\\langle x_{i}, x_{j}\\rangle} \\\\ {\\text { subject to }} & {\\mu_{i} \\geq 0}  \\\\ {} & {\\sum_{i}\\mu_{i}y_{i} = 0} \\end{array}\n",
    "$$\n",
    "\n",
    "Qu'on peut notamment réécrire:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\max_{\\mu}} & {\\sum_{i}\\mu_{i} - \\frac{1}{2}\\mu^{\\top}K\\mu} \\\\ {\\text { subject to }} & {\\mu_{i} \\geq 0}  \\\\ {} & {\\mu^{\\top}.y = 0} \\end{array}\n",
    "$$\n",
    "\n",
    "Avec $K_{ij}=y_{i}y_{j}\\langle x_{i}, x_{j}\\rangle$ qui est équivalent à définir $K=(yx).(yx)^{\\top}$ \n",
    "\n",
    "Dans le cas où on introduit des *slack variables* $\\xi_{i}$, l'allure du problème dual est de façon assez remarquable quasi identique. Seule la contrainte $\\mu_{i} \\geq 0$ est modifiée pour devenir $0 \\leq \\mu_{i} \\leq C$.\n",
    "\n",
    "## Apprentissage de fonctions de décision non linéaires: *kernels* et *kernel trick*\n",
    "Les SVM font partie d'un ensemble plus large de méthodes appelées *kernel methods* (cf. notebook dédié). L'écriture du problème dual associé aux *soft/hard-margin classifiers* a fait apparaître que les données du *training set* n'apparaissent que dans le cadre d'opérations de comparaison deux à deux (*pair-wise comparisons*), ici sous la forme d'un produit scalaire.\n",
    "\n",
    "Du fait de cette forme particulière, on peut finalement remplacer le produit scalaire par n'importe quel opérateur de comparaison entre deux vecteurs (*kernel trick*). Celui-ci, appelé *kernel*, doit cependant avoir certaines propriétés. Choisir un *kernel* $K$ revient alors à choisir un espace hilbertien (*feature space*) $\\mathcal{H}$ (théorème de Mercer/Aronszajn) potentiellement de plus haute dimension dans lequel on va séparer linéairement nos données qui y auront été mappées par un opérateur $\\Phi$ associé à $\\mathcal{H}$ appelé *feature map* et qu'on a pas besoin de connaître explicitement.\n",
    "\n",
    "Quel que soit le kernel choisit, un résultat (thérorème du représentant) nous assure que la fonction de décision $\\hat{f}$ sera toujours de la forme: \n",
    "\n",
    "$$\\hat{f}(x) = \\sum_{i=1}^{n}\\mu_{i}K(x_{i}, x)$$\n",
    "\n",
    "$\\hat{f}$ est ainsi toujours une combinaison linéaire de fonctions potentiellement non linéaires de $x$. C'est là que réside tout l'intérêt et la puissance du *kernel trick*: on peut apprendre des fonctions de décision non linéaires en $x$ en passant dans un espace de dimension supérieure (et potentiellement infinie) sans jamais avoir à expliciter et à calculer la transformation $\\Phi(x)$, opération potentiellement très coûteuse. Seule la connaissance de la *kernel function* $K$ suffit.\n",
    "\n",
    "Les cas présentés jusqu'ici correspondent au cas particulier du *kernel* linéaire où la *kernel function* $K$ correspond simplement au produit scalaire canonique sur $\\mathbb{R}^n$ (fonction bilinéaire). $\\hat{f}$ s'écrit alors comme une somme de fonctions linéaires en $x$ et est ainsi elle même une fonction linéaire en $x$. On retrouve le résultat issu de la résolution de problèmes d'optimisation vus plus haut.\n",
    "\n",
    "# SVM en régression (SVR)\n",
    "\n",
    "## $\\epsilon$-SVR\n",
    "Les SVM peuvent aussi être utilisés en régression. Dans le cas sans *kernel* (*kernel* linéaire), on va chercher à ajuster au nuage de points un hyperplan tel que l'ensemble des points du nuage soient inclus dans la marge, celle-ci ne pouvant dépasser $2\\epsilon$. En s'aidant des raisonnements utilisés pour aboutir au problème d'optimisation du SVM en classification dans le cas linéairement séparable, on obtient le problème suivant: \n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\min_{w, b}} & {\\frac{1}{2}\\|w\\|^2} \\\\ {\\text { subject to }} & {y_{i} - (x_{i}^{\\top}.w + b) \\leq \\epsilon} \\\\ {} & {(x_{i}^{\\top}.w + b) - y_{i} \\leq \\epsilon} \\end{array}\n",
    "$$\n",
    "\n",
    "Remarque: La fonction apprise est $\\hat{f}(x) = x_{i}.\\hat{w} + \\hat{b}$, le SVR avec *kernel* linéaire est donc une régression linéaire pour une *loss function* particulière (cf. plus loin).\n",
    "\n",
    "Il peut cependant ne pas être possible de trouver un hyperplan et des marges telles que tous les points du *training* puissent être inclus dans une bande de largeur $2\\epsilon$. Comme dans le cas non séparable en classification, on va introduire des *slack variables* $\\xi_i$. Un point situé en dehors de la bande de largeur $2\\epsilon$ sera ainsi à une distance $\\epsilon + \\xi_i$ de l'hyperplan à trouver. Le problème se réécrit alors:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\min_{w, b, \\xi_i, \\xi_{i}^*}} & {\\frac{1}{2}\\|w\\|^2 + C\\sum_{i}(\\xi_{i} + \\xi_{i}^*)} \\\\ {\\text { subject to }} & {y_{i} - (x_{i}^{\\top}.w + b) \\leq \\epsilon + \\xi_i} \\\\ {} & {(x_{i}^{\\top}.w + b) - y_{i} \\leq \\epsilon + \\xi_{i}^*} \\\\ {} & {\\xi_{i}, \\xi_{i}^* \\geq 0} \\end{array}\n",
    "$$\n",
    "\n",
    "Comme dans le cas de la classification, un peu de calcul nous permet d'obtenir le dual du problème primal donné ci-dessus: \n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\max_{\\mu_i, \\mu_{i}^*}} & {-\\frac{1}{2}\\sum_{i}\\sum_{j}(\\mu_i - \\mu_{i}^*)(\\mu_j - \\mu_{j}^*)k(x_i, x_j) - \\epsilon\\sum_{i}(\\mu_i + \\mu_{i}^*) + \\sum_{i}y_{i}(\\mu_i - \\mu_{i}^*)} \\\\ {\\text { subject to }} & {\\sum_{i}(\\mu_i - \\mu_{i}^*) = 0} \\\\ {} & {0 \\leq \\mu_i, \\mu_{i}^* \\leq C}\\end{array}\n",
    "$$\n",
    "\n",
    "Avec $\\mu_i$ et $\\mu_{i}^*$ les multiplicateurs de Lagrange associés aux contraintes $y_{i} - (x_{i}^{\\top}.w + b) \\leq \\epsilon + \\xi_i$ et $(x_{i}^{\\top}.w + b) - y_{i} \\leq \\epsilon + \\xi_{i}^*$ respectivement et $k$ un *kernel* défini positif. Dans le cas le plus simple (*kernel* linéaire): $k(x_i,x_j)=\\langle x_i, x_j \\rangle_{\\mathbb{R}^n}$\n",
    "\n",
    "On montre que la fonction $\\hat{f}$ finalement apprise est: \n",
    "\n",
    "$$\\hat{f}(x) = \\sum_{i}(\\mu_i - \\mu_{i}^*)k(x, x_i) + b$$\n",
    "\n",
    "Comme dans le cas de la classification: \n",
    "* Un SVR est une combinaison linéaire de fonctions de $x$.\n",
    "* Un SVR est une *sparse kernel machine*: on remarque en effet que les multiplicateurs de Lagrange $\\mu_i$ et $\\mu_{i}^*$ sont par *complementary slackness* nuls pour tous les points situés à l'intérieur de la bande de largeur $2\\epsilon$. Calculer $\\hat{f}(x)$ demande ainsi de n'évaluer $k(x, x_i)$ que pour les $x_i$ situés à l'extérieur de la bande de largeur $2\\epsilon$ (les *support vectors*).\n",
    "* Le *kernel trick* s'applique si on souhaite apprendre des fonctions non linéaires de $x$. La forme de la solution $\\hat{f}(x)$ ne change pas, $\\hat{f}$ devient juste une combinaison linéaire de fonctions potentiellement non linéaires en $x$.\n",
    "\n",
    "Pour plus de détails, voir notamment : [A tutorial on Support Vector Regression](https://alex.smola.org/papers/2003/SmoSch03b.pdf)\n",
    "\n",
    "## *Loss function*\n",
    "Comme dans le cas de la classification, l'introduction de *slack variables* correspond à l'introduction d'une *loss*: on ajoute un terme $\\sum_{i}(\\xi_{i})$ qui s'ajoute au terme $\\frac{1}{2}\\|w\\|^2$ qui en favorisant les cas à vaste marge vise à améliorer la performance en généralisation et joue donc un rôle régularisant.\n",
    "\n",
    "On remarque que $\\xi_i = max(0, |y_{i}-f(x_{i})|-\\epsilon)$ : $\\xi_i$ est donc nul dans la bande $[-\\epsilon, \\epsilon]$ et positif et égal à $|y_{i}-f(x_{i})|$ en dehors. Considérant qu'on cherche à pénaliser toute observation se retrouvant à l'extérieur de la bande de largeur $2\\epsilon$, $\\xi_i$ joue bien le rôle d'une *loss*.\n",
    "\n",
    "La *loss function* de régression associée se nomme *epsilon-insensitive hinge loss* et se définit $\\mathcal{L}_{\\epsilon}(y, f(x)) = max(0, |y-f(x)|-\\epsilon)$.\n",
    "\n",
    "On comprend dès lors que les deux problèmes d'optimisation suivant sont équivalents (cas du *kernel* linéaire où chercher $f$ revient à minimiser sur $w$). On passe de l'un à l'autre par une reparamétrisation):\n",
    "\n",
    "* Forme avec *slack variables*:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\min_{w, b, \\xi_i, \\xi_{i}^*}} & {\\frac{1}{2}\\|w\\|^2 + C\\sum_{i}(\\xi_{i} + \\xi_{i}^*)} \\\\ {\\text { subject to }} & {y_{i} - (x_{i}^{\\top}.w + b) \\leq \\epsilon + \\xi_i} \\\\ {} & {(x_{i}^{\\top}.w + b) - y_{i} \\leq \\epsilon + \\xi_{i}^*} \\\\ {} & {\\xi_{i}, \\xi_{i}^* \\geq 0} \\end{array}\n",
    "$$\n",
    "\n",
    "* Forme avec *loss function*:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\min_{w, b}} & {\\frac{1}{2}\\|w\\|^2 + C\\sum_{i}\\mathcal{L}_{\\epsilon}(y, f(x))}\\end{array}\n",
    "$$\n",
    "\n",
    "Le premier problème est obtenu à partir d'une interprétation géométrique du problème de maximisation de la marge. Le second correspond à une formulation plus classique des problèmes de régression (ou de classification), où on recherche parmi une classe de fonction le meilleur compromis entre minimisation d'une *loss* et performance en généralisation. On parle pour ce dernier cas d'interprétation fonctionnelle. On voit que dans le cas des SVM, interprétation fonctionnelle et géométrique sont équivalentes pour les *hinge loss functions* et permettent chacune d'aboutir au problème primal du SVM. Cette reparamétrisation est nécéssaire quand on part de la formulation fonctionnelle du problème, les *hinge losses* des problèmes de SVM n'étant pas suffisamment régulières pour permettre de résoudre directement le problème d'optimisation. \n",
    "\n",
    "La forme avec la *loss function* ci-dessus peut être retravaillée pour obtenir une forme plus classique qui permet aussi de retrouver le rôle opposé du paramètre de régularisation $C$ des SVM et du paramètre de régularisation classique $\\lambda$:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\min_{w, b}} & {\\frac{1}{n}\\sum_{i}\\mathcal{L}_{\\epsilon}(y, f(x)) + \\frac{\\lambda}{2}\\|w\\|^2 }\\end{array}\n",
    "$$\n",
    "\n",
    "## $\\nu$-SVR\n",
    "Comme en classification, il existe une formulation type $\\nu$-SVM permettant d'avoir plus de contrôle sur le nombre de *support vectors* finalement obtenus dans la solution. Dans le cas du $\\epsilon$-SVR (où les paramètres à choisir sont $\\epsilon$ et $C$ dans $\\mathbb{R}+$), on contrôle la largeur maximale de la bande mais on on ne contrôle pas le nombre finalement obtenu de *support vectors*. Le $\\nu$-SVR permet à l'inverse de s'assurer d'un nombre de *support vectors*, la largeur de la bande $\\epsilon$ n'étant désormais plus choisie par l'utilisateur mais utilisée comme paramètre par le modèle. Dans le cas du $\\nu$-SVR, l'utilisateur doit choisir les paramètres $C \\in \\mathbb{R}+$ et $\\nu \\in [0,1]$. \n",
    "\n",
    "## Le SVR dans `sklearn`\n",
    "La librairie `sklearn.svm` propose deux implémentations pour le $\\epsilon$-SVR à travers les objets `LinearSVR` (*kernel* linéaire) et `SVR` (pour l'utilisation de *kernels*). Une implémentation de $\\nu$-SVR est disponible avec l'objet `NuSVR`.\n",
    "\n",
    "# *Dataset* déséquilibré\n",
    "En cas de classes déséquilibrées, la solution la plus couramment utilisée est d'introduire des paramètres de régularisation $C$ différents pour chacune des classes (le cas particulier du cas séparable n'est pas impacté par un éventuel déséquilibre des classes). Le problème prend alors la forme:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\min_{w, b, \\xi}} & {\\frac{1}{2}w^{\\top}w + C^{+}\\sum_{y_{i}=1}\\xi_{i} + C^{-}\\sum_{y_{i}=-1}\\xi_{i}} \\\\ {\\text { subject to }} & {y_{i}.(x_{i}^{\\top}.w + b) \\geq 1-\\xi_{i}}  \\\\ {} & {\\xi_{i} \\geq 0} \\end{array}\n",
    "$$\n",
    "\n",
    "Dans la librairie `sklearn.svm`, cette situation est notamment contrôlée par l'argument `class_weight`. On donne d'abord une valeur au paramètre de régularisation `C`, puis $C^{+}$ et $C^{-}$ sont calculés en multipliant `C` par les poids passés à `class_weight`. Si on choisit le mode `balanced` pour `class_weight`, le poids par lequel est multplié `C` pour une classe donnée est l'inverse de sa prévalence dans l'échantillon : la classe minoritaire aura donc le $C$ le plus élevé.\n",
    "\n",
    "L'idée est globalement de mettre un $C$ plus élevé à la classe minoritaire. En l'absence de cette précaution, on prend le risque comme souvent de biaiser le classificateur en faveur de la classe majoritaire ce qui dans le cas du SVM se manifeste par une part trop importante d'éléments de la classe minoritaire à l'intérieur de leur marge voire mal classifiés à l'entrainement. Voir notamment [cet exemple](https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html).\n",
    "\n",
    "Pour plus de détails, voir notamment [la partie 6 de la documentation de la libraire LIBSVM](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf).\n",
    "\n",
    "## *One-class* SVM\n",
    "Le *one-class* SVM est une utilisation non supervisée des SVM pour la détection d'anomalie introduite par [Schölkopf et al. (2001)](http://users.cecs.anu.edu.au/~williams/papers/P126.pdf). Par opposition, on désigne aussi cette technique comme permettant d'estimer le support d'une distribution: si on ne se situe pas dans la région estimée c'est donc qu'on est un *outlier* pour la distribution estimée.\n",
    "\n",
    "Le *one-class* SVM correspond au problème d'optimisation suivant:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\min_{w, \\xi, \\rho}} & {\\frac{1}{2}w^{\\top}w - \\rho + \\frac{1}{n.\\nu}\\sum_{i}\\xi_{i}} \\\\ {\\text { subject to }} & {w^{\\top}.\\Phi(x_{i}) \\geq \\rho-\\xi_{i}}  \\\\ {} & {\\xi_{i} \\geq 0} \\end{array}\n",
    "$$\n",
    "\n",
    "Le problème semble très proche de celui du $\\nu$-SVM (on note que la contrainte $\\rho \\geq 0$ a disparu, tout comme l'*intercept* $b$). Ce problème est en fait équivalent à (et présenté comme) à établir une séparation maximale entre l'ensemble des points du *dataset* (labélisés $y_{i}=1$, d'où l'apparente absence des $y_{i}$) et l'origine du *feature space* (pour lequel $y_{i}=-1$ mais comme $w^{\\top}.\\Phi(x_{i})=0$, la contrainte n'apparait pas ce qui revient aussi implicitement à dire que $\\xi_{i} = 0$ pour l'origine). Comme pour le $\\nu$-SVM, on autorise au plus une fraction $\\nu$ des observations à être classifiée *out-of-margin*: ce sont nos *outliers*.\n",
    "\n",
    "Le *one-class* SVM ne semble pas utilisé sans *kernel*, le *kernel* semblant être le plus utilisé étant le RBF. Ce *kernel* permet aussi de mieux comprendre l'idée consistant à chercher à séparer les observations de l'origine avec un hyperplan. Avec le *kernel RBF*, $k(x_{i}, x_{j}) \\gt 0$, tous les points se situent donc dans le même orthant du *feature space*. On a de plus $k(x_{i}, x_{i}) = 1$ pour tout $i$, tous les vecteurs sont donc unitaires (rappel: le kernel est un produit scalaire dans le *feature space*). Tous les points sont donc ainsi distribués sur la portion d'hyper-sphère de rayon 1 de cet orthant, les *outliers* étant sur les côtés ($k(x_{i}, x_{j}$ proche de $0$ pour l'ensemble des $j$). Un hyperplan séparant le nuage de points de l'origine peut ainsi du fait de la courbure de la sphère venir placer les *outliers* du côté de cette dernière et le reste des observations sur le côté opposé.\n",
    "\n",
    "Il s'agit de l'approche implémentée pour l'objet `sklearn.svm.OneClassSVM` (dont on notera que le *kernel* par défaut est `rbf` et non `linear`).\n",
    "\n",
    "### *Support Vector Data Description* (SVDD)\n",
    "[Tax et al. (2004)](http://homepage.tudelft.nl/a9p19/papers/ML_SVDD_04.pdf) proposent une approche semblable à celle du *one-class* SVM de Schölkopf et al. et plus conforme à l'intuition de l'estimation d'un support. Leur approche consiste à estimer le rayon de la plus petite hyper-sphère englobant les données (*minimum enclosing hypersphere*). Le problème d'optimisation associé s'exprime simplement:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\min_{R, a, \\xi}} & {R^2 + C\\sum_{i}\\xi_{i}} \\\\ {\\text { subject to }} & {((x_{i}-a)^{\\top}.(x_{i}-a)) \\leq R^2 + \\xi_{i}}  \\\\ {} & {\\xi_{i} \\geq 0} \\end{array}\n",
    "$$\n",
    "\n",
    "Ce modèle se rencontre sous le nom *Support Vector Data Description* (SVDD) mais peut être aussi confondu avec le *one-class* SVM de Schölkopf et al.. Il s'utilise sans problème avec un *kernel* (qu'on peut faire apparaître en développant les expressions des contraintes ci-dessus - cf. eq. 9 et 10) et semble fournir des résultats proches de ceux du *one-class* SVM de Schölkopf et al.. \n",
    "\n",
    "Voir aussi [cette illustration](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3521128/figure/F1/) pour une  interprétation graphique des approches du *one-class* SVM et du SVDD (notamment quand les données se distribuent sur une hyper-sphère dans le *feature space*)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
