{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la plupart des modèles probabilistes d'un intérêt pratique, il n'existe pas de solution analytique autorisant une inférence exacte. A côté d'algorithmes d'inférence basé sur des approximations déterministes, on considère ici des méthodes approchées se fondant sur un échantillonage numérique aussi connues sous le nom de Monte Carlo. \n",
    "\n",
    "Les techniques présentées visent :\n",
    "* A calculer l'espérence des valeurs prises par une fonction relativement à une distribution de probabilités p(z). Le calcul de l'intégrale peut être très difficile voire sans solution analytique, l'idée sera de l'approcher par une somme finie. \n",
    "* A générer des échantillons d'une loi pour laquelle cette génération est difficile.\n",
    "\n",
    "### Méthodes standards\n",
    "\n",
    "### Rejection sampling\n",
    "Prenons une distribution de probabilité p(z) (pour l'instant supposée univariée) pour laquelle la génération d'échantillons est difficile mais dont l'évaluation en un point particulier est aisée. Le rejection sampling (méthode du rejet) va s'appuyer sur une distribution auxiliaire (proposal distribution) q(z) dont l'échantillonage est aisé. On impose à cette distribution q de respecter l'inégalité suivante : Ek>0 Az, k.q(z)>p(z). L'algorithme va consister à :\n",
    "1) On génère un premier nombre z0 à partir de la distribution q(z)\n",
    "2) On génère un deuxième nombre u0 à partir de la distribution uniforme sur [0, kq(z0)], l'ensemble des couples (z0, u0) ainsi générés sont répartis uniformément sous la courbe de kq(z).\n",
    "3) Le couple (z0, u0) n'est retenu uniquement si u0<=p(z0) (et est rejeté sinon, d'où le nom de la méthode). L'ensemble des couples (z0, u0) finalement conservés seront ainsi uniformément répartis sous la courbe de p(z), les valeurs de z correspondantes étant ainsi distribuées suivant p(z).\n",
    "\n",
    "Choix de la constante k: afin de minimiser le nombre d'échantillons rejetés et ainsi de maximiser l'efficacité de l'algorithme, la valeur opltimale de k est la plus petite valeur de k pour laquelle l'inégalité est respectée (on montre facilement que la probabilité d'accepter une paire (z,u) est inversement proportionnelle à k). \n",
    "\n",
    "Doit-on connaitre la constante de normalisation ?\n",
    "\n",
    "Problème en plus haute dimension\n",
    "\n",
    "Remarque : adaptative rejection sampling\n",
    "\n",
    "\n",
    "### Importance sampling\n",
    "Une des motivations de l'échantillonage d'une distribution de probabilité complexe peut être le calcul d'une espérance. L'importance sampling a de particulier que la technique ne fournit pas une approximation de la distribution de probabilité sur laquelle est calculée l'espérance mais une approximation de l'intégrale elle-même. \n",
    "\n",
    "On suppose ici qu'il est difficile de générer des échantillons selon la distribution de probabilité p(z) mais que le calcul de p(z0) étant donné z0 est aisé. L'idée de départ est de discrétiser l'espace dans lequel z prend ses valeurs, d'y échantillonner z et d'approcher l'intégrale par une somme finie de la forme :\n",
    "\n",
    "Le problème de cette approche est potentiellement double : \n",
    "* Le nombre de subdivisions à échantilloner de l'espace où z prend ses valeurs augmente exponentiellement avec la dimension de z. \n",
    "* Dans la plupart des cas et d'autant plus en dimension élevée, l'essentiel de la masse de la distribution p(z) se concentrera dans des régions relativement restreintes de l'espace de z. Un échantillonage uniforme de cet espace sera ainsi d'autant plus inefficace que la dimension sera élevée. \n",
    "L'idée est alors de ne pas échantillonner l'espace de façon uniforme mais de le faire préférentiellement dans les zones où p(z) voire mieux, où p(z).f(z) est élevée. Comme dans le rejection sampling, on va s'aider d'une distribution auxiliaire (proposal distribution) q(z) dont on demande :\n",
    "* Que la génération d'échantillons soit aisée\n",
    "* Qu'elle corresponde le plus possible à la distribution p(z). L'idée étant de disposer du plus d'échantillons possibles dans les régions où p(z) prend des valeurs élevées. Corrolaire : q(z) doit en particulier ne surtout pas être faible ou nulle dans les régions où p(z).f(z) est élevée. On donne via q(z) un poids, une importance plus élevée à ces régions de l'espacé à échantilloner d'où le nom de la méthode.\n",
    "\n",
    "Les quantités p(z)/q(z) apparaissant dans l'expression viennent corriger le biais introduit par le fait qu'on échantillonne l'espace avec la mauvaise distribution (q au lieu de p). \n",
    "\n",
    "Remarques:\n",
    "* Contrairement au rejection sampling, l'ensemble des échantillons sont conservés.\n",
    "* Peut-on se permettre de connaître p(z) à une constante de normalisation près ?\n",
    "* Peut-on se permettre de connaître q(z) à une constante de normalisation près ?\n",
    "* L'utilisation d'une proposal distribution q(z) même judicieuse ne nous débarrasse pas de la curse of dimensionnality : en haute-dimension l'échantillonage de l'espace de z reste très inefficace et s'accompagne du risque que trop peu d'échantillons soient tirés dans les zones où p(z).f(z) est importante. On risque alors d'avoir pour ces régions des estimations très biaisées malgré des variances des poids faibles (??)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
