{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Kernel methods*\n",
    "\n",
    "## *Kernels* définis positifs\n",
    "Considérons un ensemble $\\mathcal{X}$ pouvant prendre des formes très diverses: images, séquences, vecteurs de $\\mathbb{R}^{n}$, etc. On appelle ***kernel* (noyau) défini positif** sur $\\mathcal{X}$ toute fonction $K: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ vérifiant les deux propriétés suivantes: \n",
    "* $K$ est symétrique: $\\forall(x, y) \\in \\mathcal{X}^2, K(x, y) = K(y, x)$\n",
    "* Pour tout $n \\in \\mathbb{N}$, pour toute les séquence de $n$ membres de $\\mathcal{X}$, $\\sum_{i}\\sum_{i}a_{i}a_{j}K(x_{i},x_{j}) \\geq 0$ pour tout vecteur $a$ de $\\mathbb{R}^n$ : $\\forall n \\in \\mathbb{N}, \\forall (x_{1}, ..., x_{n}) \\in \\mathcal{X}^n, \\forall a \\in \\mathbb{R}^n, \\sum_{i}\\sum_{i}a_{i}a_{j}K(x_{i},x_{j}) \\geq 0$\n",
    "\n",
    "De manière équivalente, $K$ est un kernel défini positif sur $\\mathcal{X}$ si et seulement si pour tout $n \\in \\mathbb{N}$, pour toute les séquence de $n$ membres de $\\mathcal{X}$, la matrice $\\textbf{K}$ telle que $k_{i,j} = K(x_{i},x_{j})$ est symétrique définie positive.\n",
    "\n",
    "Remarques: \n",
    "* La matrice $\\textbf{K}$ est parfois nommée matrice de Gram. \n",
    "* Les *kernel methods* correspondent à l'ensemble des algorithmes prenant une matrice telle que $\\textbf{K}$ en entrée. Ces algorithmes peuvent ainsi s'appliquer à de nombreux types de données (images, texte, séquences, vecteurs, etc.), c'est à dire pour tout ceux pour lesquels on peut définir un kernel défini positif $K$. Ces algorithmes sont aussi de ce fait parfaitement modulaires: *kernel* et algorithme (*kernel method*) peuvent être choisis indépendamment.\n",
    "* Par construction (par la définition du *kernel*), les *kernel methods* ont de particulier qu'elles ne reposent que sur des comparaisons paires à paires des données (et font donc appel à une fonction de comparaison, de similarité: le *kernel*).\n",
    "* On note tout de suite que la complexité du calcul de $\\textbf{K}$ (au minimum en $O(n^2)$) peut poser des prolèmes de scalabilité.\n",
    "\n",
    "De part sa définition, on sent qu'un *kernel* défini positif est assez proche d'un produit scalaire. Le théorème d'Aronszajn nous dit qu'un *kernel* défini positif correspond à un produit scalaire dans un certain espace de Hilbert $\\mathcal{H}$. Autrement dit, à tout *kernel* défini positif correspond un espace de Hilbert $H$ et pour lequel il constitue un produit scalaire. \n",
    "\n",
    "**Théorème d'Aronszajn** : $K: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ *kernel* dénifi positif **si et seulement si** il existe un espace de Hilbert $\\mathcal{H}$ appelé *feature space* et une fonction $\\Phi: \\mathcal{X} \\rightarrow \\mathcal{H}$ appelée *feature mapping* tels que: $\\forall (x, y) \\in \\mathcal{X}^2, K(x, y) = \\langle \\Phi(x), \\Phi(y) \\rangle_{\\mathcal{H}}$\n",
    "\n",
    "Remarque: Ce résultat est parfois désigné sous le nom de théorème de Mercer qui énonce le même résultat mais pour une famille d'ensembles $\\mathcal{X}$ restreinte (compacts de $\\mathbb{R}$), le résultat d'Aronszajn étant le plus général. \n",
    "\n",
    "### Premier aperçu du *kernel trick*\n",
    "Les éléments énoncés jusqu'ici sont suffisant pour comprendre la base du *kernel trick* sur lequel on reviendra plus loin.\n",
    "\n",
    "**Le *kernel trick* se base sur le fait qu'un *kernel* défini positif peut se voir comme un produit scalaire (Théorème d'Aronszajn)**\n",
    "\n",
    "Le *kernel trick* concerne et s'applique à toute méthode prenant entrée des vecteurs (de dimension finie) de $\\mathcal{X}^n$ mais qui ne repose que sur des comparaisons paires à paires de ceux-ci (par exemple des produits scalaires). Il découle immédiatement du Théorème d'Aronszajn que la même méthode peut potentiellement s'appliquer à des vecteurs de dimension potentiellement infinie du *feature space* $\\mathcal{H}$ d'un *kernel* en remplaçant tous les opérateurs de comparaison/produits scalaires par une évaluation de ce même *kernel*.\n",
    "\n",
    "**Attention: Comme nous allons le voir, le *feature space* $\\mathcal{H}$ est un espace de fonctions**\n",
    "\n",
    "Remarque: Toutes les méthodes ne faisant finalement apparaître que des produits scalaires entre vecteurs de $\\mathcal{X}$ étant éligibles au *kernel trick*, toutes les méthodes purement *distance-based* le sont donc aussi pour donner des inattendus *kernel* $k$-means, *kernel $k$-nearest-neighbors*, etc.\n",
    "\n",
    "Parmi les avantages majeurs de la méthode, on peut citer:\n",
    "* Qu'on a pas à expliciter et connaître le *feature mapping* $\\Phi$ pour calculer le produit scalaire dans $\\mathcal{H}$, seule la connaissance du *kernel* suffit.\n",
    "* Le *kernel* permet d'évaluer facilement et à peu de frais des produits scalaires dans des espaces dont la dimensionnalité aurait été associée à un fort impact sur la performance (Ex: dimension de $\\mathcal{H}$ en $O(n^d)$ pour les *kernels* polynomiaux, infinie pour le *kernel* gaussien). \n",
    "* Qu'il permet d'appliquer des méthodes à des données non vectorielles (images, texte, séquences, etc.).\n",
    "\n",
    "## *Reproducing Kernel Hilbert Spaces* (RKHS)\n",
    "Dans cette section, nous allons apporter des précisions sur le *feature space* $\\mathcal{H}$ mentionné dans le Théorème d'Aronszajn.\n",
    "\n",
    "Commençons par énoncer que:\n",
    "* Le *feature space* $\\mathcal{H}$ est un **espace de fonctions** de $\\mathcal{X}$ dans $\\mathbb{R}$ : $\\mathcal{H} \\subset \\mathbb{R}^{\\mathcal{X}}$\n",
    "* Corrolaire: chaque point $x \\in \\mathcal{X}$ est représenté dans $\\mathcal{H}$ par une fonction $\\Phi(x) = K_x \\in \\mathcal{H}$ avec $K_x: t \\mapsto K(x, t)$.\n",
    "\n",
    "On a vu plus haut que pour un espace hilbertien $\\mathcal{H} \\subset \\mathbb{R}^{\\mathcal{X}}$, il pouvait exister une fonction $K: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ appelée *kernel* telle que $K$ correspond à un produit scalaire dans $\\mathcal{H}$. On montre qu'il peut exister attaché à $\\mathcal{H}$, un *kernel* particulier appelé *reproducing kernel* (noyau reproduisant) avec une propriété intéressante (la *reproducing property*).\n",
    "\n",
    "Théorème : Soit $\\mathcal{H} \\subset \\mathbb{R}^{\\mathcal{X}}$ espace de fonctions hilbertien réel muni du produit scalaire $\\langle ., .\\rangle_\\mathcal{H}$. Soit une fonction $K: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$, $K$ *kernel* défini positif est appelé *reproducing kernel* de $\\mathcal{H}$ si les deux propriétés suivantes sont vérifiées:\n",
    "* $\\forall x \\in \\mathcal{X}, K_x : t \\mapsto K(x,t) \\in \\mathcal{H}$\n",
    "* *Reproducing property*: $\\forall f \\in \\mathcal{H}, \\forall x \\in \\mathcal{X}, f(x) = \\langle f, K_x \\rangle_\\mathcal{H}$\n",
    "\n",
    "Si ces deux propriétés sont vérifiées : \n",
    "* $K$ est alors le *reproducing kernel* de $\\mathcal{H}$.\n",
    "* $\\mathcal{H}$ est alors un *Reproducing Kernel Hilbert Spaces* (RKHS)\n",
    "\n",
    "**Remarque importante** : Cas particulier de la *reproducing property* : $\\langle \\Phi(x), \\Phi(y) \\rangle_{\\mathcal{H}} = \\langle K_x, K_y \\rangle_{\\mathcal{H}} = K_x(y) = K(x, y)$\n",
    "\n",
    "On montre de plus: \n",
    "* Si $\\mathcal{H}$ est un RKHS, alors il admet un unique *reproducing kernel* $K$.\n",
    "* Un *kernel* défini positif $K$ peut être le *reproducing kernel* d'au plus un RKHS.\n",
    "* Une fonction $K: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ est définie positive si et seulement si c'est un *reproducing kernel*. \n",
    "\n",
    "Pourquoi s'intéresser au *reproducing kernel* ?\n",
    "Comme nous l'avons vu avec le *kernel trick* ci-dessus, on peut chercher en *machine learning* à mapper nos observations $x \\in \\mathcal{X}$ dans un espace de fonctions hilbertien $\\mathcal{H}$ de plus haute dimension (le RKHS) à l'aide d'un *feature mapping* $\\Phi$ : à chaque $x \\in \\mathcal{X}$, $\\Phi$ associe un représentant $K_x = \\Phi(x) \\in \\mathcal{H}$. On va ensuite chercher à résoudre le problème correspondant à la minimisation de notre fonction de coût (avec ici une régularisation L2) dans cet espace:\n",
    "\n",
    "$$ \\min _{f \\in \\mathcal{H}} \\frac{1}{n} \\sum_{i=1}^{n} \\mathcal{L}(y_{i}, f(x_{i})) + \\lambda \\|f\\|_{\\mathcal{H}}^{2}$$\n",
    "\n",
    "Une fois l'optimum trouvé $\\hat{f}$ et considérant le *reproducing kernel* $K$ de $\\mathcal{H}$ connu, sa *reproducing property* va nous permettre de d'évaluer simplement $\\hat{f}$ en tout point $x \\in \\mathcal{X}$. \n",
    "\n",
    "En particulier, on peut s'intéresser aux fonctions linéaires de $\\mathcal{H}$ (les combinaisons linéaires des $K_x = \\Phi(x)$ par exemple) qui peuvent se révéler ne pas être linéaires en $x$. Le passage dans $\\mathcal{H}$ va nous permet ainsi d'obtenir des variantes non linéaires d'algorithmes linéaires bien connus.\n",
    "\n",
    "Autre avantage, notre *feature space* est doté d'un produit scalaire car Hilbertien, une grande variété d'opérations géométriques y existe donc: calcul d'angle (ex: *cosine similarity*), de distances, de projectés, de barycentre, etc. On montre par exemple facilement que la distance $d_K$ entre les représentants de deux points $x$ et $y$ de $\\mathcal{X}$ dans le RKHS associé au kernel $K$ s'exprime: \n",
    "\n",
    "$$\\forall (x,y) \\in \\mathcal{X}^2, d_{K}(x,y)^2 = \\|\\Phi(x)-\\Phi(y)\\|^2=K(x,x)+K(y,y)-2K(x,y)$$\n",
    "\n",
    "La possibilité de calculer un barycentre nous permet par exemple de centrer nos données dans le *feature space* ou encore de calculer la distance d'un point à un ensemble lui même représenté par son barycentre. Attention cependant, le barycentre n'a pas dans le cas général d'antécédent dans $\\mathcal{X}$.\n",
    "\n",
    "### Régularité\n",
    "On montre facilement par Cauchy-Schwarz:\n",
    "$$\\forall f \\in \\mathcal{H} \\subset \\mathbb{R}^{\\mathcal{x}}, \\forall (x,y) \\in \\mathcal{X}^2, |f(x)-f(y)| \\leq \\|f\\|_{\\mathcal{H}}.d_{K}(x,y)$$\n",
    "\n",
    "Ainsi pour deux points $x$ et $y$ de $\\mathcal{X}$ donnés et donc une distance entre ces deux point donnée, les valeurs prises par $f$ pour ces deux points sont contrôlées par la norme de $f$ dans $\\mathcal{H}$. La norme de $f$ dans $\\mathcal{H}$ contrôle donc les variations de $f$ sur $\\mathcal{X}$ : plus sa norme est faible, plus ses variations sont lentes (mesuré par la métrique que définit le *kernel* $K$ sur $\\mathcal{X}$). \n",
    "\n",
    "On comprend ainsi pourquoi le terme $\\lambda \\|f\\|_{\\mathcal{H}}^{2}$ du problème d'optimisation ci-dessus correspond au terme de régularisation.\n",
    "\n",
    "\n",
    "## Exemples de *kernels*\n",
    "\n",
    "### Kernel linéaire ($\\mathcal{X} = \\mathbb{R}^n$)\n",
    "Il s'agit du kernel le plus simple et correspondant au cas le plus simple, au cas avec lequel on pose le problème de la plupart des *kernel methods*. Comme tous les *kernels* définis positifs, il est le *reproducing kernel* d'un RKHS $\\mathcal{H}$. \n",
    "\n",
    "#### Kernel $K$\n",
    "Le *kernel* linéaire correspond à la fonction $K: \\mathbb{R}^n \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}$:\n",
    "\n",
    "$$(x, y) \\in (\\mathbb{R}^n)^2 \\mapsto K(x,y)=x^{\\top}.y$$\n",
    "\n",
    "#### *Feature space* $\\mathcal{H}$ (RKHS) et *feature mapping* $\\Phi$\n",
    "Le RKHS $\\mathcal{H}$ (*feature space*) associé à $K$ correspond à l'espace des fonctions $f_w: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ de la forme $x \\mapsto f_w(x) = \\langle w,x \\rangle_{\\mathbb{R}^n}$ avec $w \\in \\mathbb{R}^n$. On remarque que $\\mathcal{H}$ correspond au dual de $\\mathbb{R}^n$ (ensemble des formes linéaires de $\\mathbb{R}^n$) et est donc hermitien.\n",
    "\n",
    "Le *feature mapping* $\\Phi$ associé correspond par définition à la fonction de $\\mathcal{X}=\\mathbb{R}^n \\rightarrow \\mathcal{H}$ telle que $\\Phi: x \\mapsto K_x$ avec : $K_x: t \\mapsto \\langle x,t \\rangle_{\\mathbb{R}^n}$\n",
    "\n",
    "#### Produit scalaire $\\langle .,. \\rangle_{\\mathcal{H}}$\n",
    "\n",
    "$$\\forall (f, g) \\in \\mathcal{H}^2, \\langle f,g \\rangle_{\\mathcal{H}} = \\langle f_w,f_v \\rangle_{\\mathcal{H}} = \\langle w,v \\rangle_{\\mathbb{R}^n}$$\n",
    "\n",
    "### Kernels polynomiaux ($\\mathcal{X} = \\mathbb{R}^n$)\n",
    "\n",
    "#### Kernel $K$\n",
    "Les *kernels* polynomiaux correspondent aux fonctions $K: \\mathbb{R}^n \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}$ de la forme:\n",
    "\n",
    "$$(x, y) \\in (\\mathbb{R}^n)^2 \\mapsto K(x,y)=(a(x^{\\top}.y)+b)^d$$\n",
    "\n",
    "Avec $a \\in \\mathbb{R}$, $b \\in \\mathbb{R}$ et $d \\in \\mathbb{N}$ les paramètres du *kernel*.\n",
    "\n",
    "#### *Feature space* $\\mathcal{H}$ (RKHS) et *feature mapping* $\\Phi$\n",
    "On ne donnera ici pour la compréhension que le *feature space* et le *feature mapping* du *kernel* polynomial de degré 2: $K:(x,y) \\mapsto (x^{\\top}.y)^2 = Tr(xx^{\\top}, yy^{\\top}) = \\langle xx^{\\top}, yy^{\\top} \\rangle_{F}$\n",
    "\n",
    "La norme associée au produit scalaire $\\langle .,. \\rangle_{F}$ correspond à la norme de Frobenius d'une matrice.\n",
    "\n",
    "Le RKHS $\\mathcal{H}$ (*feature space*) associé à $K$ correspond à l'espace des fonctions $f_S: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ de la forme $x \\mapsto f_S(x) = x^{\\top}Sx$ avec $S \\in S_n(\\mathbb{R})$ symétrique réelle. $\\mathcal{H}$ est bien hilbertien car isomorphe à $S_n(\\mathbb{R})$ (et donc de dimension en $O(n^2)$).\n",
    "\n",
    "Le *feature mapping* $\\Phi$ associé correspond par définition à la fonction de $\\mathcal{X}=\\mathbb{R}^n \\rightarrow \\mathcal{H}$ telle que $\\Phi: x \\mapsto K_x$ avec : $K_x: t \\mapsto \\langle xx^{\\top}, tt^{\\top} \\rangle_{F} = f_{xx^{\\top}}(t)$\n",
    "\n",
    "Remarques: \n",
    "* $[xx^{\\top}]_{ij} = x_{i}x_{j}$, on retrouve les *features* polynomiales associées au *feature space*.\n",
    "* **Tous les éléments de $\\mathcal{H}$ n'admettent pas forcément un antécédent dans $\\mathcal{X}$ par $\\Phi$**:\n",
    "    * Par exemple dans le cas du *kernel* polynomial d'ordre 2 ci-dessus, la fonction $f_S$ associé à la matrice symétrique $S = \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix}$ ne peut pas être obtenue à l'aide de $\\Phi$ dont l'image ne se compose que des fonctions $f_S$ avec $S = \\begin{pmatrix} x_1 & x_1.x_2 \\\\ x_1.x_2 & x_2 \\end{pmatrix}$.\n",
    "    * Cette remarque s'applique aussi par exemple pour le barycentre des représentants $K_x$ dans le *feature space* (qui peut ne pas admettre d'antécédent dans $\\mathcal{X}$).\n",
    "\n",
    "#### Produit scalaire $\\langle .,. \\rangle_{\\mathcal{H}}$\n",
    "Les éléments données ci-dessus laissaient déjà entrevoir que le produit scalaire sur $\\mathcal{H}$ auquel correspond le *kernel* polynomial d'ordre 2 présenté précédemment est le produit scalaire associé à la norme de Frobenius:\n",
    "\n",
    "$$\\forall (f, g) \\in \\mathcal{H}^2, \\langle f,g \\rangle_{\\mathcal{H}} = \\langle f_{S_1},f_{S_2} \\rangle_{\\mathcal{H}} = \\langle S_1, S_2 \\rangle_{F} = Tr(S_1^{\\top}S_2)$$\n",
    "\n",
    "### Kernel gaussien ($\\mathcal{X} = \\mathbb{R}^n$)\n",
    "\n",
    "#### Kernel $K$\n",
    "Le *kernel* gaussien correspond à la fonction $K: \\mathbb{R}^n \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}$ :\n",
    "\n",
    "$$(x, y) \\in (\\mathbb{R}^n)^2 \\mapsto K(x,y)=exp(-\\frac{\\|x-y\\|^2}{2\\sigma^2})$$\n",
    "\n",
    "Avec $\\sigma > 0$ un paramètre appelé \"bande passante\". On trouve parfois ce kernel dans sa forme normalisée (à l'aide d'un facteur $\\frac{1}{(\\sqrt{2\\pi}\\sigma)^n}$). Le terme de bande passante s'interprète mieux en considérant la fonction $K_x: t \\mapsto exp(\\frac{\\|x-t\\|^2}{2\\sigma^2})$. Cette fonction d'une seule variable est centrée sur $x$ et décroit d'autant plus vite en s'éloignant de $x$ que $\\sigma$ est faible.\n",
    "\n",
    "Le *kernel* est en fait un cas particulier de *radial basis functions* (RBF - fonctions centrées sur un point), terme parfois utilisé pour le désigner.\n",
    "\n",
    "On peut citer quelques particularités du *kernel* gaussien:\n",
    "* $\\forall x \\in \\mathcal{X} K(x,x) = 1 = \\|\\Phi(x)\\|^2$ : Tous les représentants $\\Phi(x)$ sont donc localisés sur la sphère de rayon 1 dans le *feature space*.\n",
    "* $\\forall (x,y) \\in \\mathcal{X}^2 0 \\geq K(x,x) \\leq 1$ : Contrairement à d'autres *kernels* qui peuvent prendre leurs valeurs dans tout $\\mathbb{R}^+$, le *kernel* gaussien constitue une mesure de similarité plus parlante.\n",
    "\n",
    "#### *Feature space* $\\mathcal{H}$ (RKHS) et *feature mapping* $\\Phi$\n",
    "Le *kernel* gaussien a la particularité d'être associé à un RHKS de dimension infinie qu'on donne pour information:\n",
    "\n",
    "$$\\mathcal{H} = \\{f \\in \\mathcal{C}^0(\\mathbb{R}^n), f \\in L_{1}(\\mathbb{R}^n), \\int_{\\mathbb{R}^n}|\\mathcal{F}[f](t)|^{2}exp(\\frac{\\sigma^2\\|t\\|_{\\mathbb{R}^n}^{2})}{2})dt \\}$$\n",
    "\n",
    "Avec $\\mathcal{F}$ la transformée de Fourier de $f$. Le *feature space* étant de dimension infinie, le *feature mapping* possède \"une infinité de composantes\". On peut chercher à les expliciter en développant $K(x, y)$ en série (qui compte donc une infinité de termes) dont on cherchera à réécrire le terme principal pour obtenir un produit scalaire. \n",
    "\n",
    "On donne (notamment pour les problèmes d'optimisation décrits plus bas) la norme correspondant au *kernel* dans le RKHS décrit ci dessus: \n",
    "\n",
    "$$\\forall f \\in \\mathcal{H}, \\|f\\|_{\\mathcal{H}}^{2} = \\frac{1}{(2\\pi)^n} \\int_{\\mathbb{R}^n}|\\mathcal{F}[f](t)|^{2}exp(\\frac{\\sigma^2\\|t\\|_{\\mathbb{R}^n}^{2})}{2})dt$$\n",
    "\n",
    "### Autres kernels\n",
    "La définition d'un *kernel* est finalement assez générale. On peut définir d'autres *kernels* sur d'autre ensembles $\\mathcal{X}$ comme $\\mathbb{N}$, $\\mathbb{R}^+$, $\\{-1, 1\\}$, etc. mais aussi pour des objets plus complexes comme des images, des séquences de texte, des séries temporelles, des graphes, etc.\n",
    "\n",
    "## Résolution du problème d'optimisation: Théorème du représentant\n",
    "Reprenons le problème suivant:\n",
    "\n",
    "$$ \\min _{f \\in \\mathcal{H}} \\frac{1}{n} \\sum_{i=1}^{n} \\mathcal{L}(y_{i}, f(x_{i})) + \\lambda \\|f\\|_{\\mathcal{H}}^{2}$$\n",
    "\n",
    "Sa résolution peut s'avérer à priori ardue du fait en autres que $\\mathcal{H}$ est de dimension infinie. C'est là que le théorème du représentant (*representer theorem*) vient considérablement simplifier les choses.\n",
    "\n",
    "**Théorème du représentant**: Soit $K$ un *kernel* défini positif sur $\\mathcal{X}$, $\\mathcal{H}$ le RKHS associé, $S$ un ensemble fini de $n$ points de $\\mathcal{X}$ (le *training set*) et $\\Psi:\\mathbb{R}^{n+1} \\rightarrow \\mathbb{R}$ strictement croissante par rapport à la dernière variable, alors toute solution $\\hat{f}$ au problème suivant:\n",
    "\n",
    "$$\\min _{f \\in \\mathcal{H}} \\Psi(f(x_{1}), ..., f(x_{n}), \\|f\\|_{\\mathcal{H}})$$\n",
    "\n",
    "admet une représentation de la forme: $\\forall x \\in \\mathcal{X}, \\hat{f}(x) = \\sum_{i}\\alpha_{i}K_{x_{i}}(x)$ avec $\\alpha_{i} \\in \\mathbb{R}$.\n",
    "\n",
    "On en tire deux remarques importantes: \n",
    "* $\\hat{f}$ appartient à un sous-espace de dimension finie de $\\mathcal{H}$ ce qui permet une résolution efficace du problème même si $\\mathcal{H}$ de dimension infinie.\n",
    "* $\\hat{f}$ s'écrit comme une combinaison linéaire des représentants $K_{x_{i}} = \\Phi(x_{i})$ des $x_{i} \\in \\mathcal{X}$ dans $\\mathcal{H}$. $\\hat{f}$ est donc une fonction linéaire des $\\Phi(x_{i})$.\n",
    "\n",
    "L'évaluation de la solution obtenue en un point de $\\mathcal{X}$ est ensuite rendue triviale par la *reproducing property*:\n",
    "\n",
    "$$\\forall x \\in \\mathcal{X}, \\hat{f}(x) = \\langle \\hat{f}, K_{x} \\rangle_{\\mathcal{H}} = \\sum_{i}\\alpha_{i}\\langle K_{x_{i}}, K_{x} \\rangle_{\\mathcal{H}} $$\n",
    "\n",
    "Le plus souvent la fonction $\\Psi$ est de la forme: \n",
    "\n",
    "$$\\Psi(f(x_{1}), ..., f(x_{n}), \\|f\\|_{\\mathcal{H}}) = C(f(x_{1}), ..., f(x_{n})) + \\Omega(\\|f\\|_{\\mathcal{H}})$$\n",
    "\n",
    "Avec:\n",
    "* $C$ qui mesure la *goodness of fit* de $f$ au problème étudié (régression, classification, réduction de dimension, etc.).\n",
    "* $\\Omega$ strictement croissante et permettant s'assurer d'une certaine régularité de la solution retenue.\n",
    "\n",
    "En pratique on passe le plus souvent d'un problème d'optimisation sur $f \\in \\mathcal{H}$ à un problème plus simple à résoudre optimisant $\\alpha \\in \\mathbb{R}^{n}$. On remarque en effet que:\n",
    "* $\\hat{f}(x_{j}) = \\sum_{i}\\alpha_{i}K_{x_{i}}(x_{j}) = \\sum_{i}\\alpha_{i}K(x_{i}, x_{j}) = [\\textbf{K}\\alpha]_{j} $ \n",
    "* $\\|\\hat{f}\\|_{\\mathcal{H}}^2 = \\sum_{i}\\sum_{j}\\alpha_{i}\\alpha_{j}K(x_{i}, x_{j}) = \\alpha^{\\top}\\textbf{K}\\alpha$\n",
    "\n",
    "On en déduit immédiatement que le problème\n",
    "\n",
    "$$\\min _{f \\in \\mathcal{H}} \\Psi(f(x_{1}), ..., f(x_{n}), \\|f\\|_{\\mathcal{H}})$$\n",
    "\n",
    "est équivalent au problème suivant : \n",
    "\n",
    "$$\\min _{\\alpha \\in \\mathbb{R}^n} \\Psi([\\textbf{K}\\alpha]_{1}, ..., [\\textbf{K}\\alpha]_{n}), \\alpha^{\\top}\\textbf{K}\\alpha)$$\n",
    "\n",
    "On ajouter qu'on peut donner deux interprétations complémentaires à la plupart des *kernel methods*:\n",
    "* Une interprétation géométrique : Grace au *kernel trick* on applique un algorithme linéaire dans le *feature space* (la plupart des méthode travaillant en fait dans le sous-espace généré par les représentants (*embeddings*) des points de l'espace de départ à notre disposition) obtenant potentiellement une version non linéaire de celui-ci. Cette interprétation aboutie le plus souvent au problème primal de la méthode considérée.\n",
    "* Une interprétation fonctionnelle où la méthode peut se voir comme un problème d'optimisation sur le RKHS associé au kernel utilisé. Cette interprétation aboutie le plus souvent au problème dual de la méthode considérée.\n",
    "\n",
    "### Un exemple en régression : Kernel Ridge Regression\n",
    "Problème: On cherche la fonction $\\hat{f}: \\mathcal{X} \\rightarrow \\mathbb{R}$ solution du problème d'optimisation suivant pour un *training set* $\\mathcal{S}_n = (x_{i}, y_{i})_{i=1, \\dots, n} \\in (\\mathcal{X}, \\mathbb{R})^n$: \n",
    "\n",
    "$$min_{f \\in \\mathcal{H}} \\frac{1}{n}\\sum_{i=1}^{n}(y_i-f(x_i))^2 + \\lambda\\|f\\|_{\\mathcal{H}}^2$$\n",
    "\n",
    "Notre régression est des moindres carrées du fait du choix de la MSE comme fonction de coût. On s'intéresse directement au problème régularisé afin d'éviter les problèmes d'*overfitting* (et donc d'instabilité de la solution) notamment dans les cas où $\\mathcal{H}$ est de très grande dimension.\n",
    "\n",
    "Soit $K$ le kernel sur $\\mathcal{X}$ associé au RKHS $\\mathcal{H}$ et de matrice de Gram $\\textbf{K}$, par le théorème du représentant, on sait que $\\hat{f}(x) = \\sum_{i=1}^{n}\\alpha_{i}K(x_{i}, x)$ avec $\\alpha \\in \\mathbb{R}^n$ et on a de plus montré que:\n",
    "* $(\\hat{f}(x_1), \\dots, \\hat{f}(x_n))^{\\top} = \\textbf{K}\\alpha$\n",
    "* $\\|\\hat{f}\\|_{\\mathcal{H}}^2 = \\sum_{i}\\sum_{j}\\alpha_{i}\\alpha_{j}K(x_{i}, x_{j}) = \\alpha^{\\top}\\textbf{K}\\alpha$\n",
    "\n",
    "Le problème de la *least squares linear ridge regression* se simplifie alors considérablement en:\n",
    "\n",
    "$$min_{\\alpha \\in \\mathbb{R}^n} \\frac{1}{n}(\\textbf{K}\\alpha-y)^{\\top}(\\textbf{K}\\alpha-y) + \\lambda\\alpha^{\\top}\\textbf{K}\\alpha$$\n",
    "\n",
    "Il suffit alors d'égaliser à zéro le gradient de cette expression par rapport à $\\alpha$ pour obtenir:\n",
    "\n",
    "$$\\hat{\\alpha} = (\\textbf{K} + n\\lambda I_n)^{-1}y$$\n",
    "\n",
    "Remarque: la matrice ci-dessus est inversible car $\\textbf{K}$ définie positive: se placer dans une base de diagonalisation de $\\textbf{K}$: on ajoute des quantités strictement positives sur toute la diagonale, l'ensemble n'a donc pas de valeurs propres nulles.\n",
    "\n",
    "#### Exemple avec le *kernel* linéaire \n",
    "\n",
    "**Remarque importante**: Se rappeler que le *feature space* $\\mathcal{H}$ est un espace de fonctions, **choisir un *kernel* c'est choisir une famille de fonctions**. D'après le théorème du représentant $\\hat{f}$ est une combinaison linéaire finie de fonctions de $\\mathcal{H}$ ce qui ne veut pas dire que $\\hat{f}$ est linéaire en $x$. Dans le cas général, $\\hat{f}$ sera une combinaison linéaire de fonctions non linéaires en $x$. Dans le cas particulier du *kernel* linéaire, $\\mathcal{H}$ se trouve être un espace de fonctions linéaires en $x$, une combinaison linéaire de fonction linéaires en $x$ se trouvant être aussi linéaire en $x$, $\\hat{f}$ sera aussi linéaire en $x$. \n",
    "\n",
    "Si $K$ *kernel* linéaire et pour $\\mathcal{X} = \\mathbb{R}^p$, on retrouve le cas de la régression linéaire où $\\hat{f}$ est une fonction linéaire des $x_i$ (cf. remarque précédente).\n",
    "\n",
    "Chaque fonction $f$ de $\\mathcal{H}$ pouvant être représentée par un vecteur $w \\in \\mathcal{X} = \\mathbb{R}^p$, on désigne la solution optimale $\\hat{f}$ par son vecteur $\\hat{w}$ associé. D'après le théorème du représentant: $\\hat{w} = \\sum_{i}\\alpha_{i}x_{i}$ avec $\\alpha \\in \\mathbb{R}^n$ et $x_{i} \\in \\mathbb{R}^p$ : chaque $\\alpha_i$ est un scalaire mais chaque $x_i$ est un vecteur de $\\mathbb{R}^p$. Ce qui peut se réécrire $\\hat{w} = X^{\\top}\\alpha$\n",
    "\n",
    "On a de plus $\\textbf{K}_{ij} = \\langle x_i, x_j \\rangle_{\\mathbb{R}^n} = \\sum_{k}x_{ik}x_{jk} = (X^{\\top}X)_{ij}$, et donc $\\textbf{K} = XX^{\\top}$. On a donc : \n",
    "\n",
    "$$\\hat{w} = X^{\\top}(XX^{\\top} + n\\lambda I_n)^{-1}y$$ \n",
    "\n",
    "On remarque que cette expression est légèrement différente de l'expression \"classique\" des coefficients d'une *least squares linear ridge regression*: \n",
    "\n",
    "$$\\hat{w} = (X^{\\top}X + n\\lambda I_p)^{-1}X^{\\top}y$$\n",
    "\n",
    "On peut montrer que ces deux expressions matricielles sont égales. Mais d'où la différence provient-t-elle ?\n",
    "* La seconde équation dite classique est obtenue par résolution d'un problème d'optimisation minimisant sur $w \\in \\mathbb{R}^p$. Il s'agit de la résolution du problème primal.\n",
    "* La première équation est obtenue par résolution d'un problème d'optimisation minimisant sur $\\alpha \\in \\mathbb{R}^n$ qui par le théorème du représentant est équivalent à une minimisation sur $f \\in \\mathcal{H}$. Il s'agit de la résolution du problème dual.\n",
    "\n",
    "On remarque de plus que dans le cas du primal, la matrice à inverser est $p \\times p$ alors que dans le cas du dual (et plus largement de l'utilisation de *kernels*) elle est $n \\times n$. Suivant le problème, il peut être plus avantageux de résoudre le primal (beaucoup d'observations $n > p$) ou le dual (beaucoup de features / haute dimension $p > n$).\n",
    "\n",
    "### Exemples en classification\n",
    "#### *Kernel logistic regression*\n",
    "Bien que le problème d'optimisation de la régression logistique soit avant tout un problème de maximisation de la vraisemblance, il est aussi équivalent à la minimisation d'une *loss function* particulière: la *logistic loss*. \n",
    "\n",
    "Le problème peut donc s'écrire: \n",
    "\n",
    "$$min_{f \\in \\mathcal{H}} \\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{L}(y_i, f(x_i)) + \\lambda\\|f\\|_{\\mathcal{H}}^2$$\n",
    "\n",
    "Avec $\\mathcal{L}(y_i, f(x_i))=ln(1+exp(-yf(x)))$\n",
    "\n",
    "La fonction $\\hat{f}$ obtenue après résolution peut s'interpréter comme un *regularized conditional maximum likelihood estimator*. Là encore le problème n'est pas simple à résoudre tel quel, mais en prenant $\\mathcal{H}$ RKHS et $K$ son *kernel* associé sur $\\mathcal{X}$, le théorème du représentant nous permet comme dans les exemples ci-dessus de simplifier le problème en:\n",
    "\n",
    "$$min_{\\alpha \\in \\mathbb{R}^n} \\frac{1}{n}\\sum_{i}^{n}ln(1+exp(-y_{i}[\\textbf{K}\\alpha]_{i})) + \\lambda\\alpha^{\\top}\\textbf{K}\\alpha$$\n",
    "\n",
    "Toutes les fonctions de $\\alpha$ sont ici régulières et convexes. La recherche de l'optimum peut se faire avec des méthodes numériques classiques (ex: méthode de Newton). \n",
    "\n",
    "#### Kernel SVM\n",
    "De façon générale, le problème des classificateurs à vaste marge (*large-margin classifiers*) dont les SVM font partie peut s'écrire: \n",
    "\n",
    "$$min_{f \\in \\mathcal{H}} \\frac{1}{n}\\sum_{i=1}^{n}\\varphi(y_if(x_i)) + \\lambda\\|f\\|_{\\mathcal{H}}^2$$\n",
    "\n",
    "Avec $\\varphi$ une *loss function* fonction de la marge $yf(x)$. Il existe toute une théorie autour des propriété que doit avoir une \"bonne\" fonction $\\varphi$. Elle sont toutes au moins positives et décroissantes (souvent nulles pour les marges positives). Avoir $\\varphi$ convexe et suffisamment régulière est souvent recherché.\n",
    "\n",
    "Dans le cas des SVM, $\\varphi$ est la *hinge loss* avec $\\varphi: u \\mapsto max(1-u,0)$ avec $u=yf(x)=y(x^{\\top}.w +b)$ la marge. Comme dans les exemples ci-dessus, en prenant $\\mathcal{H}$ RKHS et $K$ son *kernel* associé sur $\\mathcal{X}$, le théorème du représentant nous permet de simplifier le problème en:\n",
    "\n",
    "$$min_{\\alpha \\in \\mathbb{R}^n} \\frac{1}{n}\\sum_{i}^{n}\\varphi(y_{i}[\\textbf{K}\\alpha]_{i}) + \\lambda\\alpha^{\\top}\\textbf{K}\\alpha$$\n",
    "\n",
    "A cause de la *hinge loss*, l'objectif n'est pas régulier, on va donc reparamétriser le problème en introduisant des *slack variables* $\\xi_i$ qui vont nous permettre d'obtenir un problème régulier et qui nous fait retomber sur le cas du SVM non séparable. Avec ces *slack variables*, toutes les marges doivent pouvoir être satisfaites (supérieures à 1), quitte à ce que $\\xi_i$ soit très grand. Toutes les observations vérifie donc la contrainte $y_{i}[\\textbf{K}\\alpha]_{i} - \\xi_i \\geq 1$. $\\xi_i$ mesurant l'écart éventuel de la marge à 1, $\\xi_i \\geq 0$. On remarque que $\\xi_i = 0$ si la marge $y_{i}[\\textbf{K}\\alpha]_{i}$ est supérieure à 1 et $\\xi_i = 1 - y_{i}[\\textbf{K}\\alpha]_{i}$  si la marge est inférieure à 1. $\\xi_i$ prend donc exactement les mêmes valeurs que la *hinge loss* $\\varphi(y_{i}f(x_i))$ et on peut donc substituer $\\varphi(y_{i}[\\textbf{K}\\alpha]_{i})$ par $\\xi_i$ dans la fonction objectif. Le problème ci-dessus peut alors se réécrire : \n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{min_{\\alpha, \\xi \\in (\\mathbb{R}^n)^2}} & {\\frac{1}{n}\\sum_{i}^{n}\\xi_i + \\lambda\\alpha^{\\top}\\textbf{K}\\alpha} \\\\ {\\text { subject to }} & {y_{i}[\\textbf{K}\\alpha]_{i} \\geq 1-\\xi_{i}}  \\\\ {} & {\\xi_{i} \\geq 0} \\end{array}\n",
    "$$\n",
    "\n",
    "On obtient un problème d'optimisation sous contraintes classique avec un ojectif et des contraintes régulières et convexes. On le résout sans problème à l'aide des KKT.\n",
    "\n",
    "### Un exemple en réduction de dimension : Kernel PCA\n",
    "#### Rappel sur le problème de la PCA\n",
    "La PCA présuppose que les données $x \\in \\mathbb{R}^p$ sont **centrées** (mais pas forcément réduites): $\\frac{1}{n}\\sum_{i}^{n}x_{i}=0$. Le problème de la PCA peut se voir comme la rechercher de directions $w$ de variance (empirique) maximale, cette variance suivant l'axe $w$ s'écrivant, les données étant centrées: \n",
    "\n",
    "$$var_{w} = \\frac{1}{n}\\sum_{i=1}^{n}h_{w}(x_{i})^2$$\n",
    "\n",
    "Avec $h_{w}(x_{i})$ le projeté orthogonal de $x_i$ sur $w$ qui dans $\\mathbb{R}^{p}$ s'écrit: $h_{w}(x_{i})=x^{\\top}.\\frac{w}{\\|w\\|}$\n",
    "\n",
    "Trouver la $i^e$ direction $w_i$ de variance maximale correspond alors à la résolution du problème d'optimisation suivant:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{max_{w \\in \\mathbb{R}^p}} & {\\frac{1}{n}\\sum_{i}^{n}(x^{\\top}.w)^2 = \\frac{1}{n}w^{\\top}X^{\\top}Xw = w^{\\top}\\Sigma w} \\\\ {\\text { subject to }} & {w^{\\top}w=1}  \\\\ {} & {w \\perp (w_1, w_2, \\dots, w_{i-1})} \\end{array}\n",
    "$$\n",
    "\n",
    "Remarques: \n",
    "* Pour la première composante principale $w_1$, le problème n'intègre qu'une seule contraine: $w^{\\top}w=1$\n",
    "* $\\Sigma$ correspond à la matrice de covariance des données et $\\frac{1}{n}X^{\\top}X = \\Sigma \\in \\mathcal{M}(\\mathbb{R}^p)$ du fait que les données sont centrées.\n",
    "\n",
    "On montre ensuite que le jeu des directions $w_i$ est une base orthonormée de vecteurs propres de $\\Sigma$. La PCA revient de ce point de vue (si les données sont centrées) à diagonaliser la matrice de covariance des données $\\Sigma$.\n",
    "#### Kernel PCA\n",
    "Soient $x_i \\in \\mathcal{X}$, $K$ un *kernel* sur $\\mathcal{X}$, $\\mathcal{H}$ et $\\Phi$ le *feature space* (RKHS) et le *feature mapping* associés. On va reprendre le raisonnement présenté ci-dessus mais dans le *feature space* $\\mathcal{H}$. \n",
    "\n",
    "On suppose les données centrées dans le *feature space*: $\\frac{1}{n}\\sum_{i}^{n}\\Phi(x_{i})=0$. On va chercher la direction $f \\in \\mathcal{H}$ maximisant la variance empirique. Le projeté orthogonal $h_{f}(\\Phi(x))$ s'écrit:\n",
    "\n",
    "$$h_{f}(\\Phi(x)) = \\frac{\\langle \\Phi(x), f \\rangle_{\\mathcal{H}}}{\\|f\\|_{\\mathcal{H}}} = \\frac{f(x)}{\\|f\\|_{\\mathcal{H}}}$$\n",
    "\n",
    "Le problème de maximisation de la variance correspondant à la recherche de la $i^e$ direction de variance maximale s'écrit donc:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{max_{f \\in \\mathcal{H}}} & {\\frac{1}{n}\\sum_{i}^{n}h_{f}(\\Phi(x))^2 = \\frac{1}{n}\\sum_{i}^{n}f(x_i)^2} \\\\ {\\text { subject to }} & {\\|f\\|_{\\mathcal{H}}=1}  \\\\ {} & {f \\perp (f_1, f_2, \\dots, f_{i-1})} \\end{array}\n",
    "$$\n",
    "\n",
    "On remarque que le *kernel trick* s'est appliqué: on a eu à aucun moment à effectuer de calculs dans le *feature space*.\n",
    "\n",
    "En reparamétrisant un peu le problème, on montre facilement que le théorème du représentant s'applique. Les directions $f_i$ peuvent ainsi s'écrire sous la forme:\n",
    "\n",
    "$$\\forall x \\in \\mathcal{X}, f_i(x)=\\sum_{j=1}^{n}\\alpha_{i,j}K(x_j, x)$$\n",
    "\n",
    "Ainsi, on montre:\n",
    "* $\\langle f_i, f_j \\rangle_{\\mathcal{H}} = \\alpha_{i}^{\\top}\\textbf{K}\\alpha_{j}$\n",
    "* D'où: $\\|f_i\\|_{\\mathcal{H}}^2=\\alpha_{i}^{\\top}\\textbf{K}\\alpha_{i}$\n",
    "* D'où: $\\sum_{i}^{n}f(x_i)^2 = \\alpha_{i}^{\\top}\\textbf{K}^{2}\\alpha_{i}$\n",
    "\n",
    "Le problème d'optimisation se simplifie alors en: \n",
    "$$\n",
    "\\begin{array}{cl}{max_{\\alpha \\in \\mathbb{R}^n}} & {\\alpha^{\\top}\\textbf{K}^{2}\\alpha} \\\\ {\\text { subject to }} & {\\alpha^{\\top}\\textbf{K}\\alpha=1} \\\\ {} & {\\alpha^{\\top}\\textbf{K}\\alpha_{j}=0 \\; j=1, \\dots, i-1} \\end{array}\n",
    "$$\n",
    "\n",
    "$\\textbf{K}$ étant symétrique réelle et donc diagonalisable, on peut montrer que la solution au problème d'optimisation est $\\alpha_i = \\frac{u_i}{\\sqrt{\\Delta_i}}$ avec $u_i \\in \\mathbb{R}^n$ vecteur propre associé à la $i^e$ plus grande valeur propre $\\Delta_i$.\n",
    "\n",
    "Remarque: $\\mathcal{H}$ de dimension infinie, en théorie on pourrait avoir un ensemble infini de directions principales. Or par le théorème du représentant, le vecteurs qu'on appelle \"directions principales\" appartiennent en fait à un espace de dimension finie $n$.\n",
    "\n",
    "La résolution de la Kernel PCA revient donc à diagonaliser la matrice de Gram $\\textbf{K}$ de données centrée. On peut vérifier qu'on retrouve le cas de la PCA \"classique\" si le *kernel* choisi est le *kernel* linéaire. Dans ce cas, la $i^e$ directions $w_i = X^{\\top}\\alpha_i$ combinaison linéraire des $n$ vecteurs de données $x_i$ de $\\mathbb{R}^p$. \n",
    "\n",
    "**Attention**: Comme dans le cas de la *kernel ridge regression* plus haut, choisir un *kernel* linéaire ne nous fait pas retomber exactement sur le problème obtenu par un raisonnement \"classique\" de maximisation de la variance. Dans le cas de la PCA/Kernel PCA, la matrice de Gram $\\textbf{K}$ ne coincide pas avec la matrice de covariance $\\Sigma$ quand on choisit un *kernel* linéaire. Cela se voit immédiatement au niveau des dimensions: dans le cas d'un problème à $n$ point $x \\in \\mathbb{R}^p$, $\\textbf{K}$ est toujours $n \\times n$ et la matrice de covariance des données $\\Sigma$ est $p \\times p$. On peut montrer que les résolutions des problèmes d'optimisation sur $\\alpha \\in \\mathbb{R}^n$ pour $\\textbf{K}$ ou $w \\in \\mathbb{R}^p$ pour $\\Sigma$ aboutissent au même maximum de la fonction objectif, juste l'un des deux problèmes correspond au primal ($\\Sigma = X^{\\top}X$), l'autre étant son dual ($\\textbf{K} = XX^{\\top}$). \n",
    "\n",
    "**Remarques sur le centrage des données** : \n",
    "* Le fait de supposer les données centrées permet surtout d'alléger les calculs. Ne pas l'oublier si l'outils s'attend à des données centrées.\n",
    "* Dans `sklearn`, qu'on utilise une PCA ou une Kernel PCA, on a pas forcément à passer une *data matrix* centrées à la méthode `fit`. Celle-ci se charge dans les différents cas de centrer la matrice des données (cas de la PCA) ou la matrice de Gram $\\textbf{K}$ (dans le cas de la Kernel PCA) avant toute décomposition de matrice.\n",
    "* Dans le cas de la Kernel PCA, la matrice de Gram centrée $\\textbf{K}^{c}$ peut s'exprimer en fonction de la matrice de Gram des données $\\textbf{K}$ en remarquant que $\\textbf{K}_{ij}^{c} = \\langle \\Phi(x_i) - \\frac{1}{n}\\sum_{k}^n\\Phi(x_k), \\Phi(x_j) - \\frac{1}{n}\\sum_{k}^n\\Phi(x_k) \\rangle_{\\mathcal{H}}$ et en développant le calcul.\n",
    "\n",
    "### Un exemple pour une méthode géométrique: *Kernel* $k$-means\n",
    "L'algorithme du $k$-means étant purement basé sur des distances il est par le *kernel trick* éligible à l'utilisation de *kernels*. Plus généralement, l'utilisation d'un *kernel* en *clustering* peut se justifer par l'hypothèse que le problème sera plus facilement *clusterisable* dans le *feature space*. Voir par exemple [ce cours](http://members.cbio.mines-paristech.fr/~jvert/svn/kernelcourse/slides/master2017/master2017.pdf) à ce sujet.\n",
    "\n",
    "## Retour sur le *kernel trick*\n",
    "On a vu que pour pouvoir être classifiée comme *kernel method*, un algorithme ne devait à un moment de plus faire apparaître que des produits scalaires entre membres de $\\mathcal{X}$. \n",
    "\n",
    "On a en fait un critère plus large: il suffit que le problème puisse se mettre sous une forme d'un problème d'optimisation fonctionnelle dans un RKHS pour lequel le théorème du représentant est valable pour que l'algorithme soit une *kernel method*. C'est ainsi qu'on a pu avoir une *kernel ridge regression* alors qu'y chercher la forme/le problème d'optimisation ne faisant apparaître que des produits scalaires n'était pas évidente.\n",
    "\n",
    "Remarques: \n",
    "* Pour une *kernel method* donnée, la forme avec produits scalaires est en général le dual du problème d'optimisation correspondant à la méthode et qui est aussi le problème obtenu après application des simplifications apportées par le théorème du représentant au problème d'optimisation fonctionnelle.\n",
    "* Ce problème dual permettant de mettre en évidence le *kernel trick* est un problème d'optimisation sur $\\mathbb{R}^n$ avec $n$ le nombre d'observations là où le primal en général obtenu par des méthodes plus intuitives, plus géométriques est un problème d'optimisation sur $\\mathbb{R}^p$ avec $p$ le nombre de *features*.\n",
    "\n",
    "L'avantage apporté par le *kernel trick* est plus évident après avoir vu le théorème du représentant. Les *kernel methods* sont finalement toutes des problèmes d'optimisation fonctionnelle dans un RKHS $\\mathcal{H}$, la fonction finalement apprise pour la régression/classification correspondant à celle réalisant l'optimum. Le théorème du représentant nous dit que cette fonction est une combinaison linéaire de fonctions de $\\mathcal{H}$ potentiellement non linéaires en $x$. Le cas du *kernel* linéaire nous ramène au cas simple où la fonction apprise est linéaire mais en utilisant un autre *kernel* la même méthode va être capable d'apprendre des fonction non linéaires en $x$.\n",
    "\n",
    "![kernel-trick](./images/kernel_optim_problems.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
