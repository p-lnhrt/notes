{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Random forests*\n",
    "Les *random forests* appartiennent à la famille des *ensemble methods* rassemblant des modèles composé de multiples modèles individuels en général du même type, le modèle d'ensemble performant mieux que chacun de ses composants individuels. On distingue deux grands schéma de construction de ces modèles: \n",
    "* Le *bagging* qui consiste simplement à agréger les modèles individuels, chacun étant entrainé (potentiellement en parallèle) sur un échantillon *bootstrap* issu du *training set*.\n",
    "* Le *boosting* où les modèles individuels sont entrainés séquentiellement.\n",
    "\n",
    "Les *random forests* sont construits par *bagging*, les modèles individuels étant des arbres de décision. \n",
    "\n",
    "## *Bagging*\n",
    "Le *bagging* est une technique consistant à former un estimateur aux bonnes performances (variance faible) par aggrégation d'estimateurs moins performants. Le mot est formé par la concaténation des mots *bootstrap* et *aggregation* qui désignent les deux composantes de la méthode: \n",
    "* Le *bagging* est d'abord une *bootstrap method*: on génère à partir de notre *training set*, $B$ échantillons *bootstrap* (échantillonnage avec remise) sur chacun desquels va être entrainé un modèle, tous les modèles étant le plus souvent de même type.\n",
    "* On construit l'estimateur final par agrégation des estimateurs obtenus sur chacun des échantillons bootstrap. La méthode d'agrégation varie, il s'agit le plus souvent de la moyenne en régression et du vote majoritaire/moyenne des probabilités en classification.\n",
    "\n",
    "On illustre souvent l'idée du bagging en rappellant que pour $n$ variables aléatoire $X_i$ indépendantes et identiquement distribuées d'espérance $\\mu$ et de variance $\\sigma^2$, la moyenne empirique $\\bar{X}$ est un estimateur sans biais de $\\mu$ (qui y converge par la loi des grands nombres) et de variance $\\sigma^2/n$. La moyenne des observations est ainsi un bien meilleur estimateur de l'espérance qu'une seule observation utilisée comme estimateur; la moyenne des estimateurs constitue un meilleur estimateur que chacun des estimateur la composant.\n",
    "\n",
    "Dans notre cas, ce sont nos modèles individuels (dits \"faibles\" - *weak learners*) qui jouent le rôle des variables aléatoires $X_i$ précédentes. Chaque modèle est un estimateur et chaque estimation correspond à un tirage utilisant la densité de probabilité de l'estimateur d'où la description par une variable aléatoire.\n",
    "\n",
    "Cette analogie ne rend cependant pas totalement compte du *bagging*: les différents estimateurs/variables aléatoires $X_i$ ne peuvent cependant pas être considérées comme indépendantes du fait de la façon dont sont construits les échantillons (avec remise). On affine alors l'analogie en s'intéressant à $n$ variables aléatoires $X_i$ identiquement distribuées d'espérance $\\mu$ et de variance $\\sigma^2$, mais non indépendantes avec un coefficient de correlation deux-à-deux $\\rho$, la moyenne empirique $\\bar{X}$ reste un estimateur sans biais de $\\mu$ mais de variance: \n",
    "\n",
    "$$Var(\\bar{X}) = \\rho\\sigma^2 + \\frac{(1-\\rho)}{n}\\sigma^2$$\n",
    "\n",
    "Remarque: Cette formule se retrouve facilement à partir des propriétés de la variance: \n",
    "* $Var(aX + bY) = a^2Var(X) + b^2Var(Y) + 2abCov(X,Y)$ \n",
    "* $Var(\\sum_{i=1}^{n}X_i) = \\sum_{i=1}^{n}Var(X_i) + 2\\sum_{1 \\leq i \\le j \\leq n}Cov(X_i, X_j) $\n",
    "\n",
    "Le *bagging* à lui seul ne suffit donc pas à faire baisser la variance. On l'associe donc à deux mesures supplémentaires:\n",
    "* On augmente le nombre $n$ d'estimateurs entraînés sur échantillon *bootstrap* afin de rendre le second terme de l'équation ci-dessus négligeable.\n",
    "* On s'attache à décorréler les différents estimateurs/modèles à agréger, à faire en sorte qu'ils \"ne se ressemblent pas\".\n",
    "\n",
    "Les arbres de décision s'y prêtent d'autant mieux qu'ils sont relativement peu stable au sens où comme ils sont construits hiérarchiquement: de petites différences de *training set* peuvent aboutir à des structures relativement différentes. On va renforcer cet effet \"décorrelant\" en modifiant l'algorithme de construction de l'arbre: à chaque *split*, l'algorithme ne peut utiliser que $m \\le p$ *features* (souvent $m = \\sqrt{p}$ par défaut), celles-ci ayant été sélectionnées aléatoirement, pour déterminer son critère de *split*. \n",
    "\n",
    "Le *bagging* n'améliore pas le biais. Cette caractéristique a deux conséquences:\n",
    "* Le *bagging* s'utilise pour des modèles à faible biais mais à variance élevée, (typiquement des arbres de décision) la méthode visant à produire un estimateur agrégé de même biais que ses estimateurs individuels mais de variance inférieure.\n",
    "* L'utilisateur doit s'assurer que les modèles individuels sont d'autant moins biaisés que l'ensemble héritera de ce biais.\n",
    "\n",
    "### Remarques \n",
    "#### Sur l'analogie modèles/variables aléatoires\n",
    "Dans l'analogie modèles/variables aléatoires, les termes indépendants et indentiquement distribués peuvent se conprendre:\n",
    "* Identiquement distribués: Dans le cas des variables aléatoires cela sous-entend qu'elles sont toutes de même loi. Dans le cas de modèles, celà veut dire qu'ils partagent une même spécification, par exemple un arbre de décision, un modèle linéaire à $p$ variable, etc. (en général de biais faible pour le bagging). On peut voir chaque modèle comme une distribution postulée dont on va estimer les paramètres. On retrouve le fait que dans un certain nombre d'*ensemble methods*, les modèles sont tous de même spécification. \n",
    "* Indépendants: les différents modèles estimés ont une \"structure\" très différente. Va plutôt de pair avec des modèles à faible biais. Par exemple, les modèles linéaires sont relativement biaisés mais de faible variance: des différences de dataset ne vont pas donner des modèles significativement différents, ie. des \"droites\" de pentes assez peu différentes. A l'inverse avec un faible biais, des différences dans les données peuvent aboutir à une collection de modèles beaucoup plus \"variée\".\n",
    "\n",
    "#### Sources de variabilité\n",
    "Dans un *random forest*, la variabilité des prédictions des modèles individuels a au moins trois origines: \n",
    "* Variabilité liée à l'échantillon: on ne possède par définition qu'un nombre limité de réalisation de la ditribution à estimer.\n",
    "* Variabilité induite par le rééchantillonnage: tous les modèles ne voient pas exactement le même *training set*, d'où des différences dans les fonctions finalement apprises.\n",
    "* Variabilité induite par la sélection aléatoire des *features*. Elle est d'autant plus élevée que $m$ est faible.\n",
    "\n",
    "L'étape d'agrégation du bagging permet d'atténuer au moins la seconde (?). On voit dans tous les cas que la méthode introduit de nouvelles sources de variabilité mais qui semble largement compensée par l'agrégation.\n",
    "\n",
    "#### Sur la sélection aléatoire de *features*\n",
    "La sélection aléatoire de *features* peut se révéler être un désavantage de ce type de modèle dans les situations où seul un faible nombre de *features* est informatif, le reste étant du bruit. Si $m$ est particulièrement faible, les *features* informatives ne seront que peu fréquemment choisies et les performances du modèle d'ensemble sera assez mauvaise. C'est un problème qui n'existe cependant pas dans le cas du *boosting*. \n",
    "\n",
    "$m$ est au fond un hyperparamètre du modèle. Il se trouve que son choix peut se comprendre dans le cadre d'un compromis biais-variance:\n",
    "* Plus $m$ est faible, plus le biais de l'ensemble du modèle sera élevé.\n",
    "* Plus $m$ est élevé, plus on se prive de son effet régularisant et plus la variance de l'ensemble du modèle sera élevée.\n",
    "\n",
    "Les *extra (or extremely) randomized trees* introduisent un degré de *randomisation* supplémentaire en choisissant pour chacune des $m$ *features* sélectionnées aléatoirement un *split* aléatoire. Le *split* finalement conservé est celui des $m$ *splits* aléatoire produisant la meilleure diminution du critère de split. \n",
    "\n",
    "## *Out-of-bag error*\n",
    "Chacun des échantillons *bootstrap* étant constitué par tirage avec remise, tous les points du *training set* ne sont pas inclus dans chacun de ces échantillons. Pour un échantillon *bootstrap* donné, les point du *training set* finalement non inclus dans l'échantillon sont appelés *out-of-bag samples*. Un point donné du *training set* se retrouve ainsi *out-of-bag sample* pour un certain nombre d'échantillons *bootstrap* et n'ayant pas été vu par les modèles entrainés sur ceux-ci, il peut être utilisé pour l'évaluation de la *test error*. L'erreur ainsi estimée est appelée *out-of-bag error*. Le modèle agrégé utilisé pour calculer l'*out-of-bag error* en un point $x$ donné est constitué de tous les modèles individuels dont l'échantillon *bootstrap* n'inclut pas $x$. L'*out-of-bag error* totale correspond à la moyenne des *out-of-bag errors* sur le *training set*.\n",
    "\n",
    "On peut montrer que l'*out-of-bag error* et la *test error* obtenue par *cross validation* sont proches, la première convergeant vers la seconde quand le nombre d'échantillons *bootstrap* $B$ tend vers l'infini (cf. *The Elements of Statistical Learning*, exercice 15.2).\n",
    "\n",
    "L'utilisation de l'*out-of-bag error* présente deux principaux avantages:\n",
    "* Elle permet d'approximer convenablement la *test error* sans avoir à recourrir à une *cross validation* potentiellement couteuse et/ou contraignante (si peu de données par exemple).\n",
    "* L'erreur est calculée en même temps que le modèle est entrainé.\n",
    "\n",
    "**Attention**: L'*out-of-bag error* n'est plus indiquée si des prétraitements ont pu créer un *data leakage*.\n",
    "\n",
    "Remarque: Lorsqu'on crée un échantillon *bootstrap* à $n$ éléments à partir d'un échantillon comportant le même nombre de $n$ éléments distincts, chaque élément a une probabilité d'environ 0.37 de ne pas être sélectionné lors du rééchantillonnage. La propabilité pour un élément donné de ne pas être inclus est en effet de (pour $n$ suffisamment grand): \n",
    "\n",
    "$$(\\frac{n-1}{n})^n = (1-\\frac{1}{n})^n = e^{n.ln(1-\\frac{1}{n})} \\simeq e^{-1} \\simeq 0.37$$\n",
    "\n",
    "## *Feature importance*\n",
    "La *feature importance* (potentiellement utilisée en sélection de *features*) est une mesure de la contribution de chaque variable à la performance de l'ensemble. Elle se construit de la même façon pour les *random forests* et les arbres boostés. On trouve en général deux définition de l'*importance* pour une *feature* donnée:\n",
    "* La moyenne des réductions du critère de *split* dans l'ensemble des modèles où intervient la *feature*. Plus une *feature* améliore le critère, plus elle est *a priori* importante dans l'approximation de la fonction à approcher, plus elle apporte d'information.\n",
    "* Pour chaque arbre, on réalise une estimation de la performance (ex: *accuracy*) sur les *out-of-bag samples* puis pour chaque *feature* de l'arbre, on permute les valeurs de cette *feature* chez les *out-of-bag samples* et mesure la chute de performance associée. La *feature importance* correspond à la moyenne de ces chutes de performance dans tous les arbres où la *feature* est présente. *A priori* plus une *feature* est importante, plus forte sera la chute de performance associée à la permutation de ses valeurs. \n",
    "\n",
    "Attention: En présence de variables corrélées, la *feature importance* de ces variables sous estime l'importance de l'information apportée par le groupe de variables. \n",
    "\n",
    "## Combien d'arbres sont-ils nécéssaires ? Est-ce qu'ajouter des arbres est associé au risque de faire *overfitter* le modèle ?\n",
    "Un *random forest* peut s'écrire sous la forme : $\\hat{f}(x) = \\frac{1}{B}\\sum_{b} \\hat{f}_b(x)$ qu'on prend comme approximation de la limite $lim_{b \\to +\\infty}\\hat{f}_b(x)$ (qui converge par la loi des grands nombres). Ajouter des arbres (augmenter $B$) ne contribue pas à faire *overfitter* le modèle (augmenter la variance de l'ensemble). C'est même plutôt le contraire dans un premier temps (cf. formule plus haut) quoique le gain devienne assez rapidement négligeable. Cependant le modèle limite ci-dessus vers lequel la moyenne des modèles converge peut avoir *overfitté* les données. Cela dépend donc du choix fait pour les autres hyperparamètres (comme ceux influant la profondeur des arbres) mais pas du nombre d'arbres $B$. Dit autrement, à partir d'un certain seuil, la variance du modèle est approximativement $\\rho\\sigma^2$ qui ne dépend plus de $B$ et qui peut s'établir à un seuil relativement élevé (*overfit*) suivant les valeurs choisies pour les hyperparamètres influant sur $\\rho$ (ex: $m$) et/ou $\\sigma$ (ex: profondeur de l'arbre).\n",
    "\n",
    "Certains conseillent de ne pas considérer $B$ comme un hyperparamètre à *tuner*, pour s'économiser une dimension dans le *grid search*. Le choisir alors dès le départ suffisamment élevé pour profiter des effets du *bagging* mais pas trop afin de s'économiser du temps de calcul qui finalement n'apporte que peu de gain. Attention cependant, certains indicateurs de performance ne sont pas des fonctions strictement décroissantes de $B$ (voir notamment ce [post](https://stats.stackexchange.com/a/348246)) et il peut arriver qu'ils varient avant de se stabiliser au niveau de leur limite (si on explore une gamme de $B$ ne pas la prendre trop petite, ou ne pas s'arrêter dès qu'on semble constater une stabilisation de l'indicateur de performance).\n",
    "\n",
    "## *Datasets* déséquilibrés\n",
    "Le problème des datasets déséquilibrés se posent à plusieurs niveaux pour les *random forests*:\n",
    "* En cas de fort déséquilibre, de fortes différences de répartition entre classes peuvent apparaître entre les différents échantillons *bootstrap*, problème pouvant être résolu en constituant les échantillons *bootstrap* par stratification (quoiqu'il semble que le *resampling* donne de meilleurs résultats, cf. [ce papier, paragraphe 2.2](https://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf)).\n",
    "* Les règles de construction des arbres font *a priori* l'hypothèse de classes équilibrées.\n",
    "\n",
    "Comme toujours avec les *datasets* déséquilibrés, on a le choix entre deux approches (qu'on peut combiner):\n",
    "* Rééquilibrer le *dataset* par sous- ou sur-échantillonnage avec les précautions d'usage.\n",
    "* Modifier l'algorithme pour lui permettre de prendre en compte le déséquilibre des classes ce qui passent le plus souvent par l'introduction de poids en des points clés de l'algorithme (critère de *split*).\n",
    "\n",
    "## *Bagging* vs. *boosting*\n",
    "*Bagging* et *boosting* sont deux méthodes différentes permettant de construire un estimateur de faibles biais et variance. Le *bagging* vise à réduire la variance en moyennant et se base donc plutôt sur des modèles à faible biais mais variance élevée. Le *boosting* construit un ensemble de modèle en réduisant progressivement le biais, il se base donc plutôt sur des modèles à fort biais mais variance faible.\n",
    "\n",
    "Dans le cas où le modèle de base est un arbre de décision, un ensemble d'arbres boostés se distingue des *random forests* sur les points suivants:\n",
    "* Les arbres sont construits sans obligatoirement recourir à une sélection aléatoire des *features* (on en a pas autant besoin que pour le *bagging*), ce qui autorise de meilleures performance en présence de nombreuses *features* non informatives.\n",
    "* Un trop grand nombre d'arbres peut être source d'overfitting (?)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
