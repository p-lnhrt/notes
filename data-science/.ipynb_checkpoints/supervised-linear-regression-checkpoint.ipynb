{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that two variables which are orthogonal are (linearly) independent.\n",
    "http://www.stat.ucla.edu/~arashamini/teaching/stat100c/notes/100C_slides.pdf\n",
    "https://en.wikipedia.org/wiki/Covariance#Relationship_to_inner_products\n",
    "En cas d'indépendance il n'y a par définition aucune liaison fonctionnelle entre les deux variables aléatoires. Il n'y a en particulier pas de relation linéraire d'où une corrélation/covariance nulle. La réciproque est fausse : on peut avoir une relation fonctionnelle entre deux variables avec une corrélation/covariance nulle, c'est-à-dire sans indépendance.\n",
    "\n",
    "https://math.unice.fr/~diener/probas/Covariance.pdf\n",
    "\n",
    "Indépendance et orthogonalité \n",
    "Ne pas oublier que quand il en est question dans le contexte de la régression linéaire, les vecteurs aléatoires considérés sont supposés gaussiens, absence de corrélation est alors équivalent à indépendance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les principales hypothèses du modèle linéaire sont:\n",
    "* Spécification - Hypothèse linéaire: La réponse $Y$ s'écrit comme une combinaison linéaire des prédicteurs $X_i$ auxquels on ajoute un terme d'erreur $\\epsilon$ : $Y = X\\beta + \\epsilon$. La réponse peut ainsi se voir comme la somme des contributions, des effets de chacune des variables pondérée par les coefficients du modèle.\n",
    "* Le terme d'erreur $\\epsilon$ est d'espérance nulle.\n",
    "* Exogénéité: Aucune des variables n'est corrélée avec l'erreur. Cette hypothèse implique que seule l'erreur est stochastique et que les variables sont \"mesurées sans erreur\". Le relâchement de cette hypothèse est par exemple traitée par les *errors-in-variables models*.\n",
    "* Indépendance des erreur: Cette hypothèse a entre autres pour conséquence que la matrice de covariance de $\\epsilon$ est diagonale. Elle est notamment violée dans le cas de la régression de séries temporelles.\n",
    "* Homoscédasticité: $V(\\epsilon \\; | \\; X)$ indépendant de $X$, cette hypothèse a notamment comme conséquence que la matrice de covariance de $\\epsilon$ est un multiple de l'identité.\n",
    "* Absence de multicolinéarité: Cette hypothèse assure l'unicité de l'estimation (implique $X^{\\top}X$ inversible) mais aussi la qualité de l'estimation.  \n",
    "* Distribution normale des erreurs:  $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2I_n)$\n",
    "\n",
    "Dans le cas du modèle linéraire $Y = X\\beta + \\epsilon$, le meilleur estimateur $\\hat{Y} = X\\hat{\\beta}$ au sens des moindres carrés correspond à la solution du problème d'optimisation: \n",
    "\n",
    "$$min_{\\beta \\in \\mathbb{R}^p}\\|Y-X\\beta\\|^2$$\n",
    "\n",
    "Avec $X \\in \\mathcal{M}_{n,p}(\\mathbb{R})$. La condition du premier ordre du problème ci-dessus s'écrit simplement:\n",
    "\n",
    "$$-2X^{\\top}(Y-X\\beta) = 0$$\n",
    "\n",
    "On en déduit:\n",
    "* $(Y-X\\beta)$ orthogonal au sous-espace vectoriel de $\\mathbb{R}^n$ engendré par les colonnes de $X$ (noté $Vect(X)$). $\\hat{Y} = X\\hat{\\beta}$ correspond donc au projeté orthogonal de $Y$ sur $Vect(X)$.\n",
    "* Le développement de l'expression ci-dessus permet de trouver $\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y$ en supposant $X^{\\top}X$ inversible, inversibilité garantissant l'unicité du minimum.\n",
    "\n",
    "Remarque: On peut montrer que la dérivée seconde par rapport à $\\beta$ est égale à $2X^{\\top}X$ qui est toujours strictement définie positive (l'optimum est donc un minimum) si inversible.  \n",
    "\n",
    "Des hypothèses de normalité, d'homoscédasticité et d'indépendance des erreurs $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2I_n)$ et des propriétés de l'espérance et de la variance des vecteurs aléatoires, on déduit:\n",
    "* $Y \\sim \\mathcal{N}(X\\beta, \\sigma^2I_n)$ par $Y = X\\beta + \\epsilon$,\n",
    "* $\\hat{\\beta} \\sim \\mathcal{N}(\\beta, \\sigma^{2}(X^{\\top}X)^{-1})$ par $\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y$ (cf. formulaire sur les vecteurs aléatoires ci-dessous).\n",
    "* En particulier : $\\beta_{j} = (0, \\dots, 1, \\dots, 0)^{\\top}\\beta$ d'où $\\hat{\\beta}_{j} \\sim \\mathcal{N}(\\beta_{j}, \\sigma^{2}(X^{\\top}X)^{-1}_{jj})$\n",
    "\n",
    "On remarque ainsi que $\\hat{\\beta}$ est un estimateur sans biais (il est également l'estimateur de variance minimale) pour $\\beta$ et que la variance et donc la qualité de l'estimation est liée à l'inversibilité de $X^{\\top}X$.\n",
    "\n",
    "Remarque: On peut montrer que sous les hypothèses du modèle linéaire, $\\hat{\\beta}$ est également l'estimateur du maximum de vraisemblance.\n",
    "\n",
    "Vecteurs aléatoires - Pense-bête\n",
    "* Soient $\\mathbf{x}$ vecteur aléatoire à valeurs dans $\\mathbb{R}^n$ et $A$ matrice constante de $\\mathcal{M}_{m, n}(\\mathbb{R})$, on a $\\mathbb{E}(A\\mathbf{x}) = A\\mathbb{E}(\\mathbf{x})$\n",
    "* Soient $\\mathbf{x}$ vecteur aléatoire à valeurs dans $\\mathbb{R}^n$ de matrice d'(auto)-covariance $\\Sigma_{xx}$ et $A$ matrice constante de $\\mathcal{M}_{m, n}(\\mathbb{R})$, alors la matrice d'(auto)-covariance de $A\\mathbf{x}$ s'écrit $\\Sigma_{AxAx} = A\\Sigma_{xx}A^{\\top}$.\n",
    "\n",
    "https://statisticsbyjim.com/regression/ols-linear-regression-assumptions/\n",
    "https://en.wikipedia.org/wiki/Linear_regression#Assumptions\n",
    "https://en.wikipedia.org/wiki/Errors-in-variables_models\n",
    "http://www.stat.umn.edu/geyer/5102/slides/s5.pdf\n",
    "https://people.eecs.ku.edu/~jhuan/EECS940_S12/slides/linearRegression.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On montre que:\n",
    "* Pour l'estimateur $S^2$ de $\\sigma^2$: $\\frac{n-p}{\\sigma^{2}}S^{2} \\sim \\chi_{n-p}^{2}$\n",
    "* $S^2$ et $\\hat{\\beta}$ sont indépendants\n",
    "\n",
    "On rappelle que pour $Z \\sim \\mathcal{N}(0,1)$ et $S^2 \\sim \\chi_{n}^{2}$ avec $Z$ et $S^2$ indépendants, on a: \n",
    "\n",
    "$$\\frac{Z}{S/\\sqrt{n}} \\sim T_{n}$$\n",
    "\n",
    "Annexe\n",
    "\n",
    "Pour $X$ vecteur gaussien aléatoire à valeurs dans $\\mathbb{R}^n$ : $X \\sim \\mathcal{N}(\\mu, \\Sigma)$, on a pour tout vecteur (constant) $a \\in \\mathbb{R}^n$:\n",
    "$$a^{\\top}X \\sim \\mathcal{N}(a^{\\top}\\mu, a^{\\top}\\Sigma a)$$\n",
    "\n",
    "http://www.eco.uc3m.es/~ricmora/miccua/materials/DavidsonMacKinnon_2004_Ch4.pdf\n",
    "Section 4.4, Tests of a Single Restriction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On appelle ligne de régression la fonction $x \\mapsto \\mathbb{E}(Y|X=x) = m_{Y|X}$\n",
    "\n",
    "On montre que la meilleure prévision (le meilleur estimateur ?) de $Y$ en moyenne quadratique (celle qui minimise la MSE) est son espérance conditionnelle par rapport à $X$, c'est à dire sa ligne de régression. La régression linéaire est un modèle qui \n",
    "1. Consiste à utiliser l'espérance conditionnelle comme prévision $\\hat{Y}$ de $Y$\n",
    "2. Prend une spécification linéaire pour la ligne de régression $m_{Y|X}=X\\beta$\n",
    "\n",
    "Le modèle complet s'écrit $Y=X\\beta+\\epsilon$ avec $\\mathbb{E}(\\epsilon)=0$ ce qui permet de s'assurer que $\\mathbb{E}(Y)=X\\beta=\\mathbb{E}(\\hat{Y})$, c'est à dire que $\\hat{Y}$ est un estimateur sans biais de $Y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "https://stats.stackexchange.com/questions/344006/understanding-t-test-for-linear-regression/344008"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
