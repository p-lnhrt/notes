{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* Regularised GMM\n",
    "* Illustrations: http://imagine.enpc.fr/~obozinsg/malap/cours_malap_10.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Gaussian mixture models* (GMM)\n",
    "Les GMM sont une famille de modèles faisant l'hypothèse que la distribution à modéliser peut s'écrire comme une combinaison linéaire de $k$ gaussiennes:\n",
    "\n",
    "$$p(x)=\\sum_{i=1}^{k}\\pi_{i}\\mathcal{N}(x |\\mu_{i}, \\Sigma_{i})$$\n",
    "\n",
    "Les paramètres $\\pi_{i}$ reflètent la contribution de chaque gaussienne à l'ensemble et possèdent les deux propriétés suivantes:\n",
    "* $\\sum_{i}\\pi_{i}=1$ Il suffit pour le montrer de sommer l'égalité précédente sur $x$.\n",
    "* $0 \\le \\pi_{i} \\le 1$\n",
    "On remarque que les paramètres $\\pi_{i}$ satisfont ainsi les condition minimales requises pour constituer une distribution de probabilités.\n",
    "\n",
    "L'estimation d'un GMM consiste à estimer les paramètres $\\pi_{i}$, $\\mu_{i}$ et $\\Sigma_{i}$ qu'on désigne collectivement par le paramètre $\\theta$. Pour une *likelihood* $p(x|\\theta)=\\prod_{i=1}^{n}p(x_{i}|\\theta)$, la *log-likelihood* plus simple à manipuler, d'un GMM va ainsi s'écrire:\n",
    "\n",
    "$$logp(x|\\theta)=\\sum_{i=1}^{n}log(\\sum_{j=1}^{k}\\pi_{i}f_{\\mu_{i}, \\Sigma_{i}}(x_{j}))$$\n",
    "\n",
    "On voit à la présence d'une somme dans le log que la maximisation directe de la *likelihood* risque de ne pas être simple. L'estimation des GMM au maximum de vraisemblance recourt ainsi à l'*EM-algorithm*. On introduit pour celà des variables latentes inobservées $z$ de telle façon que le GMM $p(x)$ corresponde à la distribution marginale de la distribution jointe $p(x,z)=p(z)p(x|z)$.\n",
    "\n",
    "## Variables latentes d'un GMM\n",
    "Remarque: Par souci de clarté, $x$ désigne l'ensemble des observations dans cette partie.\n",
    "\n",
    "L'utilisation de l'*EM-algorithm* a requis l'introduction d'une variable latente $z$. Cette variable prend ses valeurs dans l'ensemble des vecteurs à $k$ composantes dont une seule est égale à $1$ tandis que l'ensemble des autres sont égales à $0$. La variable $z$ ne peut ainsi prendre que $k$ valeurs, la probabilité qu'elle puisse prendre la valeur du vecteur dont la $i^e$ composante est à $1$ étant $\\pi_i$. Dit autrement, la loi suivie par $z$ est une loi catégorielle (loi de Bernoulli généralisée à plus de deux catégories), loi de probabilité discrète dont la fonction de masse s'écrit $p(z=z_{i})=\\pi_i$ avec $z_{i}$ le vecteur de $\\mathbb{R}^k$ dont la $i^e$ composante est égale à $1$ tandis que les autres sont à $0$.\n",
    "\n",
    "L'introduction de cette variable latente donne une autre interprétation au GMM: tout se passe comme si chacune des observations n'a été généré que par une seule des gaussiennes composant le modèle. A chaque nouvelle observation, tout se passe comme si:\n",
    "* On effectuait dans un premier temps un tirage de la variable $z$ qui nous donne le vecteur $z_i$ avec une problabilité $\\pi_i$\n",
    "* On effectuait dans un second temps un tirage de la variable $x$ dont on suppose à cette étape qu'elle se distribue comme la $i^e$ gaussienne de la mixture.\n",
    "\n",
    "Il découle immédiatement de la seconde étape du raisonnement:\n",
    "\n",
    "$$p(x|z=z_{i}) = \\mathcal{N}(x |\\mu_{i}, \\Sigma_{i})$$\n",
    "\n",
    "On retrouve ainsi la spécification du modèle: \n",
    "\n",
    "$$p(x)=\\sum_{i=1}^{k}p(z=z_{i})p(x|z=z_{i})=\\sum_{i=1}^{k}\\pi_{i}\\mathcal{N}(x |\\mu_{i}, \\Sigma_{i})$$\n",
    "\n",
    "On insiste ici sur le \"tout se passe comme si\". C'est dans cette vue de l'esprit que la variable $z$ prend tout son sens: la modélisation du problème fait appel à cette variable mais qui est cependant inobservable en pratique.\n",
    "\n",
    "Remarques: \n",
    "* Plus $\\pi_i$ sera élevée, plus la $i^e$ gaussienne de la mixture sera choisie souvent et graphiquement, plus le cluster associé à cette composante sera dense.\n",
    "* On introduit la dernière quantité non encore abordée $p(z|x)$ appelée aussi *responsibility* qu'on déduit aisément des autres quantités déjà introduites à l'aide de la formule de Bayes: \n",
    "\n",
    "$$p(z=z_{i}|x) = \\frac{\\pi_{i}\\mathcal{N}(x |\\mu_{i}, \\Sigma_{i})}{\\sum_{j=1}^{k}\\pi_{j}\\mathcal{N}(x |\\mu_{j}, \\Sigma_{j})} = \\gamma_{i}$$\n",
    "\n",
    "$p(z=z_i)=\\pi_i$ peut ainsi être vue comme la *prior probability* de $z=z_i$ et la *responsibility* comme la *posterior probability* (probabilité réévaluée après observation des données) associée. De manière générale à chaque étape de l'*EM-algorithm* qui consiste en deux maximisations successives, $p(z|x)$ est à la fois la distribution issue de la première et celle utilisée dans la seconde. On comprend dès lors l'importance du rôle que vont jouer les *responsibilities* dans l'*EM-algorithm* appliqué aux GMM.\n",
    "\n",
    "## Estimation d'un GMM avec l'*EM-algorithm*\n",
    "En s'en tenant à une description générale de l'*EM-algorithm*, développons les deux étapes de la $i^e$ itération qui pour plus de clarté sera notée en exposant:\n",
    "\n",
    "### *E-step* \n",
    "Calcul de $J_{q^{(i)}}(\\theta)=\\mathbb{E}_{z \\sim q^{(i)}}[log\\frac{p(x,z|\\theta)}{q^{(i)}(z)}]$ avec $q^{(i)}(z) = p(z|x,\\theta^{(i-1)})$\n",
    "\n",
    "D'après le paragraphe précédent $p(z|x,\\theta^{(i-1)})$ est connue, sa fonction de masse correspond aux *responsibilities*. Ainsi:\n",
    "\n",
    "$$p(z=z_{j}|x,\\theta^{(i-1)}) = \\gamma_{j}(x) = \\frac{\\pi_{j}^{(i-1)}\\mathcal{N}(x |\\mu_{j}^{(i-1)}, \\Sigma_{j}^{(i-1)})}{\\sum_{u=1}^{k}\\pi_{u}^{(i-1)}\\mathcal{N}(x |\\mu_{u}^{(i-1)}, \\Sigma_{u}^{(i-1)})}$$\n",
    "\n",
    "Avec $\\mathcal{N}(x |\\mu_{j}^{(i-1)}, \\Sigma_{j}^{(i-1)}) = \\prod_{u=1}^{n}\\mathcal{N}(x_{u} |\\mu_{j}^{(i-1)}, \\Sigma_{j}^{(i-1)})$.\n",
    "\n",
    "$J_{q^{(i)}}(\\theta)$ prend donc la forme relativement simple: \n",
    "\n",
    "$$\\begin{align}\n",
    "J_{q^{(i)}}(\\theta) &= \\sum_{j=1}^{k}\\gamma_{j}log(\\frac{p(x,z_{j}|\\theta^{(i-1)})}{\\gamma_{j}})\\\\ \n",
    "&= \\sum_{j=1}^{k}\\gamma_{j}log(\\frac{p(z_{j}|\\theta^{(i-1)})p(x|z_{j},\\theta^{(i-1)})}{\\gamma_{j}})\\\\\n",
    "&= \\sum_{j=1}^{k}\\gamma_{j}log(\\frac{\\pi_{j}\\mathcal{N}(x |\\mu^{(i-1)}, \\Sigma^{(i-1)})}{\\gamma_{j}})\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "### *M-step*\n",
    "On maximise dans cette étape la fonction $\\theta \\mapsto J_{q^{(i)}}(\\theta)$ obtenue à l'étape précédente en écrivant simplement les conditions du premier ordre. La résolution des équations obtenues donnent ainsi les formules d'*update* de $\\pi^{(i)}$, $\\mu^{(i)}$ et $\\Sigma^{(i)}$ qui seront utilisées dans la $(i+1)^e$ étape de l'algorithme. On obtient ainsi après calcul: \n",
    "* $\\mu_{j}^{(i)} = \\frac{1}{n_{j}}\\sum_{u=1}^{n}\\gamma_j(x_{u})x_{u}$ avec $n_{j}= \\sum_{u=1}^{n}\\gamma_{j}(x_{u})$ \n",
    "* $\\Sigma_{j}^{(i)} = \\frac{1}{n_{j}}\\sum_{u=1}^{n}\\gamma_j(x_{u})(x_{u}-\\mu_{j}^{(i)})(x_{u}-\\mu_{j}^{(i)})^{\\top}$ avec $n_{j}= \\sum_{u=1}^{n}\\gamma_{j}(x_{u})$\n",
    "* $\\pi_{j}^{(i)} = \\frac{1}{n}\\sum_{u=1}^{n}{\\gamma_{j}(x_{u})}$\n",
    "\n",
    "On peut reformuler le déroulé de ces deux étapes pour retrouver l'algorithme usuellement présenté pour les GMM:\n",
    "* Pour chacune des $n$ observations, on calcule son jeu de $k$ *responsibilities* qui correspond aux probabilités qu'elles aient pu être générée par chacune des $k$ gaussiennes obtenues à l'étape précédente. \n",
    "* On rééstime les moyennes et matrices de covariance de chacune des $k$ gaussiennes, il s'agit des estimateurs classiques mais où chaque observation est pondérée par la *responsibility* de la gaussienne considérée: plus il est considéré probable que la gaussienne considérée ait généré l'observation, plus on donne de poids à celle-ci dans l'estimation des paramètres.\n",
    "* Pour chaque gaussienne $j$, on update son poids $\\pi_j$ avec la valeur moyenne de ses *responsibilities* $\\gamma_{j}$.\n",
    "\n",
    "## Les GMM comme algorithme de clustering\n",
    "On a considéré jusqu'ici les GMM comme un simple modèle de densité de probabilité un peu difficile à estimer. On peut toutefois exploiter le fait que le modèle consiste en la réunion de $k$ densités de probabilités pour obtenir un partitionnement des observations, les paramètres $\\pi_i$ donnant une idée des populations relatives des clusters:\n",
    "* Chaque observation est assignée au cluster/à la gaussienne qui présente la plus haute valeur de *responsibility*. L'étude des *responsibilities* nous donne une idée de la \"certitude\" avec laquelle l'assignation a été faite. C'est pour cela qu'on parle pour les GMM de *soft assignment* par opposition au *hard assignement* qu'on rencontre dans d'autres algorithmes de partitionnement comme le $k$*-means*.\n",
    "* L'assignation d'une nouvelle observation peut se faire par le calcul des distance de Mahalanobis de l'observation à chacune des gaussiennes ou un calcul de *responsibilities* (équivalent ?).\n",
    "\n",
    "### Lien avec l'algorithme du $k$*-means*\n",
    "On peut montrer que l'algorithme du $k$*-means* correspond exactement à un cas limite des GMM où les matrices de covariance des différentes gaussiennes sont notamment contraintes à être des multiples de l'identités (gaussiennes de forme sphérique). Cette restriction nous permet d'ailleurs de comprendre les limitations $k$*-means* quant à la topologie des clusters que l'algorithme est capable de correctement extraire. Ces éléments nous permettent de comprendre pourquoi les GMM sont parfois présentés comme une généralisation plus flexible du $k$*-means*.\n",
    "\n",
    "## Initialisation\n",
    "De la même manière que le $k$*-means* qui du fait de sa *greediness* peut tomber dans des maximas locaux et donc de ce fait sensible à l'initialisation, les GMM sont aussi sensibles à l'initialisation de l'*EM-algorithm* (qui peut lui aussi tomber dans des maximas locaux de la vraisemblance).\n",
    "\n",
    "On distingue globalement deux méthodes d'initialisation pour la calibration des GMM:\n",
    "* Une méthode aléatoire: on sélectionne $k$ points au hasard qui vont permettre d'initialiser les moyennes $\\mu$, les matrices de covariance $\\Sigma$ sont initialisées avec la matrice de covariance des données et les poids $\\pi$ initialisés à $\\frac{1}{n}$ (ou au hasard ?).\n",
    "* Utilisation d'un $k$*-means*: on initialise des moyennes $\\mu$ avec les poids proches de centroïdes, les poids $\\pi$ et les matrices de covariance $\\Sigma$ sont initialisés sur les clusters obtenus par le $k$*-means*.\n",
    "\n",
    "## Risques de divergence\n",
    "L'utilisation du maximum de vraisemblance pour la calibration de GMM avec $k>1$ s'expose à la présence de singularités. Supposons pour simplifier la matrice de covariance diagonale et de paramètre $\\sigma$ et qu'une gaussienne se retrouve centrée sur un point isolé, sur du bruit. Faire tendre $\\sigma$ vers zéro nous permettra alors de toujours faire croire la vraisemblance. Ces singularités donnent également un exemple du sévère *overfitting* auxquelles les méthodes du maximum de vraisemblance peuvent conduire. Ces singularités n'existent pas dans les approches bayésiennes du même problème. En attendant, il est nécessaire de se doter de garde-fous dans l'implémentation de l'*EM-algorithm*. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
