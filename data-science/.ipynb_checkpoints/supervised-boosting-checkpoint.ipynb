{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "2 idées principales :\n",
    "* L'estimateur est postulé comme une somme pondérée d'estimateurs faibles (*weak learners*) au sens de fortement biaisés, l'estimateur global ayant une performance très supérieure aux estimateurs individuels le composant et dont la performance se retrouve ainsi \"boostée\" par le fait d'être combinés dans un ensemble. Tous les *weak learners* appartiennent à la même famille de modèles (CART, regression linéaire, etc.).\n",
    "* Au boosting est aussi associé une méthode particulière de construction de l'estimateur dont on postule qu'il prend une forme additive : le *forward stagewise additive modeling* (FSAM). On ajoute séquentiellement au modèle une nouvelle fonction de base les coefficients de tous les autres modèles déjà ajoutés restant fixés lors du fit de cette dernière. Le modèle initial étant très fortement biaisé, l'idée est que chaque étape va venir en diminuer le biais.\n",
    "\n",
    "$$f(x)=\\sum_{m=1}^{M} v_{m} h_{m}(x)$$\n",
    "\n",
    "Remarque : on trouve parfois $v_{m}=1$ qui fait implictement l'hypothèse que l'espace des fonctions est stable par scaling : si $h_{m}$ appartient à l'espace, $v.h_{m}$ appartient aussi à l'espace. Trouver le meilleur modèle $h_{m}$ comprend alors le calcul de $v_{m}$.\n",
    "\n",
    "Remarque : Le *forward stagewise additive modeling* fournit une approximation de la solution du problème (souvent insoluble) suivant où on l'ensemble des fonctions est recherché \"en même temps\" :\n",
    "\n",
    "$$f^{*} = \\underset{\\left\\{\\beta_{m}, \\gamma_{m}\\right\\}_{1}^{M}}{\\operatorname{argmin}} \\sum_{i=1}^{N} L\\left(y_{i}, \\sum_{m=1}^{M} \\beta_{m} b\\left(x_{i} ; \\gamma_{m}\\right)\\right)$$\n",
    "\n",
    "L'idée générale est de trouver la fonction de la forme postulée (additive) minimisant la fonction de coût qu'on s'est choisi. Cette fonction est approchée à l'aide du FSAM. Il se trouve que pour un certain nombre de fonctions de coût répandues (MSE, MAE, exponential loss), l'algorithme FSAM peut être ramené à des problèmes qu'on sait résoudre (ex: l'algorithme Adaboost correspond à la construction d'un modèle par FSAM pour l'exponential loss) au moins approximativement (cf. cas où on trouve d'abord le modèle, puis le poids). A chacune de ces fonctions de coût correspond une méthode de résolution ad hoc mais l'utilisation du FSAM avec une fonction de coût quelconque ne semble pas évidente. On voudrait par exemple pouvoir fitter le meilleur modèle additif pour des fonctions de coût aux propriétés différentes. La MSE et l'exponential loss sont par exemple assez sensibles aux outliers mais la résolution du FSAM pour des fonctions de coût robustes peut apparaître assez peu trivial. C'est ce problème que veut résoudre le gradient boosting. \n",
    "\n",
    "L'idée du gradient boosting est de pouvoir trouver une solution (additive) approchée au problème de minimisation quelle que soit la fonction de coût pourvu que celle-ci soit différentiable par rapport à la prédiction. \n",
    "\n",
    "On peut voir l'addition de chaque fonction comme un déplacement dans l'espace de fonction : à chaque étape on se rapproche de la fonction à estimer, celle réalisant le minimum de la fonction de coût. Ce déplacement est en fait contraint à se faire dans un sous-espace tous les fonctions composant le modèle étant pris dans la \"même famille\". Contrairement à d'habitude, l'espace sur lequel on minimise la fonction de coût n'est pas un espace de paramètres ($\\mathbb{R^n}$) mais un espace de fonctions.\n",
    "\n",
    "Ce qui nous sauve de l'analyse fonctionnelle (problèmes topologiques de l'espace sur lequel on minimise, explicitation de l'incrément dans l'espace de fonctions, etc.) c'est que chaque fonction et donc la fonction de coût (l'erreur empirique) n'est évaluée que sur les $n$ points du *training set*. L'astuce consiste à optimiser la fonction de coût non pas directement par rapport à $f$ mais par rapport au vecteur de prédiction de taille $n$. La fonction de coût n'est alors à optimiser que sur $\\mathbb{R^n}$. On optimise comme souvent par descente de gradient. Chaque pas nous donne une nouvelle position dans l'espace des prédiction (correpondant à $\\mathbb{R^n}$). On parle ici d'*unconstrained direction* car la nouvelle prédiction obtenue ne fait aucune hypothèse particulière sur la fonction la réalisant. On se ramène ensuite alors à une fonction de la famille de modèles choisie en cherchant la meilleure fonction de cette famille approchant cette prédiction ce qui correspond à un simple problème de régression. Cela revient à chercher le projeté d'une fonction réalisant cette prédiction sur le sous-espace correspondant à notre famille de modèles.\n",
    "\n",
    "En régression et pour des *loss functions* comme la MSE ou MAE, il se trouve que la valeur à prédire permettant le meilleur décrément de la *loss function* à une étape donnée correspond aux résidus du modèle (complet) de l'étape précédente.\n",
    "\n",
    "Régularisation : \n",
    " * La principale technique de régularisation consiste à réduire la contribution de chaque modèle incrémental en le multipliant par un coefficient nu inférieur à 1 (cas qu'on peut interpréter comme réalisant en entier le déplacement dans l'espace des prédictions) souvent appelé shrinkage ou learning rate et pris égal à 0.1. Le modèle converge alors moins vite mais donne souvent de meilleurs résultats.\n",
    "* Une autre technique consiste à chaque étape à n'estimer la direction de meilleure diminution de la fonction de coût que sur une fraction (appelée parfois bag fraction) des données (subsampling). En plus de potentiellement accélerer l'entrainement, on peut interpréter sa propriété régularisante par le fait que les modèles ne voient pas toutes des données ce qui par rapport à leur donner l'intégralité doit aboutir à un modèle qui overfit moins. Appelé aussi mini-batch ou stochastic gradient boosting, cette technique peut se voir comme un subsampling des lignes. \n",
    "* Le subsampling des features (comme dans les random forests) est également utilisé par certains algorithmes comme XGBoost comme technique de régularisation. Cette technique peut se voir comme un subsampling des colonnes.\n",
    "* A cela peuvent s'ajouter des paramètres propres à la famille de modèles utilisée (ex: profondeur, taille des feuilles, etc. pour les arbres de décision)\n",
    "\n",
    "\n",
    "Adaboost \n",
    "Adaboost correspond à du gradient boosting appliqué au cas de l'exponential loss\n",
    "Un autre exemple de gradient boosting sur une autre loss function \n",
    "\n",
    "XGBoost, Catboost, LightGBM\n",
    "\n",
    "https://arxiv.org/pdf/1603.02754.pdf\n",
    "https://davidrosenberg.github.io/mlcourse/Archive/2018/Lectures/11b.gradient-boosting.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
