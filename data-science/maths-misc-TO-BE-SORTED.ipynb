{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le terme de méthodes Monte-Carlo désigne une famille de méthodes algorithmiques visant à calculer une valeur numérique approchée en utilisant des procédés aléatoires. Ces méthodes, dont le nom fait allusion aux jeux de hasard pratiqués à Monte-Carlo, ont été inventées en 1947 par Nicholas Metropolis, et publié pour la première fois en 1949 dans un article coécrit avec Stanislaw Ulam. Les méthodes de Monte-Carlo sont particulièrement utilisées pour calculer des intégrales en dimensions plus grandes que 1. Ces méthodes sont particulièrement utilisées en mathématiques et en physiques lorsque l'application de méthodes alternatives est trop difficile, coûteuse voire impossible. Les méthodes Monte-Carlo sont principalement utilisées pour trois grandes classes de problèmes : optimisation numérique (trouver l'extrêmum d'une fonction complexe prenant ses valeurs dans un espace de large dimension, ex: travelling salesman), intégration numérique (calcul de la valeur numérique prise par une intégrale) et génération d'échantillons suivant une distribution de probabilité donnée. Les méthodes sont aussi utilisées en finance par exemple pour introduire de l'incertitude (ex: dans un calcul de la VAN d'un projet, on va assigner des distributions aux paramètres clés du modèle pour obtenir une distribution de la varaible de décision produite par celui-ci).\n",
    "\n",
    "Ex: travelling salesman : (distances between each destination point) needed to determine the optimal path to follow are known with certainty and the goal is to run through the possible travel choices to come up with the one with the lowest total distance. However, let's assume that instead of wanting to minimize the total distance traveled to visit each desired destination, we wanted to minimize the total time needed to reach each destination. This goes beyond conventional optimization since travel time is inherently uncertain (traffic jams, time of day, etc.). As a result, to determine our optimal path we would want to use simulation - optimization to first understand the range of potential times it could take to go from one point to another (represented by a probability distribution in this case rather than a specific distance) and then optimize our travel decisions to identify the best path to follow taking that uncertainty into account.\n",
    "\n",
    "Exemple : on souhaite calculer l'espérance d'une fonction g de variable aléatoire X de densité de probabilité f_X. L'espérance va correspondre à l'intégrale de g(x).f_X(x).dx\n",
    "On peut choisir d'approximer cette intégrale, cette espérance par les valeurs prises par un estimateur de celle-ci sur un échantillon à valeur dans le support de l'intégrale et généré suivant la loi de probabilité suivie par X. Dans le cas de la quantité sous l'intégrale, une première approche est d'échantillonné x suivant une distribution uniforme sur le support de l'intégrale (procédé rapidement inefficace raffiné par de nombreuses méthodes). L'approximation de telles grandeurs par des estimateurs évalués sur un échantillon généré suivant une loi de probabilité particulière constitue le fond des méthodes Monte-Carlo. La théorie de l'estimation oriente dans le choix et la construction de ces estimateurs et fournit les justifications théoriques qui permettent d'approximer une \"grandeur\" par un estimateur aux bonnes propriétés. Un des enjeux des méthodes Monte-Carlo est aussi de pouvoir produire efficacement des échantillons distribués suivant la loi de probabilité suivie par notre variable aléatoire.\n",
    "\n",
    "a Monte Carlo method is a technique that can be used to solve a mathematical or statistical problem, and a Monte Carlo simulation uses repeated sampling to obtain the statistical properties of some phenomenon (or behavior). The method is useful for obtaining numerical solutions to problems too complicated to solve analytically. The most common application of the Monte Carlo method is Monte Carlo integration.\n",
    "\n",
    "L'idée essentielle des méthodes Monte-Carlo est d'utiliser une approche probabiliste pour la résolution de problèmes dont la formulation peut être totalement déterministe. Before the Monte Carlo method was developed, simulations tested a previously understood deterministic problem, and statistical sampling was used to estimate uncertainties in the simulations. Monte Carlo simulations invert this approach, solving deterministic problems using a probabilistic analog (see Simulated annealing). ...the Los Alamos physicists were unable to solve the problem using conventional, deterministic mathematical methods.\n",
    "\n",
    "Dans l'exemple de l'espérance, un des résultats classiques de la théorie de l'estimations est que :\n",
    "* La moyenne empirique (évaluée sur une suite d'échantillons de même loi de probabilité que X) est un estimateur sans biais de l'espérance.\n",
    "* La moyenne empirique est un estimateur consistant de l'espérance : question de la convergence (?)\n",
    "* Quelle que soit la loi suivie par X, le théorème centrale limite (TCL) donne une idée de la vitesse de convergence de l'estimateur (?) et permet de se donner un intervalle de confiance sur l'erreur commise par l'approximation de l'espérance par son estimateur : l'erreur est en O(1/sqrt(N)) avec N le nombre d'échantillons. Ainsi, multiplier la taille de son échantillon par 100 ne permet de réduire l'erreur d'approximation que d'un facteur 10 et ce quelle que soit la dimension de l'espace où les échantillons prennent leur valeur. La variance de l'estimateur fait aussi partie de la définition de l'intervalle de confiance (??) et sa réduction à nombre d'échantillons donné est l'enjeu d'une large famille de techniques.\n",
    "\n",
    "Remarque : loi des grands nombres ?\n",
    "\n",
    "A refinement of this method, known as importance sampling in statistics, involves sampling the points randomly, but more frequently where the integrand is large. To do this precisely one would have to already know the integral, but one can approximate the integral by an integral of a similar function or use adaptive routines such as stratified sampling, recursive stratified sampling, adaptive umbrella sampling[91][92] or the VEGAS algorithm.\n",
    "\n",
    "Another class of methods for sampling points in a volume is to simulate random walks over it (Markov chain Monte Carlo). Such methods include the Metropolis-Hastings algorithm, Gibbs sampling, Wang and Landau algorithm, and interacting type MCMC methodologies such as the sequential Monte Carlo samplers.\n",
    "\n",
    "Le choix de l'estimateur est crucial (la théorie de l'estimation permettant par exemple de déterminer si un estimateur donné est un ESBVM). De nombreuses techniques dites de réduction de la variance ont été développées dans le but d'améliorer la précision de l'estimation (réduire la variance de l'estimateur) et d'en diminuer le temps de calcul (pour la production d'échantillons) pour un effort d'échantillonnage donné. On trouve parmi ces techniques l'échantillonnage préférentiel (importance sampling), l'utilisation de variables de contrôle (control variates), de variables antithétiques (antithetic variates), la stratification (stratified sampling), etc.\n",
    "\n",
    "---\n",
    "\n",
    "In principle, Monte Carlo methods can be used to solve any problem having a probabilistic interpretation. By the law of large numbers, integrals described by the expected value of some random variable can be approximated by taking the empirical mean (a.k.a. the sample mean) of independent samples of the variable. When the probability distribution of the variable is parametrized, mathematicians often use a Markov chain Monte Carlo (MCMC) sampler. The central idea is to design a judicious Markov chain model with a prescribed stationary probability distribution. That is, in the limit, the samples being generated by the MCMC method will be samples from the desired (target) distribution.[7] By the ergodic theorem, the stationary distribution is approximated by the empirical measures of the random states of the MCMC sampler.\n",
    "\n",
    "---\n",
    "Rappels de base de la théorie de l'estimation :\n",
    "Soit (X1, ...,Xn) un n-écchantillon de X, (x1, ... xn) étant une réalisation de l'échantillon. On appelle statistique relative à l'échantillon toute variable aléatoire T = Phi(X1, ... Xn). La moyenne et la variance empirique de même que que les inf, sup et quantiles sont des statistiques pour un n-échantillon.\n",
    "\n",
    "Statistique vs estimateur : Soit X variable aléatoire de densité de probabilité f(x, theta), theta inconnu et dont le domaine de définition ne dépend pas de théta. Soit Phi(X1, Xn) une statistique relative à un échantillon indépendante de theta et à valeurs dans le domaine de définition de theta. T = Phi(X1, .. Xn) est un estimateur pour theta. Cette définition est finalement extrêmement large : un nombre très important de statistiques peuvent ainsi prétendre à être des estimateurs pour theta, mais lesquels sont du meilleur intérêt lorsqu'il s'agira d'approximer theta par la valeur prise par l'estimateur qu'on aura choisi sur une réalisation de notre échantillon ? On recherche donc en général parmi tous les estimateurs possibles pour theta :\n",
    "* Un estimateur sans biais : E(T) = theta. On préfère que la loi suivie par la v.a. T soit centrée sur theta.\n",
    "* Un estimateur de variance V(T) minimale. Les valeurs prises par l'estimateur nous sembleront d'autant plus fiables qu'on les sait être les moins dispersées possibles.\n",
    "\n",
    "On va donc chercher à construire pour theta, un estimateur T sans biais et de variance minimale (ESBVM). On peut montrer que la variance d'un ESB connait une borne inférieure qui ne peut être atteinte que si la loi de l'échantillon prend une forme particulière (inégalité FDCR). Si l'ESBVM existe, alors il est unique.\n",
    "\n",
    "On a pas mentionné jusqu'à présent la taille de l'échantillon alors qu'il peut paraître intéressant que la précision d'un estimateur s'accroisse quand la taille de l'échantillon augmente. On parle ainsi :\n",
    "* d'estimateur consistant ou convergent lorsque T converge en probabilité vers theta lorsque n tend vers +infinity.\n",
    "* d'estimateur fortement convergent lorsque T converge presque sûrement vers theta lorsque n tend vers +infinity.\n",
    "Remarque : Si V(T) -> 0 quand n -> +infinity alors T converge en moyenne quadratique et donc en probabilité vers theta.\n",
    "Toutefois cette propriété de convergence nous intéresse assez peu : la convergence peut être lente et augmenter la taille de l'échantillon peut être couteux. On travaille souvent à taille d'échantillon constante. Ce qui nous intéresse est de travailler avec le meilleur estimateur à notre disposition pour une taille d'échantillon donnée même s'il est rassurant de savoir que la précision de notre estimateur sera d'autant plus faible que la taille de notre échantillon sera grande.\n",
    "\n",
    "Exemple d'estimateurs connus :\n",
    "\n",
    "La moyenne empirique:\n",
    "* Si on suppose que la loi suivie par X admet une espérance et une variance, alors la moyenne empirique comme estimateur de l'espérance est :\n",
    "  - Sans biais\n",
    "  - De variance sigma^2/n (la précision de l'estimateur ne s'accroit qu'en sqrt(n))\n",
    "\n",
    "Remarque importante : ces résultats ne sont valables que pour un échantillon constitué par tirage avec remise dans une population infinie, les v.a. Xi constituant l'échantillon étant alors indépendantes et identiquement distribuées (de même loi). Dans le tirage sans remise dans une population finie, les Xi ne sont plus indépendantes. On montre que les correction apportées sont d'autant plus sensibles que la taille d'ensemble N de la population est faible et/ou que le taux de sondage n/N est élevé (avec n la taille de l'échantillon).\n",
    "\n",
    "Convergence :\n",
    "* Si la loi suivie par X admet une espérance finie m, alors la moyenne empirique converge en probabilité vers m quand la taille de l'échantillon tend vers l'infini (loi faible des grands nombres).\n",
    "* Si la loi suivie par X admet une espérance m finie et une variance finie, alors la moyenne empirique converge presque sûrement vers m quand la taille de l'échantillon tend vers l'infini (loi forte des grands nombres).\n",
    "* Loi limite pour la moyenne empirique : Si la loi suivie par X admet une espérance m finie et une variance finie, alors (quelle que soit la loi suivie par X), la loi limite suivie par X_barre - m / sigma/sqrt(n) est une N(0,1) (où sigma/sqrt(n) = sqrt(V(T)), avec V(T) la variance de l'estimateur): X_barre - m / sigma/sqrt(n) converge en loi vers une N(0, 1) (Théorème centrale limite). Ce résultat permet notamment la construction d'intervalles de confiances pour m.\n",
    "\n",
    "Remarque : on peut montrer que la moyenne empirique est l'ESBVM pour l'espérance pour un grand nombre de lois suivies par X (les démonstrations autour du caractère SBVM s'appuie beaucoup sur la forme de la loi suivie par l'échantillon).\n",
    "\n",
    "La variance empirique :\n",
    "* Si on suppose que la loi suivie par X admet une espérance et une variance, alors la variance empirique comme estimateur de la variance est :\n",
    "  - Biaisé : E(T) = (n-1)/n*sigma^2\n",
    "  - La variance de l'estimateur est de forme plus complexe mais approximable pas une quantité en O(1/n) quand n devient grand.\n",
    "  - La loi forte des grands nombres permet de montrer facilement que bien que biaisée, la variance empirique converge presque sûrement vers la variance de X quand la taille de l'échantillon tend vers l'infini.\n",
    "\n",
    "Remarque : de même que pour la moyenne empirique, si le tirage de l'échantillon se fait dans une population finie et sans remise, des corrections similaires viennent s'appliquer à l'espérance de l'estimateur (entre autres)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
