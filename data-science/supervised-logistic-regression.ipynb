{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression logistique\n",
    "La régression logistique est un algorithme de classification appartenant à l'ensemble plus large des modèles linéaires généralisés. \n",
    "\n",
    "## Modèles linéaires généralisés\n",
    "Les modèles linéaires généralisés (*Generalized Linear Models* - GLM) sont un ensemble de modèles supervisés reposant sur trois éléments principaux:\n",
    "* Une distribution postulée pour la réponse $Y$\n",
    "* Un prédicteur linéaire (*linear predictor*) $X\\beta$ avec $\\beta$ vecteur de paramètres à estimer,\n",
    "* Une fonction lien (*link function*) $g$ faisant la liaison entre les deux éléments précédents.\n",
    "\n",
    "Un GLM consiste à postuler une distribution (paramétrique) pour la réponse $Y$ d'espérance $\\mu$ et à en faire une distribution conditionnelle par rapport à $X$ en posant $\\mu=\\mu(x)=g^{-1}(X\\beta)$ avec $g$ la fonction lien.\n",
    "\n",
    "Les GLM sont des modèles probabilistes: il n'y a pas de valeur prédite $\\hat{y}$, le modèle ne donne qu'une distribution de probabilités pour $y$, distribution fonction de $x$.\n",
    "\n",
    "On peut voir un GLM comme un modèle postulant que la ligne de régression est une fonction du prédicteur linéaire: $\\mathbb{E}(Y|X)=g^{-1}(X\\beta)$. On peut alors parler de valeur prédite mais pas pour $y$, il s'agit plus d'un $\\hat{\\mu}$. Ce qu'on prédit c'est une distribution dont $y$ n'est qu'une réalisation. $\\hat{\\mu}$ correspond à l'estimation de la meilleure distribution possible au point $x$ au sens du maximum de vraisemblance.\n",
    "\n",
    "Remarque: Parler de ligne de régression et d'espérance conditionnelle présuppose qu'on se place dans un contexte de moindres carrés: l'espérance conditionnelle $\\mathbb{E}(Y|X)$ appelée aussi ligne de regression étant la meilleure prédiction de $Y$ possible au sens des moindres carrés.\n",
    "\n",
    "### Distribution de $Y$\n",
    "On exige d'elle deux propriétés: \n",
    "* Son support doit être cohérents avec le domaine de définition de la réponse\n",
    "* Elle doit appartenir à une famille de distributions paramétriques dont la densité de probabilité peut se mettre sous la forme canonique:\n",
    "\n",
    "$$f_Y(y|\\theta,\\tau)=h(y,\\tau).exp(\\frac{b(\\theta)T(y)-A(\\theta)}{d(\\tau)})$$\n",
    "\n",
    "Où $\\theta$ et $\\tau$ sont des paramètres de la distribution.\n",
    "\n",
    "Pour une réponse ne prenant comme valeurs que des entiers positifs, on peut choisir pour $P(Y)$ une distribution de Poisson: son support correspond à $\\mathbb{N}$ et sa fonction de densité peut se mettre sous la forme précédente. \n",
    "\n",
    "### Prédicteur linéaire $X\\beta$\n",
    "Les GLM font l'hypothèse que la réponse est une fonction d'un prédicteur linéaire $X\\beta$, d'une quantité correspondant à la somme des effets de chacune des variables du modèle. $\\beta$ correspond au vecteur de paramètres dont l'estimation correspond à l'ajustement du modèle.\n",
    "\n",
    "### Fonction lien $g$\n",
    "La fonction lien vient relier l'espérance $\\mu$ de la réponse/de la distribution postulée pour $Y$ au prédicteur linéaire en posant:\n",
    "\n",
    "$$g(\\mu)=X\\beta$$ \n",
    "\n",
    "En permettant de \"brancher\" $X$ dans la distribution de $Y$, la fonction lien nous permet d'obtenir une distribution conditionnelle qu'on va chercher à estimer. \n",
    "\n",
    "Choisir $g$ c'est aussi faire une hypothèse sur l'espérance conditionnelle car $\\mathbb{E}(Y|X)=\\mu(X)=g^{-1}(X\\beta)$. L'espérance conditionnelle est par ailleurs la meilleure approximation de $Y$ au sens des moindres carrés, il n'apparaît donc pas absurde que le lien entre $Y$ et le prédicteur linéaire passe par l'espérance de la distribution postulée pour $Y$. Au dela de ça, parmi tous les paramètres qu'on aurait pu choisir pour faire le lien (il n'y en a pas des centaines), l'espérance est le plus naturel.\n",
    "\n",
    "Faisant le lien entre le prédicteur linéraire et la réponse, le domaine de départ de $g$/le domaine d'arrivée de $g^{-1}$ doit être cohérent avec celui de la réponse. Dans le cas de la régression logistique par exemple, $\\mu \\in [0,1]$\n",
    "\n",
    "Remarque: Pour un certain nombre de distributions, le paramètre utilisé dans la fonction lien peut lier espérance et variance. Par exemple pour la loi de Poisson où espérance et variance de la loi sont égales à $\\lambda$ ou pour la loi binomiale (Bernoulli incluse) où $p$ intervient à la fois dans la variance et l'espérance. Cette contrainte imposée indirectement à la variance peut aboutir à une inadéquation entre variance du modèle et variance observée, la seconde étant alors souvent plus élevée que la première (*overdispersion*). \n",
    "\n",
    "### Exemples \n",
    "|Distribution de la réponse|Support de la réponse|Fonction lien|Remarques|\n",
    "|:--|:--|:--|:--|\n",
    "|Normale|$\\mathbb{R}$|Identité: $\\mu=X\\beta$|Régression linéaire|\n",
    "|Exp./Gamma|$\\mathbb{R}^{+}$|$-1/\\mu=X\\beta$|Rappel: $\\mu=k\\theta$|\n",
    "|Poisson|$\\mathbb{N}$|$ln(\\mu)=X\\beta$|Rappel: $\\mu=\\lambda$|\n",
    "|Binomiale/Bernoulli|${0,\\dots,n}$|Modèle logit (logistique): $ln(\\frac{\\mu}{1-\\mu})=X\\beta$<br>Modèle probit: $\\Phi^{-1}(\\mu)=X\\beta$ avec $\\Phi^{-1}(x)=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{x}e^{\\frac{t^2}{2}}dt$|Rappel: $\\mu=np$|\n",
    "\n",
    "Remarque: Le modèle probit est en apparence plus compliqué mais se révèle plus pratique dans certaines situation où il permet de rendre le problème *tractable* (où le modèle logit ne l'est pas) comme par exemple en estimation bayésienne. Dans tous les cas, les formes des fonctions logit et probit sont proches, d'où le fait qu'elles sont présentés comme deux variantes du modèle logistique.\n",
    "\n",
    "D'autres distributions suivant la forme canonique précisée plus haut, on peut également former des GLM à l'aide de lois binomiales (dont l'espérance correspond à une proportion de succès), binomiales négatives (qui modélise le nombre $k$ de succès attendu avant $n$ échecs), etc.\n",
    "\n",
    "### Estimation\n",
    "Modèles probabilistes, les GLM sont traditionnellement estimés au maximum de vraisemblance.\n",
    "\n",
    "On remarque toutefois que la regression linéaire est un cas particulier de GLM, celle-ci étant traditionnellement estimée aux moindres carrés. On remarque que l'estimation de $\\hat{\\beta}$ aux moindre carrés ne fait pas appel à la distribution des erreurs sauf si on souhaite avoir une distribution pour $\\hat{\\beta}$.\n",
    "\n",
    "On est alors pas surpris d'apprendre que l'estimation au maximum de vraisemblance du GLM linéaire postulant $Y$ gaussien d'espérance $X\\beta$ et de variance $\\sigma^2$ (qui revient à postuler que $Y$ se décompose en la somme d'un $X\\beta$ déterministe et d' erreurs gaussiennes d'espérance nulle et de variance $\\sigma^2$) donne exactement le même résultat que l'estimation du modèle linéaire aux moindres carrés.\n",
    "\n",
    "Cette décomposition en un terme déterministe (observable) et un terme d'erreur stochastique n'est cependant pas toujours possible. C'est pas exemple le cas de la régression logistique pour laquelle une telle décomposition demande à faire appel à des variables latentes, inobservables. Une telle décomposition a l'avantage de donner dans la définition du modèle un estimateur $\\hat{Y}$ pour $Y$, le terme d'erreur assurant le caractère sans biais s'il est centré et sa distribution permettant de dériver des propriétés de l'estimateur. Les modèles donnent tous un estimateur de $\\hat{\\mu}$ mais dans les cas où $Y$ est discret ce dernier ne peut être utilisé comme estimateur $\\hat{Y}$ pour $Y$.\n",
    "\n",
    "### Régression logistique\n",
    "On voit du tableau ci-dessus que la régression logistique correspond au GLM pour lequel:\n",
    "* On postule une distribution de Bernoulli pour la réponse: étant donné le point $x$ où je me trouve, quelle Bernoulli de paramètre $p$ est suit le plus vraisembablement $y$ ?\n",
    "* La fonction lien correspond à $g:x \\mapsto ln(\\frac{x}{1-x})$ avec $g^{-1}:x \\mapsto \\frac{1}{1-e^{-x}}$, la fonction dite logistique,\n",
    "* La fonction lien relie le prédicteur linéraire $X\\beta$ à l'espérance $\\mu=p=p(x)=P(Y=1|X)$\n",
    "* Etant donnée la forme de la fonction lien, on voit que le modèle logistique équivaut à postuler une spécification linéaire pour le *log odds* $ln(\\frac{P(Y=1|X)}{P(Y=0|X)})=ln(\\frac{p}{1-p})$\n",
    "* La spécification du modèle logisitique revient à modéliser le *log odds* comme une somme d'effets.\n",
    "\n",
    "Remarques: \n",
    "* On voit que les frontières de décision d'un modèle logistique sont linéaires. Sur la frontière: $P(Y=0|X)=P(Y=1|X)$, le lieu des points $X$ réalisant cette condition est donné par $X\\beta=0$ \n",
    "\n",
    "#### Ajustement\n",
    "Modèle probabiliste, la régression logistique est traditionnellement ajustée au maximum de vraisemblance. On maximise le plus souvent la log-vraisemblance (ou minimise la *negative log-likelihood*) qui supposant les observations i.i.d., s'écrit: \n",
    "\n",
    "$$Log\\mathcal{L(\\beta)=\\sum_{i}y_{i}.ln(h_{\\beta}(x_i))+(1-y_{i}).ln(1-h_{\\beta}(x_i))}$$\n",
    "\n",
    "avec $h_{\\beta}(x_i)=\\frac{1}{1+e^{-\\beta.x_i}}$\n",
    "\n",
    "Numériquement $\\hat{\\beta}$ peut être estimé par descente de gradient. Analytiquement, on peut résoudre le système d'équation obtenu en écrivant les différentes conditions du premier ordre par une méthode (itérative) type Newton-Raphson. L'exemple classique est l'*iteratively reweighted least squares* (IRLS) qui nous donne le $\\hat{\\beta}_{ML}$ mais aussi une distribution asymptotique (cf. par exemple ESL p.125) qui donne accès à la variance de l'estimation pour chaque coefficient et permet la réalisation de test de significativité pour chacun d'entre eux. \n",
    "\n",
    "#### Mesures de performance \n",
    "Malgré la confusion à laquelle peut prêter son nom, la régression logistique est un modèle de classification pour lequel on peut donc utiliser des métriques classiques de classification (*precision*, *recall*, etc.).\n",
    "\n",
    "Il existe aussi des mesures d'adéquation du modèle aux données (*goodness of fit*) plus \"statistiques classiques\". En particulier, on a un analogue du coefficient de détermination pour la régression linéaire, la déviance. \n",
    "\n",
    "De façon générale, la déviance est une mesure de dissimilarité, on a donc $d(x,x)=0$ et $d(x,y)\\geq0$. En régression linéaire (via des ratios de MSE) comme en régression logistique (via la déviance), on veut souvent comparer notre modèle à deux modèles de référence: \n",
    "* Le modèle \"nul\" (*null model*) qui ne comporte aucun prédicteur, seulement une constante. En régression linéaire, le coefficient de détermination correspond au ratio des MSE des modèles *fitté* et nul.\n",
    "* Le modèle \"saturé\" (*saturated model*) qui comporte autant de prédicteurs que d'observations et dont le *fit* est parfait. Ce modèle joue d'abord un rôle théorique puisqu'il n'est souvent pas connu en pratique. \n",
    "\n",
    "Par exemple, la déviance par rapport au modèle saturé s'écrit: $-2ln(\\frac{model\\:likelihood}{saturated\\:model\\:likelihood})$\n",
    "\n",
    "Dans ce cas comme dans la comparaison au modèle nul, on parle de *scaled deviance*.\n",
    "\n",
    "Comme pour la régression linéaire, on peut former des ratios de *goodness of fit* entre modèles emboités. On compare deux modèle $M_1$ et $M_2$, le second incluant le premier plus $k$ prédicteurs supplémentaire. Ayant plus de prédicteurs, $M_2$ est plus en adéquation avec les données que $M_1$. On cherche alors à savoir si $M_2$ est **significativement** plus en adéquation avec les données, si ce gain par rapport à $M_1$ est significatif. On entreprend alors un test statistique dont l'hypothèse nulle est que le modèle plus complexe $M_2$ ne *fit* pas significativement plus les données que $M_1$ (*goodnesses of fit* proches):\n",
    "* Dans le cas de la régression linéaire, la statistique de test correspond à un ratio de MSE qui sous l'hypothèse nulle suit une loi de Fisher.\n",
    "* Dans le cas de la régression logistique, la statistique de test correspond à la déviance entre les deux modèles (qui comporte un ratio de vraisemblances) qui sous l'hypothèse nulle suit une loi du $\\chi^2$. \n",
    "\n",
    "Dans le cas de la régression logistique, il existe également un certain nombres de métriques appelées pseudo-R² construites à partir de déviances ou de vraisemblances qui visent à doter la regression logistique d'un indicateur de *goodness of fit* compris entre 0 et 1 sur le modèle de la régression linéaire (la déviance appartenant à $\\mathbb{R}^+$ et pas seulement à $[0,1]$). Il reste que la déviance reste l'indicateur incontournable pour évaluer la significativité de la régression.\n",
    "\n",
    "Remarque: D'autres indicateurs comme l'AIC s'appuient aussi sur la vraisemblance.\n",
    "\n",
    "#### Cas d'un *dataset* déséquilibré\n",
    "Au dela du fait que le nombre d'observations appartenant à la classe minoritaire peut être insuffisant pour convenablement représenter sa distribution, le problème des *datasets* déséquilibré est pris en charge dans la régression logistique en pondérant la log-vraisemblance. \n",
    "\n",
    "Au lieu de maximiser:\n",
    "\n",
    "$$Log\\mathcal{L(\\beta)=\\sum_{i}y_{i}.ln(h_{\\beta}(x_i)+(1-y_{i}).ln(1-h_{\\beta}(x_i))}$$\n",
    "\n",
    "avec $h_{\\beta}(x_i)=\\frac{1}{1+e^{-\\beta.x_i}}$, on maximise: \n",
    "\n",
    "$$Log\\mathcal{L(\\beta)=\\sum_{i}w_0.y_{i}.ln(h_{\\beta}(x_i)+w_1.(1-y_{i}).ln(1-h_{\\beta}(x_i))}$$\n",
    "\n",
    "Les poids peuvent être déterminés par cross-validation. \n",
    "\n",
    "Dans `sklearn` l'argument dédié à la gestion de *datasets* déséquilibrés est `class_weight`. La logique est de pondérer les observations suivant leur appartenance à une classe: toutes les observations d'une même classe sont pondérées avec le même poids. En particulier, dans `class_weight='balanced'`, les poids sont choisis de façon à ce que les effectifs pondérés de chaque classe soient identiques et égaux à $n/N_c$ avec $N_c$ le nombre de classes et `n` le nombre total d'observations. L'option `class_weight='balanced'` recherche des effectifs pondérés équilibrés. En notant $n_k$ l'effectif de la classe $k$, $w_{i,k}$ le poid de la $i^e$ observation de la classe $k$ et en remarquant que pour tout $i$, $w_{i,k} = w_{k}$ puisque toutes les observations d'une même classe se voient assignées le même poids, on a:\n",
    "\n",
    "$$\\sum_{i=1}^{n_k}w_{i,k} = n_{k}.w_{k} = \\frac{n}{N_c}$$\n",
    "\n",
    "D'où l'expression du poids $w_k$ affecté à chaque observation de la classe $k$:\n",
    "\n",
    "$$w_{k} = \\frac{n}{n_{k}.N_c} = \\frac{p_k}{N_c}$$\n",
    "\n",
    "Avec $p_k=n_{k}/n$ la proportion d'observations de la classe $k$ dans l'effectif global. Cette équation correspond à l'expression `n_samples / (n_classes * np.bincount(y))` disponible dans la documentation.\n",
    "\n",
    "#### Régularisation\n",
    "Une approche de la régularisation d'une régression logistique est très proche de celle de la régression linéaire. Comme pour la régression linéaire, on trouve pour la régression logistique une régularisation $L_1$ (LASSO), $L_2$ (Ridge) ou mélangeant les deux (ElasticNet). La pénalisation pour la régularisation $L_1$ par exemple, vient simplement s'ajouter à la log-vraisemblance à maximiser: \n",
    "\n",
    "$$Log\\mathcal{L(\\beta)=\\sum_{i}y_{i}.ln(h_{\\beta}(x_i)+(1-y_{i}).ln(1-h_{\\beta}(x_i))} - \\lambda\\sum_{i}|\\beta_i|$$\n",
    "\n",
    "Remarque: Afin de maintenir une description unifiée de ses différents classificateur, `sklearn` utilise la variable `C` (emprunté aux SVM) pour désigner le paramètre contrôlant la force de la régularisation qui correspond à l'inverse du $\\lambda$ utilisé ici.\n",
    "\n",
    "Autre point de vue: on peut montrer que la régularisation $L_2$ est équivalent à une maximisation du *posterior* (*Maximum A Posteriori* - MAP) en choisissant un *prior* gaussien pour les $\\beta_i$.\n",
    "\n",
    "#### Colinéarité\n",
    "La présence de colinéarité semble avoir le même effet que pour la régression linéaire: l'estimation des coefficients $\\beta$ n'est pas biaisée mais l'erreur d'estimation augmente. Il semble aussi que la probabilité que l'estimation converge (descente de la *negative likelihood* diminue) diminue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
