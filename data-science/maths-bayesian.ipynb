{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (In)dépendance conditionnelle\n",
    "\n",
    "La notion d'indépendance conditionnelle correspond à l'extension de la notion d'indépendance aux probabilités conditionnelles. \n",
    "\n",
    "Pour rappel, deux événements indépendants $A$ et $B$ sont (inconditionnellement) indépendants si et seulement si (les propositions suivantes sont équivalentes):\n",
    "* Leur probabilité jointe s'écrit comme le produit de leurs probabilités: $P(A,B) = P(A).P(B)$ \n",
    "* L'observation de $B$ n'apporte aucune information sur l'occurence, l'observation de $A$: $P(A|B)=P(A)$\n",
    "* Et réciproquement: $P(B|A)=P(B)$\n",
    "\n",
    "Cette notion peut s'étendre à deux variables aléatoires, l'indépendance (inconditionnelle) étant établie si leur densité de probabilité $f(x,y)$ peut se factoriser en un produit de densités indépendantes $f(x,y)=g(x).h(y)$.\n",
    "\n",
    "La relation d'indépendance conditionnelle fait entrer en jeu trois événements $A$, $B$ et $C$ (et plus seulement deux événements). On dit que deux événements $A$ et $B$ sont conditionnellement indépendants de $C$ si et seulement si (les propositions suivantes sont équivalentes): \n",
    "* Leur probabilité conditionnelle jointe s'écrit comme le produit de leurs probabilités conditionnelles: $P(A,B|C)=P(A|C).P(B|C)$\n",
    "* Etant observé $C$, l'observation de $B$ n'apporte aucune information supplémentaire sur l'occurence de $A$: $P(A|B,C)=P(A|C)$\n",
    "* Et réciproquement: $P(B|A,C)=P(B|C)$\n",
    "\n",
    "Cette notion peut s'étendre à deux variables aléatoires, l'indépendance conditionnelle étant établie si leur densité de probabilité $f_{Z=z}(x,y)$ peut se factoriser en un produit de densités conditionnelles indépendantes $f_{Z=z}(x,y)=g_{Z=z}(x).h_{Z=z}(y)$.\n",
    "\n",
    "**L'indépendance inconditionnelle n'implique pas l'indépendance conditionnelle et réciproquement.**\n",
    "\n",
    "Exemple: implication\n",
    "Exemple: cond indep\n",
    "Exemple: cond dep\n",
    "\n",
    "Probabilistic Graphical Model (PGM)\n",
    "\n",
    "Les PGM pour lesquels les arrêtes du graphe sont orientés correspondent à la famille des réseaux bayésiens. Un graphe particulier sert à représenter une famille de distribution de probabilité. Une arrête/flèche du graphe permet de représenter une relation de dépendance conditionnelle entre deux noeuds, un noeud pouvant être une variable observable ou latente, un paramètre inconnu, etc. Dans de tels graphes, le fait que deux noeuds ne soient pas connectés matérialise une relation d'indépendance conditionnelle. \n",
    "\n",
    "Finalement ces graphes servent à rendre compte des relations de dépendance / d'indépendance conditionnelle entre variables et pourraient même être appelés \"diagrammes d'indépendance conditionnelle\". Ils servent ainsi à exprimer que la distribution de probabilité jointe de l'ensemble du modèle décrit se factorise d'une façon bien particulière. Dit autrement, cela ne définit pas de densité de probabilité mais donne des propriétés que cette dernière doit avoir car toutes les distributions ne sont pas capables de se factoriser de la façon correspondant au graphe.\n",
    "\n",
    "Connection directe, équivalence, entre le graphe, les relations d'indépendance conditionnelle entre les variables, et la factorisation sous laquelle peut se mettre la distribution jointe.\n",
    "\n",
    "On dit qu'une distribution jointe respecte un graphe (DAG) $G$ si elle se met sous la forme:\n",
    "\n",
    "$$p(x_1,...x_n)=\\Pi_{i}p(x_i|x_{parents(i)})$$\n",
    "\n",
    "Attention: respecter le graphe n'implique pas forcément l'existence de dépendances conditionnelles entre les variables. Par exemple, prenons trois variables aléatoires $X_1$, $X_2$ et $X_3$ inconditionnellement indépendantes. Leur distribution jointe s'écrit $p(x_1,x_2,x_3)=p(x_1).p(x_2).p(x_3)$\n",
    "\n",
    "Cette distribution satisfait aussi bien le graphe sans arrête [1]    [2]    [3] que le graphe [1]-->[2]  [3] (car $p(x_2)=p(x_2|x_1)$ puisque $X_1$ et $X_2$ indépendantes inconditionnellement). A propos de ce dernier cas: ce n'est pas parce que notre distribution respecte le graphe qu'une relation de dépendance conditionnelle existe entre $X_1$ et $X_2$ comme suggéré par le graphe.\n",
    "\n",
    "Remarque: Appelons un DAG complet, un DAG dont chaque noeud est connecté à tous les autres noeuds : Toute distribution jointe de $n$ variable respecte a priori tout DAG complet à $n$ noeuds (chaque relation d'indépendance conditionnelle supprimant une arrête).\n",
    "\n",
    "Pourquoi tout ce mal? Recenser et exploiter les différentes relations d'indépendance conditionnelle peut être d'un grand secours pour une inférence tractable et efficace.\n",
    "\n",
    "L'avantage des *directed graphical models* est qu'il permettent facilement de déduire les relations d'indépendance ou de dépendance conditionnelle entre deux variables par rapport à une troisième. Les structures de graphes peuvent se ramener à trois motifs élémentaires:\n",
    "\n",
    "*Tail to tail*: A <-- C --> B\n",
    "\n",
    "Un tel cas correspond à une **cause commune** (possiblement cachée). La probabilité jointe $P(A,B,C)$ s'écrit:\n",
    "\n",
    "$$P(A,B,C)=P(A|C).P(B|C).P(C)$$\n",
    "\n",
    "On montre que: \n",
    "* Sans connaissance/conditionner sur $C$, $A$ et $B$ ne sont pas (marginalement) indépendantes dans le cas général. En effet en marginalisant: $P(A,B)=\\Sigma_{c}P(A|C=c).P(B|C=c).P(C=c)$, l'expression obtenue ne se factorise **généralement pas** en $P(A).P(B)$.\n",
    "* En conditionnant sur $C$, on montre facilement que $P(A,B|C)=P(A,B,C)/P(C)$ se factorise en $P(A|C).P(B|C)$. $A$ et $B$ sont donc indépendantes conditionnellement à $C$. Dit autrement, dans un tel cas, connaître en plus de $C$ la valeur de $B$ par exemple, n'affecte pas la distribution de probabilité de $A$.\n",
    "\n",
    "Prenons par exemple $A=taille$, $B=vocabulaire$ et $C=âge$. Sans connaître l'âge, connaître la taille influence sur la probabilité d'avoir un vocabulaire important. Taille et vocabulaire ne sont pas inconditionnellement indépendants. En revanche, connaissant l'âge, connaître la taille n'affecte plus la probabilité d'avoir du vocabulaire. Taille et âge sont conditionnellement indépendants à l'âge.\n",
    "\n",
    "\n",
    "*Head to tail*: A --> C --> B\n",
    "\n",
    "Un tel cas correspond à une **chaîne causale**. La probabilité jointe $P(A,B,C)$ s'écrit:\n",
    "\n",
    "$$P(A,B,C)=P(A).P(C|A).P(B|C)$$\n",
    "\n",
    "On montre que: \n",
    "* Sans connaissance/conditionner sur $C$, $A$ et $B$ ne sont pas (marginalement) indépendantes dans le cas général. En effet en marginalisant: $P(A,B)=\\Sigma_{c}P(A,B|C=c)$, l'expression obtenue ne se factorise **généralement pas** en $P(A).P(B)$.\n",
    "* En conditionnant sur $C$, idem que *tail to tail*.\n",
    "\n",
    "*Head to head*: A --> C <-- B\n",
    "\n",
    "Un tel cas correspond à des **effets partagés**. La probabilité jointe $P(A,B,C)$ s'écrit:\n",
    "\n",
    "$$P(A,B,C)=P(A).P(B).P(C|A,B)$$\n",
    "\n",
    "On montre que: \n",
    "* Sans connaissance/conditionner sur $C$ et en marginalisant des deux côtés de l'expression ci-dessus que $P(A,B)$ se factorise en $P(A).P(B)$.\n",
    "* En conditionnant sur $C$, la probabilité $P(A,B|C)$ ne se factorise généralement pas en $P(A|C).P(B|C)$. \n",
    "\n",
    "L'indépendance conditionnelle n'est donc pas automatique dans ce cas là: en plus de conditionner sur $C$, conditionner sur $B$ peut altérer notre densité de probabilité conditionnelle pour $P(A|C)$. Prenons par exemple $A=pollution$, $B=fumer$ et $C=cancer$. Sachant qu'on a un cancer et qu'on fume, ces informations peuvent nous conduire à diminuer l'influence de la pollution (alors que fumer et la pollution occurent de façon indépendante).\n",
    "\n",
    "Autre exemple classique illustrant le second point, prenons $A$ et $B$ i.i.d. selon une $Ber(0,5)$ (jet de pièce) et définissons $C$ comme prenant la valeur $1$ si $A=B$, $0$ sinon. L'absence d'indépendance conditionnelle est claire, connaître $C$ et $B$ (par exemple) détermine totalement $A$.\n",
    "\n",
    "https://www.irt-systemx.fr/wp-content/uploads/2017/10/StatisticalCausality_SystemX2017.pptx-2.pdf\n",
    "\n",
    "## Embedded model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda Python 3.6.12",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
