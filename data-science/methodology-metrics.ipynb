{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métriques de classification : liens avec les tests d'hypothèse\n",
    "En théorie des tests, on définit deux principaux risques liés à la décision sur la base de la valeur prise par une statistique de test : \n",
    "* Le risque de première espèce $\\alpha$ correspondant au risque de rejeter à tort l'hypothèse nulle $H_0$.\n",
    "* Le risque de seconde espèce $\\beta$ correspondant au risque d'accepter à tort l'hypothèse nulle $H_0$.\n",
    "\n",
    "**Les hypothèses nulles correspondant en général à une absence d'effet**, les propositions précédentes peuvent se reformuler :\n",
    "* Le risque de première espèce $\\alpha$ correspond au risque de conclure à tort à la présence du phénomène : c'est le risque d'obtenir un faux positif.\n",
    "* Le risque de seconde espèce $\\beta$ correspond au risque de conclure à tort à l'absence du phénomène : c'est le risque d'obtenir un faux négatif.\n",
    "\n",
    "On en dérive les mesures complémentaires: \n",
    "* La spécificité du test $1-\\alpha$. La spécificité d'un test (resp. d'un classificateur) correspond à sa capacité à conclure à raison à l'absence d'effet, c'est à dire à accepter $H_0$ à raison (resp. à assigner correctement un membre de la classe négative qui lui serait présenté). Plus un test/classificateur est spécifique, moins il fait de faux-positifs, moins un membre de la classe négative est pris pour un membre de la classe positive, moins on conclut à tort à la présence d'effet, plus on conclut à raison à l'absence d'effet. Avec une spécificité parfaite, les éléments de la classe négative ne sont pas confondus avec la classe positive, un élément de la classe positive est alors simplement détecté ou non. Quand on détecte quelque chose, on est assuré il s'agit d'un membre de la classe positive car les éléments de la classe négative ne sont pas pris pour eux, le test est donc bien spécifique pour la classe positive.  \n",
    "* La sensibilité (ou puissance) du test $1-\\beta$. La sensibilité d'un test (resp. d'un classificateur) correspond à sa capacité à conclure à raison à la présence d'effet, c'est à dire à rejeter $H_0$ à raison (resp. à assigner correctement un membre de la classe positive qui lui serait présenté). Plus un test/classificateur est sensible, moins il fait de faux-négatifs, plus il est capable de détecter les vrai-positifs, moins un membre de la classe positive est pris pour un membre de la classe négative, moins on conclut à tort à l'absence d'effet, plus on conclut à raison à la présence d'effet.\n",
    "\n",
    "Spécificité et sensibilité sont antagonistes, chercher à augmenter l'un conduit à diminuer l'autre. \n",
    "\n",
    "Ces définitions sont résumées dans la table suivante :\n",
    "\n",
    "| Décision/Réalité | $H_0$ vraie | $H_0$ fausse|\n",
    "|:-------------------------------------:|:---------------------:|:--------------------:|\n",
    "|**$H_0$ acceptée**| $$1-\\alpha$$<br>Bonne décision<br>Vrai négatif | $$\\beta$$<br> Risque de 2e espèce<br>Faux négatif |\n",
    "|**$H_0$ rejetée**| $$\\alpha$$<br> Risque de 1ere espèce<br>Faux positif| $$1-\\beta$$<br>Bonne décision<br>Vrai positif |\n",
    "\n",
    "Transposé au cas de la classification binaire, on retrouve la matrice de confusion du classificateur:\n",
    "\n",
    "| Prédiction/Réalité | Positif | Négatif |\n",
    "|:-------------------------------------:|:---------------------:|:--------------------:|\n",
    "|**Prédit positif**| Vrai positif (TP) | Faux positif (FP) <br> Erreur de 1ere espèce<br> |\n",
    "|**Prédit négatif**| Faux négatif (FN)<br> Erreur de 2e espèce<br>| Vrai négatif (TN) |\n",
    "\n",
    "Suivant si on normalise par les effectifs \"prédits\" ou les effectifs \"réels\", on obtient deux familles de ratios différentes : \n",
    "* Normalisé par les effectifs \"réels\" (P = TP+FN et N=TN+FP), on obtient : \n",
    "\n",
    "| Prédiction/Réalité | Positif | Négatif |\n",
    "|:-------------------------------------:|:---------------------:|:--------------------:|\n",
    "|**Prédit positif**| TP/(TP+FN) <br>True Positive Rate<br> <br>ou **Recall** ou **Sensibilité**<br> | FP/(TN+FP)<br>False Positive Rate<br>|\n",
    "|**Prédit négatif**| FN/(TP+FN) <br>False Negative Rate<br>| TN/(TN+FP) <br>True Negative Rate<br> <br>ou **Spécificité**<br>|\n",
    "\n",
    "* Normalisé par les effectifs \"prédits\" (PP = TP+FP et PN=TP+FN), on obtient : \n",
    "\n",
    "| Prédiction/Réalité | Positif | Négatif |\n",
    "|:-------------------------------------:|:---------------------:|:--------------------:|\n",
    "|**Prédit positif**| TP/(TP+FP) <br>**Precision**<br> | FP/(TN+FN)<br>False Discovery Rate<br>|\n",
    "|**Prédit négatif**| FN/(TP+FP) <br>False Omission Rate<br>| TN/(TN+FN) <br>Negative Predicted Value<br>|\n",
    "\n",
    "Le premier jeu de mesures et de grandeurs (spécificité, sensibilité) est définit par rapport à chacune des classes: si on me présente un membre d'une des classes, quelle est la probabilité pour que je l'assigne à tort ou à raison à l'autre ? Ces mesures ne disent rien de la qualité de la prédiction dans son ensemble qui peut être fortement impactée par le déséquilibre entre les classes. D'où un deuxième jeu de mesures normalisé sur les prédiction et plus sur les classes et qui répond aux question du type: si on me présente un membre des prédits positifs, quelle est la probabilité pour qu'il soit bien un vrai positif ? \n",
    "\n",
    "Ainsi, la différence entre précision et spécificité est d'autant plus tangible que les classes sont déséquilibrées. Précision de 1 équivaut à spécificité de 1 qui équivaut à 0 faux-positifs. En présence d'une classe négative très majoritaire, le nombre absolu de faux-positifs peut être faible devant le nombre de vrai-négatifs (spécificité élevée) mais important devant le nombre de vrai-positifs (précision faible).\n",
    "\n",
    "On déduit de ces tables et des définitions de la spécificité et de la sensibilité que les analogues pour la classification des risques de première et seconde espèce $\\alpha$ et $\\beta$ sont le *False Positive Rate* et le *False Negative Rate* respectivement.\n",
    "\n",
    "Les notions de spécificité et de sensibilité se transposent bien entendu aux classificateurs:\n",
    "* Plus un classificateur est spécifique, moins il tend à prendre un élément de la classe négative pour un membre de la classe positive (pas de faux-positifs): le classificateur est davantage spécifique pour la classe positive. Un classificateur avec une spécificité de 1 n'est pas pour autant parfait : il ne prend jamais de membre de la classe négative pour la classe positive mais il peut ne pas détecter tous les membres de cette dernière (faux-négatifs). Dans l'écrasante majorité des cas, le risque qu'on se donne est un risque de première espèce (exemple: pour les p-values). Quitte à ce que notre test/classificateur soit peu puissant/sensible, on ne veut le plus spécifique possible: au pire il manquera certains cas positifs mais on aura peu de faux-positifs. \n",
    "* Plus un classificateur est sensible, moins il se contente de détecter les cas les plus saillants, plus grande est sa capacité à trouver l'ensemble des cas positifs existant dans l'échantillon qu'on lui présente même les plus difficiles. Plus il est sensible, moins il manque d'événements au risque de dégrader sa spécificité (d'où l'antagonisme).\n",
    "\n",
    "### Accuracy \n",
    "\n",
    "*Balanced accuracy*: Doit se comparer à l'accuracy. Permet de mettre en évidence et est plus adaptée aux situations où la performance du classificateur n'est pas comparable sur les deux classes (ex: on est très bon à détecter la classe négative - bonne spécificité - mais pas aussi bon à détecter la classe positive - sensibilité moyenne -) ET nos classes sont particulièrement déséquilibrées. Ce déséquilibre va biaiser l'accuracy vers la performance du classificateur sur la classe majoritaire. La balanced accuracy donne une égale importance aux deux permformances et va donc d'autant plus différer de l'accuracy que le déséquilibre entre les classes et les performances est important. En cas de performance équilibrée sur les deux classes: les deux coincident. \n",
    "\n",
    "A vérifier/reformuler: essayer de faire apparaître les effectifs relatifs dans l'accuracy, de la décomposer en deux termes comme la balanced accuracy qui font apparaître la pondération.\n",
    "\n",
    "## Courbe ROC\n",
    "Pour un test statistique donné, la courbe ROC correspond au lieu de points de l'ensemble des couples $(\\alpha, 1-\\beta) = (1-specificity, sensitivity)$ paramétré par la valeur du seuil de décision choisi pour la statistique de test. Dit autrement, la courbe ROC correspond à la fonction $1-\\beta=f(\\alpha)$ ou encore $Sensitivity=f(1-Specificity)$.\n",
    "\n",
    "La définition pour les classificateurs est identique: \n",
    "* La courbe ROC est la représentation graphique de $Sensitivity=f(1-Specificity)$\n",
    "* Ou de manière équivalente: $Recall=f(1-Specificity)$\n",
    "* Ou de manière équivalente: $True Positive Rate=f(False Positive Rate)=f(1-True Negative Rate)$\n",
    "\n",
    "La courbe ROC ne peut par définition être tracée que pour les classificateur retournant un score et non directement un label. La courbe ROC permet alors de mesurer les performances du classificateur lorsqu'on fait varier le seuil de décision.\n",
    "\n",
    "Graphiquement: \n",
    "* Dans le cas des tests statistiques, on trace les distributions de la statistique de test sous $H_0$ et sous $H_1$. Pour une seuil de décision donné, on peut calculer la matrice de confusion du classificateur correspondant et en déduire un point de la coube ROC. Le lieu des points de la courbe ROC du test est obtenu en faisant varier le seuil de décision (sur $\\mathbb{R}$ entier sauf restrictions propres à la statistique de test et aux valeurs qu'elle prend).\n",
    "* Dans le cas des classificateurs, une interprétation quasi-équivalente est possible : il suffit de tracer la distribution des scores retournées par le classificateur pour chacune des deux classes. Le même raisonnement s'applique alors. \n",
    "\n",
    "Voir ce [lien](https://github.com/dariyasydykova/open_projects/tree/master/ROC_animation) pour des schémas explicatifs illustrant le raisonnement ci-dessus (inclut courbes ROC et PR).\n",
    "\n",
    "On déduit immédiatement de ce raisonnement une propriété importante de la courbe ROC : elle est relativement indépendante du caractère équilibré ou non du jeu de données.  \n",
    "\n",
    "Eléments remarquables: \n",
    "* Le point $(0, 1)$ correspond au classificateur parfait : aucun faux-positif, tous les prédits positifs sont des vrais-positifs. \n",
    "* Les points $(0, 0)$ et $(1, 1)$ correspondent à des classificateurs ne retournant comme prédiction que la classe négative et positive respectivement et correspondent aux valeurs extrême des seuils de décision. La courbe ROC passe donc par ces deux points. \n",
    "* La courbe ROC est concave (et donc au-dessus de $(0, 0)$ et $(1, 1)$ d'après le point précédent). Cela est conforme à l'intuition: partant de $(0, 0)$, plus on baisse la spécificité (on se déplace vers la droite sur l'axe des abscisses), plus on s'attend à ce que le classificateur soit sensible (on se déplace vers de haut sur l'axe des ordonnées). \n",
    "* On déduit du point précédent que la courbe ROC du classificateur parfait relie $(0, 0)$ à $(0, 1)$ et $(0, 1)$ à $(1, 1)$. L'aire sous la courbe ROC (en anglais *area under curve* - AUC) du classificateur parfait est donc égale à 1. C'est par définition une métrique d'évaluation d'un classificateur indépendante du seuil de décision et par la propriété de la courbe ROC, indépendante du caractère équilibré ou non du jeu de données. Dans le cas des tests comme des classificateurs, un test ou un classificateur est d'autant meilleur que son AUC est proche de 1.\n",
    "\n",
    "### Compromis Sensibilité/Spécificité\n",
    "Comme pour son utilisation originale, on peut choisir à l'aide de la courbe ROC un point de fonctionnement pour le système étudié. Une règle permettant un choix optimal suivant un certain critère reste à ajouter ici mais on se doute que le point à partir duquel la courbe commence à saturer semble être un bon compromis : à partir de ce point, se décaler plus à droite sur la courbe induit des gains de sensibilité limités pour une perte croissante de spécificité. \n",
    "\n",
    "### Le cas du classificateur aléatoire: \n",
    "Un classificateur aléatoire se caractérise par des distributions des scores identiques pour les deux classes. On déduit immédiatement de cette idée que le tracé de la courbe ROC du classificateur aléatoire par variation du seuil de décision est une droite: TPR et FPR sont en effet toujours égaux du fait de la superposition des distributions. La courbe ROC du classificateur aléatoire est donc le segment de droite reliant $(0, 0)$ et $(1, 1)$. On en déduit immédiatement que le classificateur aléatoire a une AUC de 0.5.\n",
    "\n",
    "Remarque : Un classificateur avec une AUC inférieure à 0.5 n'a pas grand sens. A l'extrême, considérons le classificateur parfaitement inexact dont la courbe ROC passe par $(1, 0)$: il suffit d'inverser toutes ses prédictions pour obtenir le classificateur parfait.\n",
    "\n",
    "## Courbe Precision/Recall (*PR curve*)\n",
    "Une autre courbe fréquemment utilisée pour l'appréciation de la performance d'un classificateur binaire est la courbe Precision/Recall correspondant à représentation graphique de $Precision=f(Recall)$. **Attention: Contrairement à la courbe ROC, le *recall* est ici en abscisses et non en ordonnées**.\n",
    "\n",
    "Graphiquement: \n",
    "* La courbe commence au voisinage de $(0, 1)$ (impossible d'avoir simultanément une précision positive et un recall stritement nul). \n",
    "* Le classificateur parfait correspond au point $(1, 1)$ : au point $(1, 1)$, toutes les prédictions sont des vrais-positifs (*precision* de 1) et tous les vrais-positifs de la population sont détectés (*recall* de 1).\n",
    "* La courbe décroit (pas forcément strictement) de $(0, 1)$ vers $(1, p)$ : \n",
    "    * La décroissance de la courbe est une traduction du compromis Precision/Recall.\n",
    "    * Elle ne décroit pas forcément jusqu'à $(1, 0)$ mais s'arrête plus généralement à la valeur $p$ de la *precision* pour un *recall* de 1. Cette valeur dépend entre autres de l'équilibre des effectifs entre les deux classes.\n",
    "    \n",
    "Comme pour la courbe ROC, on peut définir une notion d'AUC qui donne une appréciation de la qualité du classificateur indépendante du seuil de décision. Plus on est proche de 1, plus de classificateur est bon.\n",
    "\n",
    "Comme pour la courbe ROC, la courbe PR peut servir à choisir le \"point de fonctionnement\" du système, c'est à dire le meilleur seuil de décision. Dans le cas de la courbe PR, on peut notamment s'aider de courbes iso-F1. \n",
    "\n",
    "### Compromis Precision/Recall\n",
    "Comme pour la courbe ROC, les deux grandeurs constituant la courbe PR sont le plus souvent antagonistes: on ne peut pas améliorer l'une sans dégrader l'autre.\n",
    "\n",
    "Si la forme de la courbe PR s'y prête, le seuil de décision correspondant au point de la courbe se situant avant la forte décroissance de la courbe parait être le meilleur compromis : plus on se décale vers les valeurs de *recall* croissant, plus la *precision* chute, cette chute étant notamment sensible au déséquilibre de population entre les classes. A partir ce de point de rupture, les gains de *recall* se payent par une baisse importante de la *precision*.\n",
    "\n",
    "### Sensibilité au déséquilibre des classes\n",
    "Par le choix des métriques, la courbe PR se focalise essentiellement sur la classe \"positive\".\n",
    "\n",
    "Pour rappel, la courbe ROC trace la sensibilité (TP/P) en fonction du complémentaire de la spécificité (TN/N). Elle est par conséquent assez insensible au déséquilibre éventuel entre les deux populations. Le chevauchement et la forme des distributions des scores des deux classes importe mais pas leur \"taille\" relative.\n",
    "\n",
    "En utilisant la précision (TP/PP = TP/(TP+FP)), la courbe PR se rend sensible à un possible déséquilibre entre les classes. Si la classe \"négative\" est par exemple largement majoritaire, le nombre de faux-positifs peut être rapidement très important par rapport au nombre de vrais-positifs (et donc résulter en une *precision* faible) même pour des valeurs de *recall* faibles.\n",
    "\n",
    "Un classificateur peut ainsi avoir une performance (AUC) honorable du point de vue de la courbe ROC mais catastrophique du point de vue de la courbe PR. C'est pour cela qu'on recommande l'utilisation de la courbe PR dans le cas de problèmes fortement déséquilibrés. \n",
    "\n",
    "#### Interprétation graphique:\n",
    "Dans le cas de la courbe ROC, la sensibilité par exemple se calcule comme le ratio d'une partie de l'aire de la distribution sur l'aire totale de la même distribution (celle des score de la classe positive). Il est est de même pour le False Positive Rate et la distribution des scores de la classe négative. Ceci explique la relative indifférence de la ROC curve à un déséquilibre entre les classes : chacune des deux mesure ne s'intéresse qu'à une même classe. \n",
    "\n",
    "Dans le cas de la courbe PR, l'utilisation de la *precision* introduit une sensibilité au déséquilibre des populations (cf. plus haut). Graphiquement, son calcul fait intervenir au dénominateur l'aire d'une partie de la distribution des scores positifs (TP) ET l'aire d'une partie de la distribution des scores négatifs (FP), la seconde pouvant en cas de fort déséquilibre être bien plus élevée que la première (qui correspond également au numérateur). \n",
    "\n",
    "Autour de ces considérations sur les problèmes déséquilibrés et les limitations de la courbe ROC, voir aussi :\n",
    "* https://www.kaggle.com/lct14558/imbalanced-data-why-you-should-not-use-roc-curve\n",
    "* https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4349800/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
