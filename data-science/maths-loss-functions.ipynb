{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do\n",
    "Pointer les classiques. Celles utilisées dans `sklearn`. Minimisation de la loss/maximisation de la likelihood (sous entend de travailler avec une distribution ?)\n",
    "\n",
    "# Régression\n",
    "\n",
    "## *Mean square error*\n",
    "$$\\mathcal{L}(f(x), y)=(y-f(x))^2$$\n",
    "\n",
    "La MSE s'écrit:\n",
    "\n",
    "$$MSE= \\frac{1}{n}\\sum_{i}\\mathcal{L}(f(x_i), y_i)=\\frac{1}{n}\\sum_{i}(y_i-f(x_i))^2$$\n",
    "\n",
    "Variante pondérée:\n",
    "\n",
    "$$wMSE= \\frac{1}{n}\\sum_{i}w_{i}\\mathcal{L}(f(x_i), y_i)=\\frac{1}{n}\\sum_{i}w_{i}(y_i-f(x_i))^2$$\n",
    "\n",
    "# Classification\n",
    "Les *loss functions* de classification sont en général des fonctions décroissantes d'une quantité appelée la marge (*margin*) $yf(x)$:\n",
    "* La marge est négative lorsqu'une observation est mal classifiée (i.e. du mauvais côté de la frontière de décision).\n",
    "* La marge est positive lorsqu'une observation est correctement classifiée.\n",
    "* Plus la taille de la marge donne une idée de la force avec laquelle une observation a été placée du bon côté ou non de la frontière de décision. Une observation a été correctement classée avec d'autant plus de certitude que la marge est positive, qu'elle se trouve \"loin\" en territoire \"positif\". Par opposition une observation est d'autant plus mal classée que sa marge est négative et donc qu'elle a été placée loin en territoire \"négatif\".\n",
    "\n",
    "Les *loss functions* s'appuyant sur la marge se distinguent :\n",
    "* Par la pondération appliquée aux exemples particulièrement mal classés (marge fortement négative). Ex: *hinge loss* vs *squared hinge loss*\n",
    "* Par leur régularité plus ou moins grande, notamment au niveau du passage de la frontière (marge nulle). Ex: 0/1 *loss* ou *hinge loss* vs. *logistic loss*\n",
    "* Par des comportements particuliers au voisinage de la frontière. Ex: epsilon-insensitive loss (?).\n",
    "\n",
    "## *Cross entropy loss, maximum likelihood & Kullback–Leibler divergence*\n",
    "\n",
    "### La divergence de Kullback–Leibler\n",
    "La divergence de Kullback–Leibler est une mesure statistique permettant de quantifier l'écart entre une distribution de probabilité candidate $Q$ et une distribution de référence $P$, les deux partageant le même domaine $\\mathcal{X}$.\n",
    "\n",
    "Remarque: Cette mesure provient de la théorie de l'information où elle est parfois désignée sous le terme de *relative entropy*.\n",
    "\n",
    "Pour $P$ et $Q$ discrètes, la divergence de $Q$ par rapport à $P$ s'écrit:\n",
    "\n",
    "$$D_{KL}(P||Q) = \\sum_{x\\in\\mathcal{X}}P(X=x)log(\\frac{P(X=x)}{Q(X=x)})$$\n",
    "\n",
    "Dans le cas où $P$ et $Q$ sont continues:\n",
    "\n",
    "$$D_{KL}(P||Q) = \\int_{x\\in\\mathcal{X}}p(x)log(\\frac{p(x)}{q(x)})dx$$\n",
    "\n",
    "On remarque que la divergence peut être négative, elle est nulle quand les deux distributions sont identiques. Elle n'est pas symétrique, ce n'est donc pas une distance. Reécrite autrement pour le cas discret: \n",
    "\n",
    "$$D_{KL}(P||Q) = \\sum_{x\\in\\mathcal{X}}P(X=x)log(Q(X=x)) - \\sum_{x\\in\\mathcal{X}}P(X=x)log(P(X=x)) $$\n",
    "\n",
    "Dans le jargon de la théorie le l'information, la KL-divergence de $Q$ par rapport à $P$ appelée aussi entropie relative de $Q$ par rapport à $P$. Cette dernière se définie comme la différence cross-entropie de $Q$ par rapport à $P$ moins l'entropie de $P$ (qui correspondent respectivement aux deux termes de la différence ci dessus). La cross-entropie de $Q$ par rapport à $P$ se définissant donc pour le cas discret comme:\n",
    "\n",
    "$$CE(P,Q) = \\sum_{x\\in\\mathcal{X}}P(X=x)log(Q(X=x))$$ \n",
    "\n",
    "Remarque: La cross-entropie est souvent notée $H$. On a donc par définition: \n",
    "\n",
    "$$D_{KL}(P||Q) = H(P,Q) - H(P)$$\n",
    "\n",
    "### Equivalence entre *maximum likelihood* et *minimum KL-divergence*\n",
    "Dans le cadre de l'estimation de modèles paramétriques au maximum de vraisemblance, on montre que cette estimation est asymptotiquement équivalente à la minimisation de la KL-divergence entre la densité de probablité maximisant la vraisemblance $Q_{\\hat{\\theta}}$ et la \"réelle\" densité de probabilité ayant généré nos données $P_{\\theta_0}$. Dans le cas général $P$ n'a pas à appartenir à la famille de distributions paramétriques qu'on postule pour la génération de nos données $Q_{\\theta}$ (notre modèle est alors biaisé, imparfaitement spécifié). Cela ne change pas le fait que l'estimation au maximum de vraisemblance nous donne la distribution $Q_{\\hat{\\theta}}$ \"la plus proche\" de la vraie\" distribution $P$ au sens de la KL-divergence et compte tenu de la contrainte d'appartenance à la famille de distributions $Q_{\\theta}$.\n",
    "\n",
    "Posons un problème d'estimation d'une distribution paramétrique au maximum de vraisemblance $Q_{\\theta}$. On cherche $\\hat{\\theta}$ tel que la vraisemblance de $Q_{\\theta}$ est maximisée sur un échantillon de $n$ observations $(x_1, x_2, \\dots, x_n)$ supposées i.i.d. et tirées d'une distribution $P$ a priori inconnue. Le problème s'écrit:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "argmax_{\\theta} \\prod_{i=1}^{n} Q_{\\theta}(x_i) &= argmax_{\\theta} \\prod_{i=1}^{n} Q(x_i|\\theta) \\\\\n",
    "&= argmax_{\\theta} \\sum_{i=1}^{n} log(Q(x_i|\\theta)) \\\\\n",
    "&= argmax_{\\theta} \\sum_{i=1}^{n} log(Q(x_i|\\theta)) - \\sum_{i=1}^{n} log(P(x_i)) \\\\\n",
    "&= argmin_{\\theta} \\sum_{i=1}^{n} log(\\frac{P(x_i)}{Q(x_i|\\theta)}) \\\\\n",
    "&= argmin_{\\theta} \\frac{1}{n}\\sum_{i=1}^{n} log(\\frac{P(x_i)}{Q(x_i|\\theta)}) \\\\ \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Les trois dernières lignes se justifient par le fait qu'on ajoute un terme indépendant de $\\theta$ ($P(x_i)$) puis qu'on multiplie par une constante ($1/n$). L'étape suivante consiste à remarquer qu'on a affaire à la moyenne empirique de $log(\\frac{P(x_i)}{Q(x_i|\\theta)})$. Asymptotiquement, quand le nombre d'observations devient très grand, i.e. quand $n$ tend vers l'infini, la moyenne empirique converge par la loi des grands nombres vers l'espérance. On a donc: \n",
    "\n",
    "$$ argmin_{\\theta} \\frac{1}{n}\\sum_{i=1}^{n} log(\\frac{P(x_i)}{Q(x_i|\\theta)}) \\xrightarrow[n \\to +\\infty]{} argmin_{\\theta} \\mathbb{E}_x[log(\\frac{P(x)}{Q(x|\\theta)})]$$\n",
    "\n",
    "Or, les données suivant $P$, on a:\n",
    "\n",
    "$$\\mathbb{E}[log(\\frac{P(x_i)}{Q(x_i|\\theta)})] = \\int_{x}P(x)log(\\frac{P(x)}{Q(x|\\theta)})dx = D_{KL}(P||Q_{\\theta})$$\n",
    "\n",
    "Maximiser la vraisemblance est donc bien équivalent à minimisation de la KL-divergence de $Q_{\\theta}$ par rapport à $P$. La KL-divergence étant même la différence entre la cross-entropie et l'entropie de $P$ indépendante de $\\theta$, la minimisation de la KL-divergence est équivalente à celle de la cross-entropie.\n",
    "\n",
    "### La cross-entropie dans les problèmes de ML\n",
    "Dans le contexte du ML, la cross-entropie apparaît dans les problèmes de classification multiclasses dans lesquels ont cherche à estimer probabilité (conditionnelle aux données) d'appartenir à chaque classe. La cross entropie apparaît comme la *loss* du problème et s'écrit: \n",
    "\n",
    "$$CE(\\hat{y}, y) = -\\sum_{i}y_i.log(\\hat{y_i})$$\n",
    "\n",
    "Où $\\hat{y_i}$ est la probabilité prédite d'appartenir à la classe $i$ comprise entre $0$ et $1$ et où $y_i$ est égale à $1$ si l'observation appartient à la classe $i$, $0$ sinon.\n",
    "\n",
    "On remarque que cette définition de la cross-entropie est équivalente à la *negative log-likelihood* d'une distribution catégorielle (appelée aussi multinoulli ou Bernoulli généralisée). La minimisation de la cross-entropie dans ce genre de problèmes est donc équivalente à une maximisation de la vraisemblance en postulant une spécification catégorielle/multinoulli pour notre modèle.\n",
    "\n",
    "Du point de vue de la KL-divergence dans cette définition (on ne connaît pas le processus génératif réel), on cherche à minimiser la divergence entre $Q_{\\theta}$ postulée multinoulli et $P$ qui est en fait la *ground truth*, où pour l'observation $y^{[k]}$ appartenant à la classe $i$, le vecteur de probabilité est nul partout sauf pour sa $i^e$ composante qui est égale à $1$: on appartient avec certitude à la classe $i$, $y^{[k]}$ peut être vu comme ayant été généré par une multinoulli qui génère des éléments de la classe $i$ avec une probabilité de $1$. Dès lors, le terme de cross-entropie de la KL-divergence coïncide bien avec celui définit ci-dessus.\n",
    "\n",
    "## 0/1 *loss*\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(f(x), y)=\\left\\{\\begin{array}{ll}{0} & {\\text { if } y=\\operatorname{sign}(f(x))} \\\\ {1} & {\\text { otherwise }}\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Inconvénients: \n",
    "* *Loss function* non régulière \n",
    "\n",
    "## *Logistic loss*\n",
    "\n",
    "Dans le cas des modèles probabilistes faisant l'hypothèse que $p(y|x)$ est donnée par la fonction logistique $\\sigma$ telle que:\n",
    "\n",
    "$$p(y|x) = \\sigma(yf(x)) = \\frac{1}{1+exp(-yf(x))}$$\n",
    "\n",
    "La fonction $f$ finalement retenue est celle maximisant la vraisemblance. Or la *logistic loss* correspond aussi à la *negative conditionnal likelihood* faisant que maximiser la vraisemblance revient à minimiser la logistic loss (à vérifier). La *logistic loss* s'écrit donc:\n",
    "\n",
    "$$\\mathcal{L}(f(x),y) = -ln(p(y|x) = ln(1+exp(-yf(x)))$$\n",
    "\n",
    "## SVM *losses*\n",
    "Appelées *hinge loss* et *squared hinge loss* ou 1-SVM *loss* et 2-SVM *loss* respectivement. \n",
    "\n",
    "* *Hinge loss*: $max(1-u, 0)$\n",
    "* *Squared hinge loss*: $max(1-u, 0)^2$\n",
    "\n",
    "Avec $u$ la marge\n",
    "\n",
    "Fonctions convexes qui forcent les points à s'éloigner de la frontière (située pour une marge égale à 0 mais la loss n'est nulle qu'à partir d'une marge de 1). Non différentiables: problème contourné en introduisant des slack variables. \n",
    "\n",
    "## Boosting\n",
    "Adaboost (?) : $exp(-u)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
