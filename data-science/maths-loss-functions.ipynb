{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do\n",
    "Pointer les classiques. Celles utilisées dans `sklearn`. Minimisation de la loss/maximisation de la likelihood (sous entend de travailler avec une distribution ?)\n",
    "\n",
    "# Régression\n",
    "\n",
    "## *Mean square error*\n",
    "$$\\mathcal{L}(f(x), y)=(y-f(x))^2$$\n",
    "\n",
    "La MSE s'écrit:\n",
    "\n",
    "$$MSE= \\frac{1}{n}\\sum_{i}\\mathcal{L}(f(x_i), y_i)=\\frac{1}{n}\\sum_{i}(y_i-f(x_i))^2$$\n",
    "\n",
    "Variante pondérée:\n",
    "\n",
    "$$wMSE= \\frac{1}{n}\\sum_{i}w_{i}\\mathcal{L}(f(x_i), y_i)=\\frac{1}{n}\\sum_{i}w_{i}(y_i-f(x_i))^2$$\n",
    "\n",
    "# Classification\n",
    "Les *loss functions* de classification sont en général des fonctions décroissantes d'une quantité appelée la marge (*margin*) $yf(x)$:\n",
    "* La marge est négative lorsqu'une observation est mal classifiée (i.e. du mauvais côté de la frontière de décision).\n",
    "* La marge est positive lorsqu'une observation est correctement classifiée.\n",
    "* Plus la taille de la marge donne une idée de la force avec laquelle une observation a été placée du bon côté ou non de la frontière de décision. Une observation a été correctement classée avec d'autant plus de certitude que la marge est positive, qu'elle se trouve \"loin\" en territoire \"positif\". Par opposition une observation est d'autant plus mal classée que sa marge est négative et donc qu'elle a été placée loin en territoire \"négatif\".\n",
    "\n",
    "Les *loss functions* s'appuyant sur la marge se distinguent :\n",
    "* Par la pondération appliquée aux exemples particulièrement mal classés (marge fortement négative). Ex: *hinge loss* vs *squared hinge loss*\n",
    "* Par leur régularité plus ou moins grande, notamment au niveau du passage de la frontière (marge nulle). Ex: 0/1 *loss* ou *hinge loss* vs. *logistic loss*\n",
    "* Par des comportements particuliers au voisinage de la frontière. Ex: epsilon-insensitive loss (?).\n",
    "\n",
    "## 0/1 *loss*\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(f(x), y)=\\left\\{\\begin{array}{ll}{0} & {\\text { if } y=\\operatorname{sign}(f(x))} \\\\ {1} & {\\text { otherwise }}\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Inconvénients: \n",
    "* *Loss function* non régulière \n",
    "\n",
    "## *Logistic loss*\n",
    "\n",
    "Dans le cas des modèles probabilistes faisant l'hypothèse que $p(y|x)$ est donnée par la fonction logistique $\\sigma$ telle que:\n",
    "\n",
    "$$p(y|x) = \\sigma(yf(x)) = \\frac{1}{1+exp(-yf(x))}$$\n",
    "\n",
    "La fonction $f$ finalement retenue est celle maximisant la vraisemblance. Or la *logistic loss* correspond aussi à la *negative conditionnal likelihood* faisant que maximiser la vraisemblance revient à minimiser la logistic loss (à vérifier). La *logistic loss* s'écrit donc:\n",
    "\n",
    "$$\\mathcal{L}(f(x),y) = -ln(p(y|x) = ln(1+exp(-yf(x)))$$\n",
    "\n",
    "## SVM *losses*\n",
    "Appelées *hinge loss* et *squared hinge loss* ou 1-SVM *loss* et 2-SVM *loss* respectivement. \n",
    "\n",
    "* *Hinge loss*: $max(1-u, 0)$\n",
    "* *Squared hinge loss*: $max(1-u, 0)^2$\n",
    "\n",
    "Avec $u$ la marge\n",
    "\n",
    "Fonctions convexes qui forcent les points à s'éloigner de la frontière (située pour une marge égale à 0 mais la loss n'est nulle qu'à partir d'une marge de 1). Non différentiables: problème contourné en introduisant des slack variables. \n",
    "\n",
    "## Boosting\n",
    "Adaboost (?) : $exp(-u)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
