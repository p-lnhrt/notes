{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Docker a lightweight virtual machine ?\n",
    "Docker et plus largement les conteneurs, Docker n'étant pas la seule solution disponible sur le marché, sont souvent rapidement vulgarisés comme des machines virtuelle (VM) *light*. En première approche et sans y regarder de près, un conteneur ressemble beaucoup et se fait surtout sentir à l'utilisateur comme s'il travaillait avec une VM: on a l'impression d'avoir son propre *process space*, sa propre interface réseau, on peut s'amuser à changer les tables de routage, lancer des services, exécuter des instructions en *root*, etc.. Cependant ce n'est pas vrai: un conteneur n'est pas une VM et trop se baser sur cette confortable comparaison peut mener à formuler des représentations et des hypothèses fausses.\n",
    "\n",
    "Les conteneurs diffèrent des VM dans le sens où ils se partagent le même *host kernel* (et donc contrairement aux VM, ne peuvent *booter* différents *kernels*/OS). Un conteneur n'est qu'un groupe de processus isolés, preuve en est que faire un `ps` sur l'*host* nous permet de voir l'ensemble des processus tournant dans des conteneurs là où une VM aurait été opaque. La principale différence avec une VM est que la VM garantit une forme d'isolation. \n",
    "\n",
    " computer program running on an ordinary operating system can see all resources (connected devices, files and folders, network shares, CPU power, quantifiable hardware capabilities) of that computer. However, programs running inside of a container can only see the container's contents and devices assigned to the container.\n",
    "\n",
    "Un conteneur n'est qu'un groupe de processus isolés (*isolated processes*) tournant sur notre OS. Du point de vue de l'OS, il n'y a aucune différence entre ces processus particuliers et les autres du fait qu'ils se partagent le même *kernel*. La technologie des conteneurs s'appuie sur différentes *features* existantes du *kernel* afin de créer ces *isolated runtime environment* (et des contributions au *kernel* poursuivent et approfondissent le développement de ces *features* afin de perfectionner la technologie des conteneurs). Les solutions tels que Docker ne font qu'utiliser ces *kernel features* (car le *kernel* Linux ne permet pas aujourd'hui de créer directement de tels environnement ou l'utilisation d'outils comme LXC est trop lourde), leur principale fonction étant de faciliter la création et la gestion de tels *isolated runtime environment* (conteneurs).\n",
    "\n",
    "L'utilisation de tels *isolated runtime environment* est aussi plus largement désignée sous le terme *OS-level virtualization* ces environnements étant qualifiés différemment suivant les solutions (*containers* pour Docker, *zones* chez Solaris, *virtual private servers* chez OpenVZ, etc.) qui d'ailleurs peuvent présenter quelques écarts du points de vue des *features* recherchées quand on fait de l'*OS-level virtualization*. Par souci de clarté, on désigne à partir de maintenant par conteneur ces *isolated runtime environments*.\n",
    "\n",
    "La virtualisation est donc une solution à la recherche de l'isolation. L'isolation est recherchée suivant plusieurs dimensions:\n",
    "* Dépendances, notamment si différents *softwares* se reposent sur plusieurs versions d'une même dépendance, on veut s'économiser la gestion des potentiels conflits.\n",
    "* Sécurité.\n",
    "* Accès aux ressources hardware.\n",
    "\n",
    "Pour faire simple, un conteneur se base sur deux principales *features* du *kernel* Linux introduite et développées depuis le début des années 2000: les *namespaces* et les *cgroups* (*control groups*). *Namespaces* et *cgroups* sont attachés à un *process* et pour faire simple, les premiers disent ce que le *process* voit des ressources système et les second disent à quelles resources le *process* a accès. Un conteneur n'est donc de point de vue qu'un *process* (ou groupe de *processes* Linux) auquels sont attachés des *namespaces* et des *cgroups*, c'est à dire un ensemble de permissions.\n",
    "\n",
    "\n",
    "* Les *cgroups* permettent d'implémenter du *metering* et de limiter les ressources utilisées par un *process* (CPU, RAM, I/O (*block* I/O et *network* I/O), *devices*, etc.). Concrètement l'allocation des ressources pour chaque sous-système (CPU, RAM, etc.) est réglée par une hiérarchie décrite par une arborescence dans laquelle chaque *process* appartient à un noeud. Les *processes* d'un même noeud se partagent les mêmes ressources, les *child process* étant placés dans le même groupe que leur parent. Les hiérarchies des différents sous sytèmes sont indépendantes et peuvent être différentes. A l'extrême quand aucune hiérarchisation des ressources n'est faite, notre arborescence ne comporte qu'un unique noeud rassemblant tous les *processes* qui ont en théorie tous accès à l'ensemble des ressources de la machine. \n",
    "* Les *namespaces* donnent à un *process* sa vue du système et donc permet de limiter ce qu'il peut voir. De même que pour les *cgroups*, il existe un *namespace* par aspect système (`IPC`, `Network`, `Mount`, `PID`, `User`, `UTS`, `Cgroup`) et un même *process* est représenté dans chacun d'entre eux. Quand le dernier *process* d'un *namespace exits*, le *namespace* est détruit. Par exemple:\n",
    "    * Un *process* d'un *PID namespace* ne voir que les *processes* du même *PID namespace*. Chaque *PID namespace* a sa propre numérotation commençant à 1, le PID 1 jouant un rôle particulier car s'il se termine, l'ensemble du *namespace* est *killé*. Un *process* peut finir avec de multiples PID: une par *namespace* dans lesquels il se trouve. Par exemple dans le cas d'un conteneur, le PID 1 dans le *namespace* du conteneur sera le PID 2328 dans le *namespace* de l'*host*, à l'extérieur du conteneur. Ces *namespaces* pouvant être emboîtés (*nested*) cela peut aller plus loin (si on s'amuse à faire un conteneur dans un conteneur, etc.).\n",
    "    * Tous les *processes* d'un même *network namespace* ont une même *network stack* \"privée\" (*network interfaces*, *routing tables*, `iptables` *rules*, *sockets*).\n",
    "    * Le *mount namespace* est globalement ce qui permet aux *process* d'un même *namespace* d'avoir leur propre *root filesystem* et se rapproche conceptuellement de ce que fait `chroot`.\n",
    "    * Le *user namespace* est globalement ce qui permet d'être le *root user* (*user 0*) dans un conteneur mais de ne pas avoir les mêmes privilèges en dehors: on est *user 0* (*root user*) dans le conteneur mais *user* différent de 0 vu de l*host*, en dehors du conteneur. \n",
    "* *Copy-on-write storage*: Cette *feature* est ce qui permet aux conteneurs de démarrer aussi vite et de minimiser leur empreinte (*footprint*) sur le système: tous les fichiers dont ce dernier a besoin ne sont pas dupliqués, il n'y a copie uniquement s'il y a modification (*copy on write*). Il existe différentes solutions permettant d'implémenter ce comportement dans la gestion des fichier (désignées sous le nom de *storage drivers*), on peut citer AUFS, overlayFS, BTRFS, ZFS, etc. Pour une illustration, voir \n",
    "* Concernant la sécurité et si on souhaite que les conteneur soient \"vraiment\" étanche, on s'appuie sur d'autres technologies: voir SELinux et AppArmor notamment.\n",
    "\n",
    "Voir notamment [cette vidéo](https://www.youtube.com/watch?v=0Xb421-9CTo) pour des explications claires et des exemples sur les *namespaces* et *cgroups*. La [seconde vidéo de la série](https://www.youtube.com/watch?v=3BkCaBxq5Ag) aborde notamment les questions de *copy-on-write* et de *storage driver*.\n",
    "\n",
    "Remarque: La commande `chroot` change le *root directory* apparent pour le *process* courant et tous ses *child processes*.\n",
    "\n",
    "Même si beaucoup de solutions (*runtimes*) de conteneurs se reposent sur les *namespaces* et les *cgroups*, ce n'est pas le cas de toutes. Parmi celle se reposant dessus, on peut citer:\n",
    "* LXC qui est historiquement la première mais pas facile d'utilisation notamment pour les développeurs.\n",
    "* Le Docker Engine qui historiquement se reposait sur LXC mais qui maintenant repose sur sa propre implémentation (`libcontainer`).\n",
    "* systemd-nspawn\n",
    "* *rkt* (*rocket*) et *runC* qui se focalisent uniquement sur l'exécution de conteneurs. Comparativement à Docker: pas d'API, pas de gestion des images, pas de capacité de build des images.\n",
    "\n",
    "Globalement, comme les différentes solutions mentionnées reposent sur les mêmes *features* du *kernel*, leurs performances sont équivalentes. Ce qui les distingue et potentiellement les destine à des utilisateurs différents sont leur design, les *features* proposées et leur écosystème.\n",
    "\n",
    "Ne se reposant pas sur les *namespaces* et les *cgroups* on peut citer l'ancien et éprouvé OpenVZ et en dehors du monde Linux, on peut citer les Zones sur Solaris et les Jails sur BSD.\n",
    "\n",
    "VM\n",
    "Permettent une parfaite isolation: les processes ne sont pas sur le même OS et des technologies permettent également une isolation des ressources (ex: VT-X pour les CPUs). \n",
    "Lentes à démarrer ? Pas forcément, mais au fond tout dépend de l'usage: si le temps de démarrage est finalement négligeable, il n'est pas un problème.\n",
    "Conteneurs et isolation des dépendences: On peut nuancer, suivant le niveau ce n'est pas forcément au conteneur de gérer nos dépendances: \n",
    "* Si on a des dépendances à des librairies, on gère ça avec le *build system*.\n",
    "* Si on a des dépendances à des binaires (déjà plus rare), on gère ça avec un conteneur.\n",
    "* Si on a des dépendances au *kernel*, on gère ça avec une VM. \n",
    "\n",
    "Set up VM \n",
    "On a un hyperviseur qui joue le rôle d'*abstraction layer* pour le *hardware* (hyperviseur de Type I) et plusieurs *kernels* différents pouvant ensuite être lancé et gérés par cet hyperviseur. Chacun de ces *kernels* correspond à une VM. Dans le cas des conteneurs et plus spécifiquement de Docker, l'ensemble des conteneurs tournent sur et partagent donc le même *kernel* qui assume le rôle d'*abstraction layer* pour le *hardware*. Ce rôle n'est pas exemple pas assumé par le Docker *deamon* qui tourne sur le *kernel* et dont le rôle est simplement de créer, *monitorer* et gérer des conteneurs en s'appuyant sur des *kernel features*. Le *deamon* ne joue donc pas l'analogue d'un hyperviseur mais est plus une *container management interface*.\n",
    "\n",
    "Remarques: \n",
    "* C'est du fait que différents conteneurs tournent sur le même OS/*kernel* que ces derniers sont effectivement plus léger qu'une même installation utilisant des VM.\n",
    "\n",
    "## Qu'est ce qui justifie d'avoir encore des VM ? Quels cas d'usage ?\n",
    "\n",
    "\n",
    "Volumes: \n",
    "* Permettent de découpler data et conteneurs\n",
    "* Permettent de partager des données entre conteneurs\n",
    "\n",
    "## Notion de montage d'un périphérique de stockage de données\n",
    "Dans les systèmes Windows, les partitions de disque dur ainsi que tous les autres périphériques de stokage de données (clé USB, disque réseau, CD, disquette) sont affichés comme des lecteurs indépendants en haut de leur propre arborescence\n",
    "\n",
    "Si l'on considère par exemple un système comprenant une partition de disque dur où est installé le système (Windows ou Unix), une partition de disque dur où se trouvent les données des utilisateurs et une clé USB, sous Windows, on accédera à ces données de manière séparée via :\n",
    "* Lecteur `C:` pour la partition système du disque dur (`C:\\path\\to\\file`),\n",
    "* Lecteur `D:` pour la partition utilisateur du disque dur,\n",
    "* Lecteur `X:` (ou autre lettre) pour les données stockées sur clé USB (`X:\\usb-dir\\usb-sub-dir\\file`).\n",
    "\n",
    "Pour les systèmes Unix, toutes ces sources de données sont incluse dans l'arborescence qui ne possède qu'une seule racine (`\\`), racine à partir de laquelle on accède à n'importe quel fichier. L'opération consistant à \"brancher\", à attacher un périphérique de stockage de données à l'arborescence s'appelle \"montage\" (*mount*). Dans le cas des systèmes Unix, le point de l'arborescence auquel on accroche celle du périphérique est appelé point de montage (*mount point*). Lorsque ses données sont accessibles depuis un point de montage, le périphérique est dit monté (*mounted*). Dans la plupart des systèmes Unix, les point de montage classiques sont `/mtn` pour les systèmes de fichiers temporaires (*temporary filesystems*) et `/media` pour les médias amovibles (*removable medias*) comme les CD-ROM ou les clés USB. Un point de montage est ainsi un répertoire à partir duquel des données se trouvant sous la forme d'un *filesystem* et physiquement stockées sur un périphérique de stockage sont accessibles. Pour reprendre l'exemple donné ci-dessus dans le cas d'un système Unix, l'accès se fera via: \n",
    "* `/` pour la partition système du disque dur\n",
    "* `/home` pour la partition utilisateur du disque dur\n",
    "* `/media/usb-drive` (ou autre nom) pour les données stockées sur clé USB (`/media/usb-drive/usb-dir/usb-sub-dir/file`)\n",
    "\n",
    "Dans les systèmes Unix, le *root directory* `/` est le *top directory* et le *root filesystem* repose sur la partition de disque où se trouve le *root directory*. Tous les autres *filesystems* sont ensuite montés sur ce *root filesystem*.\n",
    "\n",
    "L'opération de montage se fait à l'aide de la commande `mount`. Par exemple `mount /dev/sdc1 /media/usb-drive/` va monter la partition USB (*mount into*) `/dev/sdc1` en `/media/usb-drive` (le point de montage). On voit apparaître une autre spécificité de Linux: les périphériques sont accessible dans l'arborescence via un fichier spécial (*special file* ou ici *device file*) dans le répertoire `/dev` (*devices* - périphériques). Ces fichiers spéciaux sont des abstractions utiles pour interagir avec ces objets avec la ligne de commande (entre autres).\n",
    "\n",
    "Remarque: Voir le [Filesystem Hierarchy Standard (FHS)](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard) pour plus d'informations sur la structure standard de l'arborescence Linux.\n",
    "\n",
    "Docker fait apparaître des notions de montage pour la persistence des données. L'idée est de monter la partition correspondant à une partie de l'arborescence de l'hôte sur l'arborescence du conteneur: on rend ainsi accessible une partie du *filesystem* de l'hôte à partir d'un répertoire du conteneur. Par exemple, dans `docker container run -d ---mount \"src=<volume-name>,dst=/app\" <image>`, le *mount point* du volume `<volume-name>` (`/var/lib/docker/volumes/<volume-name>/_data`, cf. `docker volume inspect`) est monté sur le répertoire `/app` du conteneur. Les données du `/var/lib/docker/volumes/<volume-name>/_data` de l'hôte sont accessibles depuis le conteneur et réciproquement, les données du `/app` du conteneur sont donc accessibles depuis l'extérieur du conteneur.\n",
    "\n",
    "Remarque: Voir [documentation](https://docs.docker.com/storage/volumes/) pour les différences de syntaxe entre les arguments `--mount` et `--volume`.\n",
    "\n",
    "L'opération de montage n'est pas spécifique à Linux puisqu'elle désigne le fait plus général de rendre accessible un périphérique ou une partition de disque dur à partir d'un répertoire et s'applique ainsi aussi à Windows. L'opération inverse s'appelle le démontage (*unmounting*) et ne peut être réalisée si aucun fichier du périphérique ou de la partition à démonter n'est en train d'être lu ou écrit et si aucun processus utilise un répertoire du périphérique comme *working directory*. Ces opérations de démontage on par exemple lieu à chaque arrêt normal de l'OS. De même, \"retirer le périphérique en toute sécurité\" revient à demander le démontage de celui-ci. Cette opération est recommandée car suivant le type de périphérique et de *file system* utilisé par celui-ci, les conséquences d'une coupure de l'accès au périphériques avant la fin du démontage peuvent être graves. \n",
    "\n",
    "Docker\n",
    "https://blog.risingstack.com/operating-system-containers-vs-application-containers/\n",
    "http://file.allitebooks.com/20151213/Using%20Docker.pdf\n",
    "http://file.allitebooks.com/20170825/Docker%20for%20Data%20Science.pdf\n",
    "http://file.allitebooks.com/20160607/Docker%20in%20Practice.pdf\n",
    "\n",
    "IaaS CaaS PaaS\n",
    "https://www.youtube.com/watch?v=XcHE5V82OxM&t=2267s\n",
    "https://medium.com/@nnilesh7756/what-are-cloud-computing-services-iaas-caas-paas-faas-saas-ac0f6022d36e\n",
    "\n",
    "CI/CD\n",
    "http://file.allitebooks.com/20181229/Python%20Continuous%20Integration%20and%20Delivery.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
