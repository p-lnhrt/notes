{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Frontslide](./png/frontslide.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Un risque psycho-social: le *bore-out*\n",
    "* *Bore-out* ou \"syndrôme d'épuisement professionnel par l'ennui\" (Werder et Rothlin, 2007)\n",
    "* Origines multiples: sur-qualification, mise au placard, monotonie des tâches confiées, manque de défis, de stimulation, absence de perspectives d'évolution, etc.\n",
    "* Premiers symptômes: insatisfaction, démotivation, frustration, anxiété, tristesse:\n",
    "    * Naissent d'une incapacité à utiliser leurs compétences et connaissances ou à pouvoir faire des efforts reconnus.\n",
    "    * A plus long-terme: culpabilisation, dévalorisation de soi, dépression, épuisement, etc.\n",
    "* En sortir: être soutenu, se soigner, parler, partir.\n",
    "* Première condamnation en France: Desnard vs. Interparfums:\n",
    "    * L'arrêt rendu admet que les faits relatifs au *bore-out* sont constitutifs d'un harcèlement moral.\n",
    "    * Condamnation aux prud'hommes en 2018, confirmée en appel en 2020.\n",
    "    * La justice a prononcé la nullité du licenciement et a reconnu un harcèlement moral par \"mise à l'écart\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Un risque psycho-social: souffrance psy au travail\n",
    "\n",
    "Si vous ne parlez pas de votre malaise, personne ne saura que vous souffrez d'ennui. Grand tabou, honte, le salarié qui s'ennuie est un parresseux. Options: l'analyse de la situation professionnelle, la recherche d'une solution au sein de l'entreprise, et, en dernier lieu, la recherche d'un autre emploi.\n",
    " \n",
    " a Cour d'appel de Paris a rendu un arrêt inédit dans lequel elle admet que les faits relatifs au bore-out d'un salarié sont constitutifs d'un harcèlement moral (2). Elle condamne alors pour la première fois l'ennui au travail comme constitutif de harcèlement moral.\n",
    "\n",
    "D'après la Cour d'appel, le manque d'activité et l'ennui du salarié sont confirmés dans cette affaire, les agissements répétés de l'employeur ont dégradé ses conditions de travail et de santé. Les conditions de travail du salarié (ennui et manque d'activité) sont en lien avec son état de santé dégradé (dépression, crise d'épilepsie). L'employeur n'a pas réussi à prouver que ces éléments n'étaient pas constitutifs de harcèlement moral. Il est donc condamné à verser des dommages-intérêts au salarié."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Frontslide](./png/frontslide.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Apache Spark ?\n",
    "Apache Spark is an:\n",
    "* open-source \n",
    "* in-memory \n",
    "* general-purpose \n",
    "* distributed-computing framework "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Voila. Pour l'instant ce n'est pas très parlant et je ne vais pas expliquer cette slide. L'idée de cette présentation est bien sûr de vous donner une bonne représentation du fonctionnement de Spark autant qu'il est possible en si peu de temps, des problématiques et points de vigilance attachés au calcul distribué. Ne retenir que cela vous permettra j'espère de vous approprier rapidement l'outil et d'entrevoir plus facilement des optimisations et les sources des problèmes que vous rencontrerez. Mais aussi et j'y tiens, je souhaite que vous comprenniez ce qui a constitué l'apport de Spark et donc comprendre ce qui a fait qu'il est devenu un des outils incontournables de l'écosystème Hadoop et au delà. C'est pour cela que je vais commencer par une assez longue introduction sur ce qu'il y avait avant Spark qui vous permettra peut être de rafraichir ou de compléter votre connaissance d'Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Presentation outline\n",
    "1. Introduction: From Hadoop MapReduce to Spark\n",
    "2. Apache Spark: A general presentation\n",
    "3. What is the architecture of a Spark application ?\n",
    "4. Introducing Spark main features with SparkSQL\n",
    "5. Working with Spark: useful things you should know about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Introduction: From Hadoop MapReduce to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "L'architecture d'une application Spark une réponse à sa façon d'aborder le calcul distribué. On va \n",
    "commencer par rappeler en quoi consiste le paradigme dominant utilisé pour l'implémentation d'opérations\n",
    "de calcul distribué: MapReduce.\n",
    "\n",
    "Après en avoir présenté le principe général de fonctionnement, nous survolerons son implémentation la plus \n",
    "connue: Hadoop MapReduce, ce qui nous permettra d'introduire Hadoop, principal framework de calcul et de \n",
    "stockage distribué sur lequel s'exécutent des applications Spark.\n",
    "\n",
    "On verra alors apparaître en quoi Spark constitue une évolution de MapReduce tout en venant palier à un \n",
    "certain nombre de ses défauts. On commencera alors à voir apparaître les principales caractéristiques de\n",
    "Spark sur lesquelles s'est fondé son succès. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.1 Distributing computations: MapReduce\n",
    "\n",
    "* MapReduce is a distributed computing design pattern used to build scalable and fault-tolerant implementations of a given parallelizable algorithm or problem:\n",
    "    * Scales linearly: if more data, just throw in more machines to keep the same execution time,\n",
    "    * Gracefully handles partial failure.\n",
    "* The model is a specialization of the split-apply-combine strategy for data analysis.\n",
    "* 3 phases: map, shuffle & sort, reduce.\n",
    "* 2 key components: mappers + reducers.\n",
    "* Originates from *MapReduce: Simplified Data Processing on Large Clusters* (Google, 2004)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "L'idée du map-reduce est la suivante: On va partitionner et répartir (mapper) les données d'entrée entre différents éléments du programme appelés mappers. Ces mappers vont calculer en parallèle des résultats intermédiaires pour chaque fragment des données d'entrée appelée partition. Ces résultats intermédiaires sont identifiés par une clé. Dans un second temps, les résultats intermédiares produits par les mappers sont envoyés à d'autres éléments du programme en charge de combiner (reduce) les résultats intermédiaires de même clé pour obtenir le résultat final de l'opération. Ces éléments sont appelés des reducers et l'opération de rerépartition des résultats intermédiaires est appelé un shuffle. \n",
    "\n",
    "L'idée derrière map-reduce est donc assez simple. Sans doute des gens y on-t-ils pensé avant mais la gloire de map-reduce commence avec un papier de Google de 2004: MapReduce: Simplified Data Processing on Large Clusters dans lequel l'entreprise présente ce paradigme de calcul développé pour leurs besoins internes. \n",
    "\n",
    "On précise que toutes les opérations ou algoritmes ne se coulent pas avec la même facilité dans ce pattern.\n",
    "\n",
    "Pour expliquer tout en illustrant l'idée du map-reduce, je vais m'appuyer sur un exemple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.1 Distributing computations: MapReduce (Map phase)\n",
    "![Map-1](./png/mapreduce-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.1 Distributing computations: MapReduce (Map phase)\n",
    "![Map-2](./png/mapreduce-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.1 Distributing computations: MapReduce (Map phase)\n",
    "![Map-3](./png/mapreduce-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.1 Distributing computations: MapReduce (Map phase)\n",
    "![Map-4](./png/mapreduce-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.1 Distributing computations: MapReduce (Shuffle & sort phase)\n",
    "![Shuffle-1](./png/mapreduce-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.1 Distributing computations: MapReduce (Shuffle & sort phase)\n",
    "![Shuffle-2](./png/mapreduce-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.1 Distributing computations: MapReduce (Shuffle & sort phase)\n",
    "![Shuffle-3](./png/mapreduce-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.1 Distributing computations: MapReduce (Shuffle & sort phase)\n",
    "![Shuffle-4](./png/mapreduce-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.1 Distributing computations: MapReduce (Shuffle & sort phase)\n",
    "![Shuffle-5](./png/mapreduce-9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.1 Distributing computations: MapReduce (Shuffle & sort phase)\n",
    "![Shuffle-6](./png/mapreduce-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.1 Distributing computations: MapReduce (Shuffle & sort phase)\n",
    "![Shuffle-7](./png/mapreduce-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.1 Distributing computations: MapReduce (Reduce phase)\n",
    "![Reduce-1](./png/mapreduce-12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.1 Distributing computations: MapReduce (Reduce phase)\n",
    "![Reduce-2](./png/mapreduce-13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.1 Distributing computations: MapReduce (Reduce phase)\n",
    "![Reduce-3](./png/mapreduce-14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.2 Apache Hadoop\n",
    "\n",
    "* Hadoop's origins are rooted into two fundamental papers:\n",
    "    * The Google File System (Google, 2003)\n",
    "    * MapReduce: Simplified Data Processing on Large Clusters (Google, 2004)\n",
    "\n",
    "* on which are based Hadoop's two fundamental services:\n",
    "    * A distributed filesystem: Hadoop Distributed File System (HDFS)\n",
    "    * A distributed computing service: Hadoop MapReduce\n",
    "    \n",
    "* Hadoop 2.0 (2013) introduced the third and last key component of Hadoop: a cluster resource manager called YARN (Yet Another Resource Negociator):\n",
    "    * YARN is the product of a deep refactoring of MapReduce which split resource management (YARN) and job monitoring (delegated to dedicated applications called Application Masters)\n",
    "    * Opened the door for the execution of non-MapReduce jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Le papier MapReduce est avec un autre papier de Google de 2003 intitulé The Google File System à l'origine du projet Hadoop dont la première release (0.1.0) date de la même époque, avril 2006. Hadoop consiste en deux principaux volets, en deux couches: un volet stockage distribué pris en charge par HDFS et un volet calcul distribué correspondant à l'implémentation de map-reduce fournie par Hadoop appelée...Hadoop MapReduce. \n",
    "\n",
    "Côté calcul distribué, Hadoop MapReduce est un framework: il donne une interface que le développeur doit implémenter pour que son application puisse être exécutée par MapReduce (et donc de façon distribuée) sur des données stockées sur HDFS. Le programmeur n'a pas à se soucier de la logistique propre au calcul distribué, ce n'est pas à lui de gérer, de fournir une implémentations des transferts de données, les shuffles, etc. Tout cela est géré par Hadoop MapReduce, lui n'a qu'à fournir le code des mappers et celui des reducers en respectant l'interface demandée, l'ensemble constituant une application MapReduce. \n",
    "\n",
    "L'idée d'Hadoop de proposer à la fois un système de stockage et de calcul visant notamment à permettre à chaque mapper de s'exécuter sur la machine ou le plus près possible de la machine sur laquelle ses données ont été stockées par HDFS afin de permettre d'économiser autant que possible les importants coûts réseau associés au  transfert de données de leur stockage vers leur lieu de traitement (data locality). Une autre des forces d'Hadoop et qui a largement contribué à son succès est que les machines physiques utilisées pour construire le cluster peuvent être globalement n'importe quel serveur du commerce (commodity hardware) par opposition aux très couteux calculateurs customs utilisés jusqu'alors pour le traitement de gros volumes de données. \n",
    "\n",
    "Hadoop est donc la somme de HDFS et de MapReduce. C'était vrai jusqu'à Hadoop 2 released en mars 2012 (stable octobre 2013) (Hadoop 1 ayant été released en décembre 2011) qui introduit un troisième composant: YARN. YARN est le cluster resources manager d'Hadoop (appelé aussi cluster manager ou resource manager). YARN est en charge du monitoring des ressources du cluster. C'est à YARN qu'on soumet un job, en fonction des resources disponibles sur le cluster (information qui lui est remontée par ces process appelés Node managers), YARN le lance ou pas. Hadoop aujourd'hui (Hadoop 3 a été released en décembre 2017 n'a rien changé de ce point de vue) c'est donc HDFS + MapReduce + YARN. YARN est en fait le produit d'un refactoring de MapReduce où on été séparés management des ressources (géré par YARN) et monotoring des jobs (refactorisé dans un application master, un par application, une application consistant en un job ou un DAG de jobs). On ne s'étend pas là dessus. C'est une étape importante car à partir d'Hadoop 2, les jobs pouvant s'exécuter sur un cluster Hadoop ne sont plus uniquement des jobs MapReduce. On peut y exécuter n'importe quel job, YARN jouant le rôle d'orchestrateur qui n'existant avant pas (gestion des ressources et scheduling des jobs MapReduce étaient alors confondus).\n",
    "\n",
    "Noter que Hadoop est codé en Java, et il est requit des jobs MapReduce d'être écrits en Java. Une de ses limitations.\n",
    "\n",
    "Timeline:\n",
    "* First release: April 2006\n",
    "* Hadoop 1.0: December 2011\n",
    "* Hadoop 2.0: October 2013\n",
    "* Hadoop 3.0: December 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.2 Apache Hadoop: Submitting a YARN job\n",
    "![YARN-job-1](./png/submit-yarn-job-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Je vais à présent vous présenter le déroulement de la soumission et de l'exécution d'un job (pas forcément MapReduce dont je sais qu'il est obsolète) sur YARN qui va nous permettre de voir apparaître une structure qu'on retrouvera pour les applications Spark. Chaque entité s'exécute dans un conteneur YARN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.2 Apache Hadoop: Submitting a YARN job\n",
    "![YARN-job-2](./png/submit-yarn-job-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.2 Apache Hadoop: Submitting a YARN job\n",
    "![YARN-job-3](./png/submit-yarn-job-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.2 Apache Hadoop: Submitting a YARN job\n",
    "![YARN-job-4](./png/submit-yarn-job-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "L'exécution d'un job se décompose en l'exécution distribuée de différentes tasks. L'exécution distribuée sous-entend que différents programmes concourant à l'exécution d'un même job vont en traiter les\n",
    "taches sur les différentes machines constituant le cluster. Cela nécessite de la coordination, toutes les entités du job tournant sur le cluster pour exécuter le programme ne sont dont pas toutes dédiées à \n",
    "l'exécution des tâches. On a une structure hiérarchique avec un application master en charge de la coordination des taches, de leur suivi, du redémarrage des taches plantées et de la négociation des \n",
    "ressources avec YARN et les exécuteurs qui ne font qu'exécuter les taches que leur a affecté l'application master. Il y a un application master par job YARN.\n",
    "\n",
    "Remarque: Conteneur YARN: on le trouve parfois entre guillemets. Il semble que la notion de conteneur dans YARN semble se limiter du point de vue de l'isolation des ressources aux CPU et à la RAM, dont l'utilisation est surveillée par le NodeManager qui killera l'application si elle dépasse. Il semble quand même y avoir aussi une namespace isolation qui permet d'isoler ce que l'application voit de son environnement d'exécution (notamment les filesystem). L'isolation d'un conteneur YARN ne semble donc pas complète d'où les guillemets. La technologie de conteneur utilisée n'est pas très clair, je penche sur une implémentation maison pour isoler que ce qui avait été jugé nécessaire d'isoler. Depuis Hadoop 3.1, Docker est supporté ce qui permettra une bien meilleure utilisation de Python par exemple. De ce fait YARN se rapproche davantage d'un véritable orchestrateur de conteneurs comme l'est Kubernetes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.2 Apache Hadoop: Submitting a YARN job\n",
    "![YARN-job-5](./png/submit-yarn-job-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.2 Apache Hadoop: Submitting a YARN job\n",
    "![YARN-job-6](./png/submit-yarn-job-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.2 Apache Hadoop: Submitting a YARN job\n",
    "![YARN-job-7](./png/submit-yarn-job-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.2 Apache Hadoop: The Hadoop ecosystem\n",
    "![Hadoop-ecosystem-1](./png/ecosystem-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Retenir qu'on peut voir Hadoop comme un ensemble de librairies, un framework permettant d'implémenter des applications distribuées. Hadoop se charge de la tuyauterie et abstrait les détails et les problèmes propres au stockage et au calcul distribué (replication, fault-tolerance), le développeur n'a qu'à utiliser ses librairies et ses interfaces pour créer ses applications et tout se passera bien de la même manière que votre OS abstrait toute la complexité des plus bas niveaux de votre machine. C'est pour celà qu'on parle parfois  d'Hadoop comme d'un OS du distributed computing. \n",
    "\n",
    "Open-source, fonctionnant sur du commodity-hardware et fournissant un bon framework à la fois de calcul et de stockage distribué, Hadoop connu un grand succès et sa grande prévalence aujourd'hui est le produit de ce succès qui s'est certes pas mal émoussé depuis l'émergence d'ochestrateurs de conteneurs comme Kubernetes.\n",
    "\n",
    "Son succès et son adopption a donné lieu et a été amplifié par la création de différents outils aux visées très diverses basés sur Hadoop:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.2 Apache Hadoop: The Hadoop ecosystem\n",
    "![Hadoop-ecosystem-2](./png/ecosystem-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Différents systèmes de bases de données basés sur HDFS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.2 Apache Hadoop: The Hadoop ecosystem\n",
    "![Hadoop-ecosystem-3](./png/ecosystem-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Des outils d'ingestion de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.2 Apache Hadoop: The Hadoop ecosystem\n",
    "![Hadoop-ecosystem-4](./png/ecosystem-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Des outils de stream processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.2 Apache Hadoop: The Hadoop ecosystem\n",
    "![Hadoop-ecosystem-5](./png/ecosystem-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Différents data warehouse basés sur HDFS (SQL on-hadoop - MapReduce puis Spark ou autre in-memory processing tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.2 Apache Hadoop: The Hadoop ecosystem\n",
    "![Hadoop-ecosystem-6](./png/ecosystem-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Un language de scripting construit sur MapReduce "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.2 Apache Hadoop: The Hadoop ecosystem\n",
    "![Hadoop-ecosystem-7](./png/ecosystem-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Des moteurs de recherche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.2 Apache Hadoop: The Hadoop ecosystem\n",
    "![Hadoop-ecosystem-8](./png/ecosystem-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Des outils de scheduing (ordonnanceurs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.2 Apache Hadoop: The Hadoop ecosystem\n",
    "![Hadoop-ecosystem-9](./png/ecosystem-9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Des outils d'administration (monitoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.2 Apache Hadoop: The Hadoop ecosystem\n",
    "![Hadoop-ecosystem-10](./png/ecosystem-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Des outils de synchronisation pour l'accès à des ressources partagées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.2 Apache Hadoop: The Hadoop ecosystem\n",
    "![Hadoop-ecosystem-11](./png/ecosystem-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "La sécurité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.2 Apache Hadoop: The Hadoop ecosystem\n",
    "![Hadoop-ecosystem-12](./png/ecosystem-12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Outils d'analyse interactive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.2 Apache Hadoop: The Hadoop ecosystem\n",
    "![Hadoop-ecosystem-13](./png/ecosystem-13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Machine learning et calcul scientifique sur Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.2 Apache Hadoop: The Hadoop ecosystem\n",
    "![Hadoop-ecosystem-14](./png/ecosystem-14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Outils de calcul distribués in-memory\n",
    "\n",
    "L'ensemble venant former ce qu'on appelle l'écosystème Hadoop.\n",
    "\n",
    "Nous allons nous arrêter ici pour la présentation d'Hadoop et faire un point sur les faiblesses des jobs MapReduce notamment pour l'implémentation de pipelines de ML pour mieux comprendre l'apport et la spécificité de Spark:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.3 Hadoop MapReduce weaknesses\n",
    "\n",
    "* Rather heavy and rigid framework:\n",
    "    * The developper must provide a map-reduce implementation of even the simpliest tasks (alleviated by higher-level tools)\n",
    "    * Jobs must be written in Java (alleviated by higher-level tools)\n",
    "* Complex workflows require to chain dozens (or many more) of MapReduce jobs:\n",
    "    * Rather big job codebase with heavy maintenance\n",
    "    * Global job optimization is left to the developper\n",
    "    * Complex jobs are slow:\n",
    "        * Each job's partial result must be written to and then read from disk\n",
    "        * Very poor performance on iterative algorithms (ex: machine learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Nous allons nous arrêter ici pour la présentation d'Hadoop et faire un point sur les faiblesses des jobs MapReduce notamment pour l'implémentation de pipelines de ML pour mieux comprendre l'apport et la spécificité de Spark:\n",
    "- Le framework MapReduce est assez rigide: chaque opération même les plus simples doivent être décomposées et implémentées par le développeur en les deux étapes map et reduce. Même une opération très simple\n",
    "exige assez rapidement des dizaines de lignes de code (Java, désavantage pour la DS).   \n",
    "- Une chaine complexe de traitements demande d'être découpée sur plusieurs jobs MapReduce dont les résultats sont à chaque fois écrits sur disque pour être lus du même disque par le job suivant.\n",
    "- Assez inéfficace pour les algorithmes itératifs (où on fait plusieurs passes sur les mêmes données) qui sont fréquents dans le ML. Un algorithme itératif va prendre la forme d'une chaînes de jobs MapReduce\n",
    "dans laquelle au moins un job va s'exécuter plusieurs fois sur les mêmes données et recharger l'ensemble des données du disque à chaque fois."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.3 Hadoop MapReduce weaknesses: how does Apache Spark address these issues ?\n",
    "\n",
    "* Provides high-level abstractions for many basic operations that can be combined with few lines of code to build complex workflows (including using scripting languages): analysts can focus on their business problems rather than being absorbed by implementation details. Spark is easy to use.\n",
    "* Embed workflow optimizations which aim at minimizing data movements: Spark is fast.\n",
    "* Operations are carried out in-memory as much as possible: Spark is much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "La réponse de Spark est qu'il permet: \n",
    "- D'exécuter en un seul job une chaîne complexe d'opérations (pourvu que l'ensemble de celles-ci puissent se mettre sous la forme d'un DAG) et même l'optimise, ce qui était du ressort du dev côté MapReduce. \n",
    "- Abstrait la complexité du calcule distribué pour un grand nombre d'opérations de base et permet d'écrire des pipelines assez complexes en quelques lignes. On peut se focaliser sur le problème buisness, on est moins absorbé par l'implémentation\n",
    "- L'ensemble des calculs sont effectués in-memory (accès RAM au moins 1000x plus rapide que l'accès disque). Cela ne veut pas dire qu'il n'y a plus d'écritures disque. Dans le cas du ML, les données sont par exemple conservées si possible intégralement en RAM tout le long de l'exécution de l'algorithme.\n",
    "\n",
    "Reprenons donc la description de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Apache Spark: A general presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2.1 General project information\n",
    "* Started in 2009 at UC Berkeley AMPLab (Matei Zaharia PhD thesis).\n",
    "* Handed over to the Apache Software Foundation in 2013, Apache top-level project since 2014.\n",
    "* Very active community: 1000+ contributors, 30k+ JIRA tickets, 20k+ Github stars, Spark&AI summits.\n",
    "* Contributions from most of the top tech companies. Most contributions come from Databricks.\n",
    "* Written in Scala.\n",
    "* Spark 2.0 released in July 2016, Spark 3.0 released in September 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2.2 Apache Spark: 4 main libraries\n",
    "![spark-libraries](./png/libraries.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2.3 Apache Spark: 4 supported languages\n",
    "![spark-languages](./png/languages.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2.4 Apache Spark: 4 supported resource managers\n",
    "![spark-cluster-managers](./png/cluster-managers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2.5 A great variety of supported data sources\n",
    "* Storage: local FS, HDFS, cloud storage (AWS S3, Azure Storage, Google Cloud Storage, etc.)\n",
    "* Databases/data warehouses: Apache Hive, many available connectors (Cassandra, CouchBase, Neo4j, etc.) \n",
    "* Data streams: Apache Kafka, AWS Kinesis, etc.\n",
    "* File formats: CSV, JSON, Parquet, Avro, ORC, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2.6 What is Apache Spark ? (continued)\n",
    "\n",
    "* \"Apache Spark is an open-source in-memory general-purpose distributed-computing framework\"\n",
    "* \"Apache Spark is a unified engine for big data processing, data science, machine learning and data analytics workloads\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. What is the architecture of a Spark application ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.1 The Spark application: A two-level distributed architecture\n",
    "* A Spark application is a hierachical cluster of individual entities.\n",
    "* One program is called the **Spark driver** (one per application) which is in charge of translating the application's code into tasks, optimizing the tasks, allocating the tasks, monitoring their execution, relauching failed tasks.  \n",
    "* All the other entities are called **Spark executors** and their sole purpose is to execute the tasks the driver assigned to them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.2 Configuring a Spark application\n",
    "* The Spark driver and the Spark executors are JVMs.\n",
    "* When running on a cluster, Spark JVMs run in ad-hoc containers.\n",
    "\n",
    "The essential part of configuring a Spark application resides in choosing:\n",
    "* The number of executors.\n",
    "* The amount of CPU resources allocated to the driver and to each of the executors.\n",
    "* The amount of memory resources allocated to the driver and to each of the executors (JVM + overhead).\n",
    "* The deploy mode: does the driver run on (cluster mode) or outside (client mode) from the cluster ?\n",
    "\n",
    "**Notice:** Each JVM can take advantage of several cores. Each working thread is called a **Spark worker**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.3 How does a Spark application run on YARN (cluster mode) ?\n",
    "![yarn-cluster-1](./png/submit-yarn-cluster-job-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.3 How does a Spark application run on YARN (cluster mode) ?\n",
    "![yarn-cluster-2](./png/submit-yarn-cluster-job-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.3 How does a Spark application run on YARN (cluster mode) ?\n",
    "![yarn-cluster-3](./png/submit-yarn-cluster-job-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.3 How does a Spark application run on YARN (cluster mode) ?\n",
    "![yarn-cluster-4](./png/submit-yarn-cluster-job-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.3 How does a Spark application run on YARN (cluster mode) ?\n",
    "![yarn-cluster-5](./png/submit-yarn-cluster-job-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.3 How does a Spark application run on YARN (cluster mode) ?\n",
    "![yarn-cluster-6](./png/submit-yarn-cluster-job-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.3 How does a Spark application run on YARN (cluster mode) ?\n",
    "![yarn-cluster-7](./png/submit-yarn-cluster-job-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.3 How does a Spark application run on YARN (cluster mode) ?\n",
    "![yarn-cluster-8](./png/submit-yarn-cluster-job-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.3 How does a Spark application run on YARN (cluster mode) ?\n",
    "![yarn-cluster-9](./png/submit-yarn-cluster-job-9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.4 How does a Spark application run on YARN (client mode) ?\n",
    "![yarn-client-1](./png/submit-yarn-client-job-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.4 How does a Spark application run on YARN (client mode) ?\n",
    "![yarn-client-2](./png/submit-yarn-client-job-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.4 How does a Spark application run on YARN (client mode) ?\n",
    "![yarn-client-3](./png/submit-yarn-client-job-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.4 How does a Spark application run on YARN (client mode) ?\n",
    "![yarn-client-4](./png/submit-yarn-client-job-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.4 How does a Spark application run on YARN (client mode) ?\n",
    "![yarn-client-5](./png/submit-yarn-client-job-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.4 How does a Spark application run on YARN (client mode) ?\n",
    "![yarn-client-6](./png/submit-yarn-client-job-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.4 How does a Spark application run on YARN (client mode) ?\n",
    "![yarn-client-7](./png/submit-yarn-client-job-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.4 How does a Spark application run on YARN (client mode) ?\n",
    "![yarn-client-8](./png/submit-yarn-client-job-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.4 How does a Spark application run on YARN (client mode) ?\n",
    "![yarn-client-9](./png/submit-yarn-client-job-9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Introducing Spark main features with SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.1 Interacting with Spark: The Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark \n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .config('spark.app.name', 'mpdata-talk')\\\n",
    "    .config('spark.master', 'local[*]')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A Spark application can also be launched from the command line using the `spark-submit` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "La Spark session est le point d'entrée d'une application Spark: toute interaction avec l'application comme la soumission d'un job, la création d'un DataFrame Spark passe par la Spark session. La Spark session donne également accès aux informations de base relatives à l'application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.1 Interacting with Spark: The Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Noter que Spark met à la disposition de l'utilisateur une web UI: la Spark UI, lui permettant de monitorer l'exécution des jobs et d'obtenir des informations détaillées sur leur exécution. La Spark UI peut être notamment utile pour le débugage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.2 Spark's fundamental data structure: the Spark DataFrame\n",
    "* Spark data is represented using an abstraction called a DataFrame (Python API).\n",
    "* A Spark DataFrame object does not contain data: a Spark DataFrame reprensents a collection of operations to be performed on one or several data sources.\n",
    "* Spark DataFrames are built on a lower-level abstraction called Resilient Distributed Datasets (RDDs) which are no longer part of the public API since Spark 2.0 and should not be used directly.\n",
    "* Spark uses indeed **lazy evaluation**:\n",
    "    * Computations are triggered only by special methods called **actions**.\n",
    "    * Lazy evaluation allows to perform a global optimization of the operations to be performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.2 Spark's fundamental data structure: the Spark DataFrame\n",
    "* Spark distinguishes two types of Dataframe operations (methods):\n",
    "    * **Actions**: An action triggers the computation of the DataFrame on which they were called. Calling an action submits a job. Each Spark job has been launched by a call to an action. Actions: `show`, `collect`, `toPandas`, `count`, `take`, write operations.\n",
    "    * **Transformations**: A transformation simply returns a new DataFrame. Example: `select`, `filter`, `groupBy`, `join`, `withColumn`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.2 Spark's fundamental data structure: the Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "spark.read\\\n",
    "    .parquet('./data/flights.parquet')\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.2 Spark's fundamental data structure: the Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "spark.read\\\n",
    "    .parquet('./data/flights.parquet')\\\n",
    "    .select('DayofMonth', 'DayOfWeek', 'Carrier', 'DepDelay')  # Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Spark reader `spark.read.parquet` returns a `spark.sql.DataFrame` object.\n",
    "* The `select` method is a transformation and therefore returns a new `spark.sql.DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.2 Spark's fundamental data structure: the Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "spark.read\\\n",
    "    .parquet('./data/flights.parquet')\\\n",
    "    .select('DayofMonth', 'DayOfWeek', 'Carrier', 'DepDelay')\\\n",
    "    .show(5)  # Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The Spark reader `spark.read.parquet` returns a `spark.sql.DataFrame` object.\n",
    "* The `select` method is a transformation and therefore returns a new `spark.sql.DataFrame`.\n",
    "* The `show` method is an action and therefore triggers a Spark job and returns the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.3 Spark's fundamental data processing unit: the partition\n",
    "* Spark splits its data into chunks called **partitions**\n",
    "* All records within a given partition will be processed together by the same core (Spark worker) within the same Spark executor.\n",
    "* Data partitions are the smallest data processing unit: the processing of one data partition by a single Spark worker thread defines Spark's elemental processing unit: the **task**.   \n",
    "\n",
    "**Notices:**\n",
    "* A Spark executor (JVM) gathers one or more workers (threads): each worker run on its own core.\n",
    "* When asked to write its output, Spark creates one file per partition.\n",
    "\n",
    "**Warning: Row ordering \"does not make sense\" any longer with ditributed data: be careful with sort-based operations**     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.3 Spark's fundamental data processing unit: the partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "flight_data = spark.read\\\n",
    "    .parquet('./data/flights.parquet')\\\n",
    "    .persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "flight_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.4 Spark's Dataframe operations: narrow vs. wide\n",
    "* There are two types of DataFrame operations: narrow and wide operations.\n",
    "* Can the operation be performed independently on each partition ?\n",
    "    * **Yes**: the operation is a narrow operation: `select`, `filter`, simple map operations, etc.\n",
    "    * **No**: the operation requires exchange of information and is called a wide operation: `groupBy.agg`, `join`, `orderBy`, window functions, etc.\n",
    "* Wide operations imply suffling data with the associated impact on performance: wait for other executors, disk + network I/O.\n",
    "* **Never forget that operations are distributed: do not code with Spark like you would with `pandas`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.4 Spark's Dataframe operations: narrow vs. wide\n",
    "* Describing Spark computations:\n",
    "    * **Task**: Processing of one partition by one Spark worker (thread).\n",
    "    * **Stage**: Block of narrow operations. Stages are delimited by shuffles. Completing a stage required to complete as many tasks as partitions at the beginning of the stage.\n",
    "    * **Job**: Computation triggered by a Spark action. Can consist of one or several stages.\n",
    "    * **Session**: A session is tied to the application's life. Spark allows to run more than one job per session.\n",
    "\n",
    "**Notice:** The number of partitions is set when reading data and can change after a shuffle or at the user's command (`repartition`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.4 Spark's Dataframe operations: narrow vs. wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "spark.read\\\n",
    "    .parquet('./data/flights.parquet')\\\n",
    "    .select('DayofMonth', 'DayOfWeek', 'Carrier', 'DepDelay')\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.4 Spark's Dataframe operations: narrow vs. wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sqlf\n",
    "\n",
    "spark.read\\\n",
    "    .parquet('./data/flights.parquet')\\\n",
    "    .groupBy('Carrier', 'DepDelay')\\\n",
    "    .agg(sqlf.max('DepDelay').alias('max_dep_delay'))\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.5 Spark SQL query optimizer: Catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "airports = spark.read\\\n",
    "    .option('header', True)\\\n",
    "    .option('inferSchema', True)\\\n",
    "    .csv('./data/airports.csv')\\\n",
    "    .select('airport_id', 'state', sqlf.col('name').alias('airport_name'))\n",
    "\n",
    "airports.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.5 Spark SQL query optimizer: Catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "texas_delays = flight_data.join(airports, \n",
    "                                flight_data.OriginAirportID==airports.airport_id, \n",
    "                                how='left')\\\n",
    "    .groupBy('state', 'airport_name')\\\n",
    "    .agg(sqlf.max('DepDelay').alias('max_dep_delay'))\\\n",
    "    .filter(sqlf.col('state')=='TX')\n",
    "\n",
    "texas_delays.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.5 Spark SQL query optimizer: Catalyst\n",
    "![parsed-dag](./png/parsed-dag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.5 Spark SQL query optimizer: Catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "texas_delays.explain(extended=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.5 Spark SQL query optimizer: Catalyst\n",
    "* Spark lazy evaluation enable a global optimization of the computations to be performed.\n",
    "* Optimizations are performed on Spark DataFrames (not RDDs) by its embedded query optimizer: Catalyst\n",
    "* Catalyst was introduced in Spark 2.0 and is still a major source of new developments and performance gains.\n",
    "* Philosophy:\n",
    "    * Only load the data you need.\n",
    "    * Minimize costly data shuffles.\n",
    "* Operations to be performed are represented using a (directed acyclic) graph (DAG) which is used as input  by the optimizer.\n",
    "* Performance gains can be very substantial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.5 Spark SQL query optimizer: Catalyst\n",
    "![optimized-dag](./png/physical-dag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.5 Spark SQL query optimizer: Catalyst\n",
    "![catalyst](./png/catalyst.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "DAG visible dans la Spark UI: Selected physical plan\n",
    "\n",
    "Spark 3.x: Parmi les grandes principales nouveautés, 2 majeures concernent l'optimisation des jobs: \n",
    "* Adaptative Query Execution (AQE): Spark collecte des statistiques pendant l'exécution et adapte son logical plan en conséquence.\n",
    "* Dynamic partition pruning\n",
    "* Readability improvements on `explain` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.5 Spark SQL query optimizer: Catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "texas_delays.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "explain(True): All plans\n",
    "Spark 3.x: permet d'être plus fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. Working with Spark: useful things you should know about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Quelques recommendation même si c'est un peu décousu. C'est le problème avec une présentation aussi courte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5.1 Storing data on HDFS: Use the Apache Parquet format\n",
    "* Binary columnar encoded compressed format with embedded metadata (like schema).\n",
    "* Spark default file format, supported by many applications within the Hadoop ecosystem. \n",
    "* Spark Parquet reader leverages Parquet features for very high read performance: filter pushdown, partition pruning, metadata filtering.\n",
    "* Writing Parquet files:\n",
    "    * Remember that Spark creates one file per partition: beware of not writing many small files on HDFS (HDFS small files problem)\n",
    "    * Leverage features such as Parquet partitioning, bucketing and sorting for faster reads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "HDFS small file problem:\n",
    "* Le namenode garde toutes les métadonnées en mémoire, s'il y a beaucoup de fichiers donc de métadonnées se pose le problème du scaling du namenode, en tout cas il peut devenir un bottleneck\n",
    "* Beaucoup de métadonnées transmises à l'app master/driver. Par défaut une partition par block HDFS. Si énormément de petits fichiers: une task de lecture Spark par fichier et autant de partitions à gérer: lot of book-keeping overhead for the driver. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5.2 Caching\n",
    "* In complex workflows, appropriatly caching or checkpointing intermediate data avoids recomputing that data more than once.  \n",
    "* See the `cache`, `persist` DataFrame methods.\n",
    "* **Warning:** `cache` and `persist` are transformations and not actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5.3 Beware of unbalanced partitioning\n",
    "* If data in not evenly spread accross partitions (data skew), some tasks may take much longer than others forcing dependent subsequent tasks to wait.\n",
    "* Reminder: When is the data (re)partioned ?\n",
    "    * After reading.\n",
    "    * After shuffling.\n",
    "* Solution: the user forces a repartitioning using an appropriate partitioning key using the `repartition` method.\n",
    "* **Warning:**\n",
    "    * This will cost a full shuffle.\n",
    "    * Find a balance between too many partitions (book-keeping overhead) and too few partitions (executors will suffer or even crash).\n",
    "* Spark 3.0 Adaptative Query Execution introduces an automatic response to this problem.\n",
    "* **Warning** on `collect`/`toPandas`, `repartition(1)` and `coalesce(1)`: all data ends up on the same JVM and can kill it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5.4 Minimize wide operations\n",
    "* Distributed computing wide operations (aggregations, join, sort) have high performance costs due to shuffling (disk and network I/O).\n",
    "* Spark Catalyst optimizes as much as it can but:\n",
    "    * Do not use these operations (Ex: cosmetic `orderBy`) if you can find a workaround.\n",
    "    * Catalyst may miss possible optimizations (especially when using window functions) but is constantly improved.\n",
    "* **Notice**: Sorting the entire dataset has little meaning in distributed computing and can lead to missleading results and degrade performance (sorting requires to shuffle):\n",
    "    * They are very few situation outside from window functions and data writes where the user needs to call `orderBy` or any equivalent operation.\n",
    "    * Ex: First row of each group.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5.5 Sizing you application\n",
    "* People's default behavior when memory issues arise: \"If it does not work, just keep throwing more memory in\".\n",
    "* Instead, when your job keeps crashing for memory issues:\n",
    "    * Increase parallelism: more executors, more cores per executors (total number of cores = total number of Spark workers = total number of simultaneously executed tasks).\n",
    "    * **At the executor level: balance RAM and the number of cores**.\n",
    "    * Check your data partitioning: data skew ? too many or too few partitions: change the partitioning settings and/or adapt your code.\n",
    "* Key advice: Cache your data and go to the Spark UI to assess its size once in memory.\n",
    "* Key ratio: memory per core.\n",
    "* Application sizing advice:\n",
    "    * Data size => Partition count => Spark worker count (cores and executors).\n",
    "    * Spark worker count and partition size => Appropriate amount of memory.\n",
    "* Great variability in input data size: dynamic allocation of executors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5.5 Sizing you application: Spark memory layout\n",
    "![spark-memory](./png/spark-memory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5.6 Python on Spark (1/2)\n",
    "* Spark is written in Scala so where is my Python executed ?\n",
    "    * You Python application code is executed by a Python interpreter next to the **Spark driver**.\n",
    "    * Spark executors only run Python code when you resort to Spark Python user defined functions (UDFs).\n",
    "* On Python UDFs: \n",
    "    * Python UDFs can greatly hamper performance (serialization/deserialization costs), use them when you have no alternative.\n",
    "    * Major field of improvements since Spark 2.3 (`pandas` UDFs, `pyarrow`).\n",
    "* Which Python interpreter is being used? Set using `spark.pyspark.python`/`spark.pyspark.driver.python` or the corresponding `PYSPARK_PYTHON`/`PYSPARK_DRIVER_PYTHON` environment variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5.6 Python on Spark (2/2)\n",
    "* What about my dependencies? \n",
    "    * They need to be installed on each node of the cluster.\n",
    "    * Depending on your type of cluster, you can use Docker images to manage your Python environment.\n",
    "    * You can upload additional dependencies at application submission time using `spark.submit.pyFiles`/`--py-files` which are automatically added to `PYTHONPATH`.\n",
    "* Make sure to allocate enough memory for the Python interpreter especially in the driver's container if you collect all your data in a `pandas.DataFrame`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5.6 Python on Spark: Pyspark application (cluster mode)\n",
    "![pyspark-cluster](./png/pyspark-cluster.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5.6 Python on Spark: Pyspark application with Python UDFs (cluster mode)\n",
    "![pyspark-cluster-udf](./png/pyspark-cluster-udf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5.6 Python on Spark: Jupyter + Pyspark application (client mode)\n",
    "![pyspark-client-jupyter](./png/pyspark-client-jupyter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5.6 Python on Spark: Jupyter + Apache Livy\n",
    "![pyspark-client-livy](./png/pyspark-client-livy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "MapReduce revolutionized computation over\n",
    "huge data sets by offering a simple model for writing programs that could execute in\n",
    "parallel across hundreds to thousands of machines. The MapReduce engine achieves\n",
    "near linear scalability—as the data size increases, we can throw more computers at it\n",
    "and see jobs complete in the same amount of time—and is resilient to the fact that\n",
    "failures that occur rarely on a single machine occur all the time on clusters of thou‐\n",
    "sands of machines. It breaks up work into small tasks and can gracefully accommo‐\n",
    "date task failures without compromising the job to which they belong.\n",
    "\n",
    "Spark maintains MapReduce’s linear scalability and fault tolerance, but extends it in\n",
    "three important ways. First, rather than relying on a rigid map-then-reduce format,\n",
    "its engine can execute a more general directed acyclic graph (DAG) of operators. This\n",
    "means that in situations where MapReduce must write out intermediate results to the\n",
    "distributed filesystem, Spark can pass them directly to the next step in the pipeline. In\n",
    "this way, it is similar to Dryad, a descendant of MapReduce that originated at Micro‐\n",
    "soft Research. Second, it complements this capability with a rich set of transforma‐\n",
    "tions that enable users to express computation more naturally. It has a strong\n",
    "developer focus and streamlined API that can represent complex pipelines in a few\n",
    "lines of code.\n",
    "Third, Spark extends its predecessors with in-memory processing.\n",
    "\n",
    "Machine learning algorithms that make multiple passes over their training set\n",
    "can cache it in memory. When exploring and getting a feel for a data set, data scien‐\n",
    "tists can keep it in memory while they run queries, and easily cache transformed ver‐\n",
    "sions of it as well without suffering a trip to disk.\n",
    "\n",
    "Data sources / operability\n",
    "\n",
    "Spark boasts strong integration with the variety of tools in the Hadoop ecosystem. It\n",
    "can read and write data in all of the data formats supported by MapReduce, allowing\n",
    "it to interact with formats commonly used to store data on Hadoop, like Apache Avro\n",
    "and Apache Parquet (and good old CSV). It can read from and write to NoSQL data‐\n",
    "bases like Apache HBase and Apache Cassandra. Its stream-processing library, Spark\n",
    "Streaming, can ingest data continuously from systems like Apache Flume and Apache\n",
    "Kafka. Its SQL library, SparkSQL, can interact with the Apache Hive Metastore, and\n",
    "the Hive on Spark initiative enabled Spark to be used as an underlying execution\n",
    "engine for Hive, as an alternative to MapReduce.\n",
    "\n",
    "mportantly, Spark can then access\n",
    "any Hadoop data source—for example HDFS, HBase, or Hive, to name a few.\n",
    "\n",
    "Hadoop, an open source implementation of two papers written at Google that\n",
    "describe a complete distributed computing system, caused the age of big data. How‐\n",
    "ever, distributed computing and distributed database systems are not a new topic.\n",
    "Data warehouse systems as computationally powerful as Hadoop predate those\n",
    "papers in both industry and academia. What makes Hadoop different is partly the\n",
    "economics of data processing and partly the fact that Hadoop is a platform. However,\n",
    "what really makes Hadoop special is its timing—it was released right at the moment\n",
    "when technology needed a solution to do data analytics at scale, not just for\n",
    "population-level statistics, but also for individual generalizability and insight.\n",
    "\n",
    "Similar to how HDFS works, clients that wish to execute a job must first request\n",
    "resources from the ResourceManager, which assigns an application-specific Applica‐\n",
    "tionMaster for the duration of the job. The ApplicationMaster tracks the execution of\n",
    "the job, while the ResourceManager tracks the status of the nodes, and each individ‐\n",
    "ual NodeManager creates containers and executes tasks within them.\n",
    "\n",
    "in Hadoop 1, the MapReduce job/workload\n",
    "management functions were highly coupled to the cluster/resource management MapReduce can be very efficient for large-scale batch workloads, but it’s also quite\n",
    "I/O intensive, and due to the batch-oriented nature of HDFS and MapReduce, faces\n",
    "significant limitations in support for interactive analysis, graph processing, machine\n",
    "learning, and other memory-intensive algorithms. While other distributed processing\n",
    "engines have been developed for these particular use cases, the MapReduce-specific\n",
    "nature of Hadoop 1 made it impossible to repurpose the same cluster for these other\n",
    "distributed workloads.\n",
    "Hadoop 2 addresses these limitations by introducing YARN, which decouples work‐\n",
    "load management from resource management so that multiple applications can share\n",
    "a centralized, common resource management service. By providing generalized job\n",
    "and resource management capabilities in YARN, Hadoop is no longer a singularly\n",
    "focused MapReduce framework but a full-fledged multi-application, big data operat‐\n",
    "ing system.\n",
    "\n",
    "While YARN has enabled Hadoop to become a general distributed computing plat‐\n",
    "form, MapReduce (often abbreviated to MR) was the first computational framework\n",
    "for Hadoop. YARN allows for non-MapReduce frameworks such as Spark, Tez, and\n",
    "Storm (to name a few) to run alongside the original MapReduce application on a\n",
    "Hadoop cluster.\n",
    "\n",
    "Instead, Spark keeps the dataset in memory as much as possible throughout the\n",
    "course of the application, preventing the reloading of data between iterations. Spark\n",
    "programmers therefore do not simply specify map and reduce steps, but rather an\n",
    "entire series of data flow transformations to be applied to the input data before per‐\n",
    "forming some action that requires coordination like a reduction or a write to disk.\n",
    "Because data flows can be described using directed acyclic graphs (DAGs), Spark’s\n",
    "execution engine knows ahead of time how to distribute the computation across the cluster and manages the details of the computation, similar to how MapReduce\n",
    "abstracts distributed computation.\n",
    "\n",
    "Spark started\n",
    "in 2009 as a research project in the UC Berkeley RAD Lab, later to become the\n",
    "AMPLab. The researchers in the lab had previously been working on Hadoop Map‐\n",
    "Reduce, and observed that MapReduce was inefficient for iterative and interactive\n",
    "computing jobs. Thus, from the beginning, Spark was designed to be fast for interac‐\n",
    "tive queries and iterative algorithms, bringing in ideas like support for in-memory\n",
    "storage and efficient fault recovery.\n",
    "Spark was first open sourced in March 2010, and was transferred to the Apache Soft‐\n",
    "ware Foundation in June 2013, where it is now a top-level project. Spark 1.0 was released\n",
    "in May 2014.\n",
    "\n",
    "The second model is to run one application per workflow or user session of (possibly\n",
    "unrelated) jobs. This approach can be more efficient than the first, since containers can\n",
    "be reused between jobs, and there is also the potential to cache intermediate data be‐\n",
    "tween jobs. Spark is an example that uses this model.\n",
    "\n",
    "https://stackoverflow.com/questions/63914667/what-is-the-difference-between-driver-and-application-manager-in-spark\n",
    "https://luminousmen.com/post/spark-anatomy-of-spark-application#:~:text=The%20Driver(aka%20driver%20program,status%2Fresults%20to%20the%20user.\n",
    "https://medium.com/@goyalsaurabh66/running-spark-jobs-on-yarn-809163fc57e2\n",
    "https://mallikarjuna_g.gitbooks.io/spark/content/spark-driver.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In MapReduce 1, the jobtracker takes care of both job scheduling (matching tasks with\n",
    "tasktrackers) and task progress monitoring (keeping track of tasks, restarting failed or\n",
    "slow tasks, and doing task bookkeeping, such as maintaining counter totals). By con‐\n",
    "trast, in YARN these responsibilities are handled by separate entities: the resource man‐\n",
    "ager and an application master (one for each MapReduce job). The jobtracker is also\n",
    "responsible for storing job history for completed jobs, although it is possible to run a job \n",
    "history server as a separate daemon to take the load off the jobtracker. In YARN,\n",
    "the equivalent role is the timeline server, which stores application history. 5\n",
    "The YARN equivalent of a tasktracker is a node manager.\n",
    "\n",
    "The application master and the MapReduce tasks run in containers that\n",
    "are scheduled by the resource manager and managed by the node managers.\n",
    "\n",
    "YARN client mode is required for programs that have any interactive component, such\n",
    "as spark-shell or pyspark. Client mode is also useful when building Spark programs,\n",
    "since any debugging output is immediately visible.\n",
    "YARN cluster mode, on the other hand, is appropriate for production jobs, since the\n",
    "entire application runs on the cluster, which makes it much easier to retain logfiles\n",
    "(including those from the driver program) for later inspection.\n",
    "\n",
    " The idea is to have a global ResourceManager (RM) and per-application ApplicationMaster (AM). An application is either a single job or a DAG of jobs.\n",
    "\n",
    "The ResourceManager and the NodeManager form the data-computation framework. The ResourceManager is the ultimate authority that arbitrates resources among all the applications in the system. The NodeManager is the per-machine framework agent who is responsible for containers, monitoring their resource usage (cpu, memory, disk, network) and reporting the same to the ResourceManager/Scheduler.\n",
    "\n",
    "The per-application ApplicationMaster is, in effect, a framework specific library and is tasked with negotiating resources from the ResourceManager and working with the NodeManager(s) to execute and monitor the tasks.\n",
    "\n",
    "https://github.com/Swalloow/pyspark-ml-examples/blob/master/notebooks/spark-ml-starter.ipynb\n",
    "https://www.kaggle.com/tylerx/machine-learning-with-spark"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Conda Python 3.6.12",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
