{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "Thread/processes\n",
    "Structure de données qu'on recontre souvent que ce soit en multithreading/processing ou en asyncIO: les queues. D'un point de vue assez général, multithreading/processing, asyncIO, etc. ne sont que différentes techniques visant l'exécution concurrentes de tâches. Ces tâches sont générées par des producteurs (producers) et mises à disposition des entités qui les exécuteront (les consommateurs - consumers) le plus souvent via de queues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTIONS\n",
    "\n",
    "## Divers\n",
    "* \"which support threading and concurrency\": des langages ne supportent pas la concurrency (single threaded ou la mort)? \"support threading\" = fournir des représentations/abstraction permettant de manipuler des threads?\n",
    "\n",
    "\n",
    "## GIL\n",
    "* Une fois l'opération I/O \"lancée\", il est codé dans l'implémentation que le thread rend le GIL? Qu'est ce qui le fait passer en waiting state ?\n",
    "* Concrètement, à l'intérieur d'un \"process Python\", l'interpréteur Python correspond à un (ou plusieurs?) threads ? peut on parler de thread interpréteur?\n",
    "* Relation thread Python / thread interpréteur / GIL: comment ça se passe, quelle est la relation? Un thread Python acquiert le GIL et est donc automatiquement exécuté par l'interpréteur (qui ne se pose pas de question: il interprète les instructions du thread détenant le GIL) qui lui permet de progresser dans ses tâches plutôt qu'un thread Python qui acquérirait l'interpréteur en acquérant le GIL?:\n",
    "    * The GIL, or Global Interpreter Lock, is a boolean value in the Python interpreter, protected by a mutex. The lock is used by the core bytecode evaluation loop in CPython to set which thread is currently executing statements.\n",
    "\n",
    "* https://speakerdeck.com/dabeaz/understanding-the-python-gil?slide=8 / https://speakerdeck.com/dabeaz/inside-the-python-gil?slide=10\n",
    "* https://callhub.io/understanding-python-gil/\n",
    "* https://rohanvarma.me/GIL/\n",
    "\n",
    "\n",
    "Concurrency refers to the ability of different parts or units of a program, algorithm, or problem to be executed out-of-order or in partial order, without affecting the final outcome. This allows for parallel execution of the concurrent units, which can significantly improve overall speed of the execution in multi-processor and multi-core systems. 1\n",
    "\n",
    "Two tasks are concurrent if the order in which the two tasks are executed in time is not predetermined.\n",
    "Tasks are actually split up into chunks that share the processor with chunks from other tasks by interleaving the execution in a time-slicing way. Tasks can start, run, and complete in overlapping time periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPUs physiques, logiques, virtuels, coeurs et processeurs\n",
    "\n",
    "### CPU & processeurs\n",
    "Un CPU (*Central Processing Unit*) appelé aussi coeur (*core*) peut s'entendre comme la plus petite unité physique capable d'exécuter indépendemment des instructions. Encore aujourd'hui, la plupart des ordinateurs sont équipés d'une carte mère sur laquelle ne peut être monté qu'une seule puce (*chip*) appelée d'abord processeur puis micro-processeur (*processor*). Cette puce ne présentait jusqu'au début des années 2000 qu'une seule unité de calcul, un seul coeur (*core*), un seul CPU. Il y avait donc à l'époque équivalence entre nombre de processeurs (puces) et nombre de CPUs d'où le fait que les termes de (micro-)processeurs et CPUs/*cores* étaient utilisés de façon interchangeable. On parle rétrospectivement de *single-core processor*. Le décompte des capacités de calcul était alors simple: un ordinateur disposait alors autant de CPUs/*cores* que d'emplacements (*sockets*) dédiés sur sa carte mère: à chaque emplacement correspondait un CPU physique.\n",
    "\n",
    "Augmenter la capacité de calcul d'une même machine se focalisait alors soit sur une augmentation de la fréquence du CPU soit sur l'utilisation de cartes mères supportant plusieurs CPUs (*multiprocessors*). Dans le premier cas, la puissance électrique consommée par un CPU a l'inconvénient d'être proportionnelle à sa fréquence d'opération. L'augmentation de la fréquence a donc fini par se heurter aux limites imposées par la dissipation de la chaleur et aux capacités des batteries des mobiles (mais aussi aux capacité de transmission des bus ?). Dans le second cas, la communication entre deux processeurs est particulièrement lente du fait de son utilisation du bus système. Ce manque de performance du bus système pouvait créer des *bottlenecks* empêchant d'exploiter au maximum les capacités offerte par de multiples processeurs. Le gain promis par la multiplication des processeurs étaient ainsi souvent incomplet.\n",
    "\n",
    "Remarque: Aujourd'hui comme hier, les machines à multi-processeurs restent rares chez les particuliers et sont surtout des équipements professionnels qu'on retrouve dans les supercalculateurs ou qu'on utilise comme serveurs. Même les plus puissants des PC individuels sont le plus souvent des machines à un seul processeur mais comportant de nombreux coeurs (ex: 24) fonctionnant à une fréquence relativement élevée.\n",
    "\n",
    "### Processeurs *multi-cores* & *hyperthreading*\n",
    "Au début des années 2000, deux innovations visant à augmenter les capacités de calcul ajoutèrent de la confusion autour du terme de CPU:\n",
    "* La communication entre les processeurs d'un système multi-processeur est particulièrement lente du fait qu'elle passe par l'utilisation du bus système. En dupliquant au sein d'un même CPU partie de ses éléments comme le registre ou le cache L1 par exemple, Intel autorise l'exécution de deux *threads* sur un même coeur physique ceux-ci pouvant de plus partager de l'information sans passer par le bus système. C'est la technologie dite d'*hyperthreading* (à ne pas confondre avec le *multithreading*) qui est donc une caractérisque du *hardware* (fonctionnalité pouvant ensuite être activée ou non). L'*hyperthreading* n'est cependant pas équivalent à l'utilisation de deux *cores* mais améliore tout de même les performances d'exécution de deux *threads* jusqu'à 30-40% par rapport à une exécution sur un seul coeur physique sans *hyperthreading*. Du point de vue de l'OS, un *physical core* doté d'*hyperthreading* apparait comme deux *cores* dits logiques (*logical*). L'OS voit deux coeurs (logiques) alors qu'on n'a qu'un coeur physique. De plus, pour allouer les ressources CPU, l'OS se base sur un *pool* de *logical cores*. Le nombre de *logical cores* correspond ainsi à la capacité de traitement d'une machine. Sans *hyperthreading*, il y a coïncidence entre nombre de *logical* et *physical cores*.\n",
    "* L'*hyperthreading* ne permet cependant pas d'atteindre les même performances qu'avec deux processeurs indépendants. Les progrès de la miniaturisation permet alors d'intégrer plusieurs unités de calcul indépendantes, plusieurs *cores* (physiques) sur une même puce (*chip*), sur un même processeur. On parle alors de processeurs multicoeurs (*multi-core processors*). Situés sur la même puce, ces coeurs partagent entre autres un même bus interne fait du même silicium, les éventuels échanges entre CPUs sont ainsi beaucoup plus rapides que s'ils avaient à se faire par l'intermédiaire du bus système.\n",
    "\n",
    "Avec ces deux innovations, l'équivalence entre CPU et processeur venant des *single-core processors* devient source d'erreur: un *multi-core processor* contient par définition plusieurs *cores*/CPUs. A cela,l'*hyperthreading* vient ajouter la distinction entre *physical* et *logical* *core*/CPU.\n",
    "\n",
    "Pour prendre un exemple récapitulatif, une machine dotée d'un processeur *quad-cores* hyperthreadé possède 4 *cores*/CPU physiques physiquement localisés sur la même puce, l'OS voyant de son côté 8 *logical cores*. On voit parfois un processeur décrit par son nombre de *threads*, ce nombre est équivalent à son nombre de *logical cores* puisque que chaque *logical core* a une capacité de traitement d'un *thread*. L'*hyperthreading* fait que le nombre de *threads* peut cependant ne pas coïncider avec le nombre de *cores* physiques. Ainsi:\n",
    "* Un ordinateur à un processeur *quad-core* hyperthreadé possède 4 *physical cores*, 8 *logical cores* et peut être dit d'une capacité de 8 *threads*.\n",
    "* Un ordinateur à deux processeurs *quad-core* hyperthreadés possède 8 *physical cores*, 16 *logical cores* et peut être dit d'une capacité de 16 *threads*.\n",
    "* Un ordinateur à un processeur *quad-core* non hyperthreadé possède 4 *physical cores*, autant de *logical cores* et peut être dit d'une capacité de 4 *threads*.\n",
    "\n",
    "Remarque: L'adjectif \"logique\".\n",
    "En informatique on rencontre souvent une paire d'adjectifs employés de façon opposée: logique et physique. L'adjectif physique se réfère à l'organisation réelle d'un système. A l'opposé, l'adjectif logique désigne l'organisation d'un système telle qu'elle est présentée à et vue par un utilisateur/client. L'adjectif logique renvoie ainsi très souvent à des abstractions.\n",
    "\n",
    "La présence de multiples coeurs, qu'ils soient physiques ou logique améliore-t-elle forcément la performance ? Cela dépend de si les applications concernées savent exploiter la présence de plusieurs *cores* ou pas.\n",
    "\n",
    "### *Multitasking*\n",
    "Un OS ne limite pas le nombre de *threads* pouvant s'exécuter au nombre de *logical cores*. On peut avoir bien plus de *threads* en cours d'exécution sur une machine que son nombre de *logical cores*, ceux-ci sont simplement exécutés concurremment. On parle alors plus de *multitasking* qui est une caractéristique de l'OS. Chaque OS implémente ainsi différentes stratégies d'allocation d'une ressource partagée (temps CPU) entre différentes tâches ne pouvant être exécutées simultanément. En alternant suffisamment rapidement l'allocation de temps CPU entre les différentes tâches, l'OS permet à chacune de progresser et donne l'illusion à l'utilisateur qu'elles s'exécutent en même temps alors que ce n'est pas le cas. Cette alternance permet notamment à faire du *multitasking* sur une machine dotée d'un seul coeur. Le *multithreading* est à distinguer du *multitasking*, le premier étant une caractéristique d'un programme là où le second est une caractéristique de l'OS.\n",
    "\n",
    "### *Virtual cores/CPUs*\n",
    "La notion de *virtual core*/CPU se rapporte à la virtualisation et une nuance doit être établie entre *virtual* et *logical core*. Un *virtual core* est l'abstraction présenté par une machine virtuelle à ses applications utilisatrices de ressource CPU. Le *logical core* est une notion se rapportant au hardware sous-jacent et utilisé par l'OS pour l'allocation de ressources. En général, à un *virtual core* va correspondre un *logical core*, ce qui rend les termes presque interchangeables. Mais cela pourrait ne pas être le cas, rien n'interdit à un *virtual core* de ne représenter qu'une fraction de *logical core* et d'avoir ainsi pour une même machine physique beaucoup plus de *virtual cores* que de *logical* et *a fortiori* de *physical cores*. \n",
    "\n",
    "Remarque: Pour les *kernels* les supportant, on parle parfois des *kernel threads* comme des *virtual processors*, il n'y cependant pas identité entre le nombre maximum de *kernel threads* créés par le *kernel* et le nombre de *logical cores*: il peut y avoir bien plus de *threads* que de *cores*, les premiers ne progressant dans leurs tâches uniquement quand ils se voient allouer du temps CPU par le *scheduler* du *kernel*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'interpréteur Python\n",
    "\n",
    "### Compilation vs. interprétation\n",
    "Comme pour tout language, le code source de Python doit à un moment être traduit en instructions compréhensibles par le CPU (code machine) afin de pouvoir être directement exécuté par celui-ci. Il existe tout un continuum de techniques, de la compilation à l'interprétation, permettant de passer du code source au code machine et d'exécuter ce dernier. \n",
    "\n",
    "Dans sa définition la plus large, un compilateur est un programme traduisant d'un langage A vers un langage B. La compilation est toutefois souvent comprise comme une traduction directe du code source vers le langage machine. Une telle compilation produit des binaires ensuite directement exécutables. Il y a dissociation entre le moment où le code est compilé et l'exécution (*runtime*). Le code est dit compilé *ahead-of-(run)time* (AOT). La compilation autorise potentiellement un grand nombre de contrôles statiques et d'optimisation du code mais a l'inconvénient de pouvoir être assez longue et doit être reproduite à chaque modification du code. Les artifacts produits par la compilation étant déjà du code machine optimisé, les codes compilés sont d'exécution très rapide. Binaires, ils ne sont également pas lisibles par un humain et maintiennent ainsi la confidentialité du code source. Cela se fait cependant au détriment de la portabilité : les binaires produits sont spécifiques à la plateforme (OS+CPU) sur laquelle ou pour laquelle ils ont été générés. Afin d'être *cross-platform*, la production de sources compilés a le désavantage de requérir la production d'au moins une source compilée par type de plateforme.\n",
    "\n",
    "A l'opposé et dans sa définition la plus large, un interpréteur d'un langage A est un programme capable d'exécuter directement les intructions de celui-ci, étant entendu que le langage exécuté n'est pas du code machine. Un interpréteur va ainsi traduire séquentiellement chaque instruction du langage à interpréter en code machine (différentes stratégies sont possibles) et immédiatement les exécuter. Compilation et exécution ne sont plus séparées. L'exécution du code en est forcément ralentie par rapport à l'exécution directe de code machine qui par définition ne comporte pas tout le processus d'interprétation. Le code source étant interprété instruction par instruction, le code finalement exécuté est aussi moins optimisé que s'il avait été optimisé globalement par un compilateur. L'interprétation exige également plus de ressources par rapport à l'exécution directe. Elle possède cependant un avantage pour le débuggage: le code étant traduit et exécuté ligne à ligne, l'exécution s'arrête dès qu'elle rencontre une erreur qui est alors facilement localisée. L'interprétation permet à un langage de gagner en portabilité: il suffit d'exporter son code source (dont la confidentialité est en revanche plus difficile à assurer) et si un interpréteur du language existe pour la plateforme sur laquelle on veut finalement l'exécuter, alors on pourra l'y exécuter.\n",
    "\n",
    "Remarque: On peut voir le CPU comme un interpréteur de code machine.\n",
    "\n",
    "**Attention: le caractère compilé ou interprété n'est pas une propriété du language mais de l'implémentation**. De nombreux langages sont aujourd'hui perçus comme compilés (resp. interprétés) du fait que leur implémentation dominante l'est. De plus, un language peut ne pas être que compilé ou interprété. \n",
    "\n",
    "### Le Cas de Python\n",
    "Les principales implémentations de Python se reposent toutes sur un mélange de compilation et d'interprétation. Du fait qu'il existe toujours une étape d'interprétation, Python est ainsi souvent rangé dans la catégorie des languages interprétés. Le code source (`.py`) est d'abord compilé en *bytecode*. Ce *bytecode* est ensuite interprété par une machine virtuelle (VM) dans le cas des implémentations CPython (C), Jython (Java) et IronPython (C#) par exemple. Seul le *bytecode* dans lequel est compilé le code source diffère entre ces trois implémentations. Le premier est exécuté par la Python Virtual Machine (PVM) de CPython, le second correspond à du *bytecode* Java exécutable par une JVM et le dernier peut être exécuté dans le *framework* .NET de Microsoft (CIL).\n",
    "\n",
    "A noter que dans le cas de l'implémentation PyPy, l'interpréteur de *bytecode* est augmenté d'un compilateur (de *bytecode*) à la volée (*Just-In-Time (JIT) Compiler*).\n",
    "\n",
    "Remarque: Dans le cas de d'IronPython, le code source est compilé vers une représentation intermédiaire appelée *Common Intermediate Language* (CIL) commune à tous les languages du *framework* .NET (ex: C#, VB.NET, F#, etc.). Ce langage peut ensuite s'exécuter sur une VM appelée la *Common Language Runtime* (CLR), CIL et CLR formant ce qu'on appelle pour le *framework* .NET la *Common Language Infrastructure* (CLI). La paire CIL/CLR sont pour les langages du *framework* .NET l'analogue de la paire *bytecode* Java/JVM pour les langages dont l'implémentation principales se base sur la JVM (Java, Scala, Clojure, Kotlin, etc.).\n",
    "\n",
    "#### Remarque: la *JIT-compilation* \n",
    "La *JIT-compilation* désigne de façon très large et par opposition à l'*AOT-compilation*, le fait de compiler en code machine (le plus souvent) certaines parties du code pendant l'exécution. La *JIT-compilation* s'adjoint donc à un interpréteur dont on veut augmenter les performances. Pour savoir quelle partie du code compitler, les *JIT-compilers* analysent le code pendant son exécution (*profile*) pour en repérer les parties les plus fréquemment utilisés (*hot spots*) et qu'il choisira de compiler pour améliorer la performance d'ensemble du programme. Voilà pourquoi ces compilateurs sont aussi appelés *hot-spot compilers*. \n",
    "\n",
    "Le *JIT-compiler* va ainsi par exemple compiler et optimiser le code d'une fonction particulière, son code compilé étant mis en cache (*code cache*). Au prochain appel de cette instruction, ce n'est pas l'interpréteur qui se chargera de son exécution mais c'est le code compilé qui sera directement exécuté.\n",
    "\n",
    "Le code compilé peut être réoptimisé, recompilé et resubstitué au code précédemment exécuté autant de fois que le *JIT-compiler* le juge nécessaire. Les optimisations pouvant être réalisées par un *JIT-compiler* sont nombreuses: *dead code removal*, fusion de boucles, *loop unrolling*, intégration du corps de petites méthodes au corps de celles qui les appellent, etc. La compilation et l'optimisation prenant des ressources et surtout du temps, l'objectif d'un JIT-compiler est de trouver le meilleur compromis permettant un gain net de performance.\n",
    "\n",
    "La JVM est par exemple un interpréteur de *bytecode* suppléé de deux JIT-compilers (appelés c1 et c2). Le compilateur c1 est plus rapide mais produit un code moins optimisé et est particulièrement indiqué pour les *client-side* applications. Le compilateur c2 produit du code davantage optimisé mais exige plus de ressources et de temps. Il est particulièrement indiqué pour les *server-side applications*. Les deux peuvent être utilisés de concert depuis Java 8, chacun optimisant le code en parallèle, le compilateur c2 demandant plus de statistiques (*profiling*) sur l'exécution afin de réaliser des optimisations notables.\n",
    "\n",
    "Remarque: Dans le cas de PyPy, les gains apportés par ce *JIT compiler* ne sont cependant visible que si celui-ci a suffisamment de temps pour optimiser le code et que l'exécution ne se repose pas en grande partie sur des *C extensions*.\n",
    "\n",
    "#### L'interpréteur CPython\n",
    "Un interpréteur peut être implémenté suivant différentes stratégies. L'interpréteur CPython a choisi celle où le code est d'abord parsé et traduit (compilé) en une représentation, en un code intermédiaire: en *bytecode* (fichiers `.pyc`), qui est ensuite immédiatement exécuté par une machine virtuelle, la Python Virtual Machine (PVM), de la même manière que le *bytecode* Java est exécuté par la Java Vitual Machine (JVM). Le rôle de la VM est d'exécuter le *bytecode* et c'est elle qui permet de le faire de manière portable: elle abstrait le *hardware* sur lequel le code est finalement exécuté et fait l'intermédiaire entre un *bytecode* à caractère général et un OS/*hardware* spécifique. C'est elle qui fait la traduction entre des instructions données en *bytecode* vers les instructions machine comprises par le CPU.\n",
    "\n",
    "CPython peut ainsi se voir comme un language à la fois interprété (on exécute directement le code source sans avoir eu à le compiler) et compilé (l'exécution du code source demande d'abord de le compiler en *bytecode*).\n",
    "\n",
    "Que fait concrètement l'interpréteur ? En très gros, il parse le code, en génère un *abstract syntax tree* (AST), produit le *bytecode* à partir de l'AST puis l'exécute enfin avec la PVM. Le terme \"interpréteur Python\" ou plus largement ce que l'on obtient en téléchargeant Python depuis le site officiel de CPython recouvre donc plusieurs objets. L'interpréteur Python est ainsi à la fois un *parser*, un compilateur et une VM. Il vient également avec une implémentation de la librairie standard de Python ainsi qu'une API C programmable pour d'éventuelles extension. Bref la confusion est d'autant plus grande que ce qu'en toute rigueur l'interpréteur correspond surtout à la PVM qui est un interpréteur de *bytecode* mais on désigne souvent par abus de langage comme interpréteur l'ensemble constitué du *parser*, du générateur de *bytecode* (partie compilation) et de la PVM.\n",
    "\n",
    "Remarque: Si on souhaite voir en quel jeu d'instructions un bloc de code est finalement traduit, on peut le passer à la fonction `dis.dis` (*disassembly*).\n",
    "\n",
    "Il existe de nombreux longs posts de blogs et autres ressources autour des internes de Python, voir par exemple pour la PVM [cette publication](https://leanpub.com/insidethepythonvirtualmachine/read). \n",
    "\n",
    "#### Fichiers de bytecode Python\n",
    "Les fichiers correspondant au code source traduits en *bytecode* sont visibles et correspondent aux fichiers d'extension `.pyc` visibles dans les répertoires `__pycache__`. Le *bytecode* d'un ficher source `foo.py` va ainsi être placé dans un fichier `foo.pyc`. \n",
    "\n",
    "Les fichiers `.pyc` ne sont produits que pour les packages et sont créés lors du premier import s'ils n'existaient pas déjà pour la version de l'interpréteur utilisé. Sans modification de la source, les imports ultérieurs seront ainsi accélérés puisque la phase de compilation n'aura pas à être répétée. A l'inverse le premier import peut être relativement long puisqu'il peut être associé à la compilation des sources. On remarque ainsi que Python ne compile pas toutes ses sources à chaque exécution: les différents packages sont compilés à l'importation (donc potentiellement qu'à la première exécution).\n",
    "\n",
    "Un fichier `.pyc` contient entre autres deux informations visant à éviter les problèmes de compatibilité: le *magic number* et le *timestamp*. A l'exécution, le *magic number* du fichier `.pyc` doit coincider avec celui de la VM qui va l'exécuter (un fichier `.pyc` est VM dépendant mais plateforme indépendant, la VM est un exécuteur de *bytecode*, toute nouvelle version de Python peut venir y ajouter ou y modifier des instructions), sinon la source est recompilée. Le *timestamp* est là pour garder trace de la version du fichier source `.py` duquel le *bytecode* a été dérivé. Si on constate une différence de *timestamp*, on prend le risque de ne pas utiliser la dernière version du code, la source est alors recompilée.\n",
    "\n",
    "Avant Python 3.2, ces fichiers `.pyc` étaient situés dans le même répertoire que leur source `.py` et aucune disposition n'était prise pour qu'un même package situé sur le *Python path* puisse être facilement utilisé par différents interpréteurs Python: les fichiers de *bytecode* portaient simplement le même nom que leur source mais avec une extension différente et ils se situaient dans le même répertoire de leur source. Le premier point entrainait la recompilation des sources à chaque changement d'interpréteur, le second à des répertoire \"pollués\" par de nombreux fichiers `.pyc`.\n",
    "\n",
    "Python 3.2 implémente le [PEP-3147](https://www.python.org/dev/peps/pep-3147/) qui introduit deux évolutions dans le système d'import de CPython:\n",
    "* Les fichiers `.pyc` sont rassemblés dans un répertoire `__pycache__`. Ce répertoire est créé s'il n'existe pas déjà au moment de la compilation des sources (donc potentiellement à la première importation/exécution). Il y a un répertoire `__pycache__` par *package*/sous-*package*.\n",
    "* On identifie chaque `.pyc` par un nom unique en ajoutant au nom du fichier le nom et la version de l'implémentation qui l'a généré. Ex: `foo.cpython-36.pyc`. On peut ainsi avoir au sein d'un même répertoire `__pycache__` différents *bytecodes* d'un même fichier source. Avant, la compilation ne produisait qu'un `foo.pyc`, l'utilisation d'un nouvel interpréteur entrainait la recompilation des sources (toujours le cas) qui écrasait les fichiers de *bytecode* générés par l'autre interpréteur (ce qui n'est plus le cas).\n",
    "\n",
    "On peut empêcher Python de générer tout ce *bytecode* avec le risque d'impacter les performances du programme (import rallongé par la compilation en *bytecode* à chaque nouvelle exécution) en assignant n'importe quelle valeur à la variable d'environnement `PYTHONDONTWRITEBYTECODE` ou en lançant son application avec `python -B`. \n",
    "\n",
    "Remarque: Il est possible \"d'optimiser\" les fichiers `.pyc` générés avec `python -O` ou `python -OO`. Ces optimisations ne sont pas des optimisations de code telles que celles potentiellement réalisées par un compilateur mais correspondent en réalité à un allégement des fichiers de *bytecode* qui sont ensuite plus rapidement chargés. Ces fichiers optimisés possédaient avant Python 3.5 une extension propre (`.pyo`).\n",
    "\n",
    "Remarque: Les fichiers `.pyx` correspondent à des sources Cython qui sont ensuite converties en C (lors du build? de l'import?) puis finalement compilées. Faire un paragraphe sur CPython/les packages utilisant des extensions C tel que `numpy`? \n",
    "\n",
    "#### Quelques remarques sur les différentes implémentations de Python\n",
    "On trouve pour principales implémentations de Python:\n",
    "* **CPython**: CPython est la principale, la plus répendue et la plus à jour de toutes les implémentations de Python. Elle est écrite en C et vient avec une API C sur laquelle s'est appuyé et s'appuie encore le développement de nombreuses extensions C qui font la richesse de Python et participe de son large succès. Bon nombre de ces extensions en C ne sont pas accessibles aux autres implémentation qui peuvent se reposer sur d'autres développements pour proposer les mêmes fonctionnalités.\n",
    "* **PyPy**: PyPy est implémenté en RPython, une version restreinte (*restricted*) de Python développée pour son développement. Parmi les principales *features* de PyPy, on trouve principalement une architecture flexible, on peut en particulier choisir entre différents *memory managements* (*reference counting*, deux types de *garbage collection*) et un *JIT compiler* qui vient dans de nombreux cas fortement améliorer la performance d'exécution par rapport à CPython.\n",
    "* **IronPython**: IronPython est écrit en C# et est conçu de façon à pouvoir s'appuyer sur la richesse du *framework* .NET. Les librairies du *framework* .NET vient ainsi jouer le même rôle pour IronPython que les extensions C pour C Python. De la même manière que CPython permet d'étendre les fonctionnalités du languages par des extensions C, IronPython permet aux développeurs de s'appuyer sur des librairies .NET s'ils le préfèrent.\n",
    "* **Jython**: Jython est à Java/à l'écosystème JVM ce que IronPython est à C#/au *framework* .NET. La principale *feature* de cet interpréteur est qu'il peut charger et utiliser n'importe quelle classe Java. Le code Python est compilé en *bytecode* Java pouvant s'exécuter sur une simple JVM.\n",
    "\n",
    "Là où CPython s'est appuyé sur un riche écosystème d'extensions C, IronPython et Jython s'appuie sur la même richesse de librairies disponibles aux utilisateurs des *frameworks* .NET et de Java respectivement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Threads* & *processes*\n",
    "\n",
    "### Remarques préliminaires\n",
    "\n",
    "#### *Kernel* & *user space*\n",
    "On peut schématiquement distinguer deux types de tâches s'exécutant sur une machine: les tâches *kernel* et utilisateur (*user processes* / \"processus utilisateur\"), le *kernel* étant chargé de fournir l'ensemble des *core services* de l'OS et notamment de gérer l'exécution des *user processes* de manière à ce que ceux-ci n'interfèrent pas entre eux et ne fassent pas ce qu'ils veulent sur la machine.\n",
    "\n",
    "La mémoire est donc divisée en deux zones correspondantes: le *kernel* et le *user space*. Le *user space* est la partie de la mémoire où toutes les applications ne correspondant pas au *kernel* s'exécutent. Ces applications n'ont donc pas accès à l'intégralité de la mémoire disponible mais seulement au *user space*. A l'opposé, le *kernel space* est la zone mémoire réservée au *kernel*. Il s'agit de la zone où son code est stocké et où celui-ci s'exécute. Le *kernel* a en revanche accès à la totalité de la mémoire de la machine, *user* et *kernel space* compris.\n",
    "\n",
    "Les *user processes* n'ont certes pas accès au *kernel space* mais peuvent avoir accès à certaines parties du *kernel*. Ces parties du *kernel* sont exposées aux *user processes* via une interface: les *system calls*.\n",
    "\n",
    "##### OS vs. *kernel*\n",
    "L'OS ou simplement \"système\" est l'ensemble logiciel qui gère à la fois les *ressources hardware*, les applications et fournit plus largement un ensemble de services communs aux applications s'exécutant dessus. Le *kernel* est seulement une partie de l'OS. Le *kernel* c'est la partie de plus bas niveau de l'OS en charge de l'interaction avec le *hardware*. Il a notamment à sa charge la gestion des ressources (mémoire, réseau, fichiers) mais aussi des drivers et des *processes*. Un OS correspond à la fourniture d'un *kernel* et d'un ensemble de services additionnels (GUI, *shell*, etc.). On parle ainsi de distributions d'un *kernel* pour les OS se basant celui-ci. Il existe ainsi de nombreuses distributions Linux (OS basés sur le *kernel* Linux) comme Debian, Fedora, Red Hat, etc. qui se distinguent par les services qu'elles fournissent autour de ce *kernel*.\n",
    "\n",
    "On parle de librairies système (*system libraries*) pour les librairies apportées par l'OS distinctes du *kernel*. Ainsi, bien que *user* et *kernel space* désignent au départ des zones mémoires, la terminologie est utilisée par exention pour opposer librairies *kernel* et système, ces dernières étant parfois appelées *userspace librairies*. \n",
    "\n",
    "On rappelle que la limite du *kernel* trace la frontière entre VM et conteneur: \n",
    "* Un conteneur permet de contrôler son environnement d'exécution jusqu'aux librairies système mais tous les conteneurs partagent le même *kernel* qui est celui du *host OS* sur lequel tourne le *container engine*.\n",
    "* Une VM permet de contrôler son environnement d'exécution jusqu'au *kernel*. Les différentes VM ne partagent que le *hardware* via l'hyperviseur.\n",
    "\n",
    "Une application conteneurisée tourne ainsi dans \"son propre *user space*\". C'est ce qu'elle perçoit, ce *user space* est en fait virtuel.\n",
    "\n",
    "Remarque: On rappelle que les *system calls* constituent l'interface entre les applications du *user space* et le *kernel*, les premières faisant forcément à un moment appel au second (pour lire des données, etc.). Cette interface évolue avec le *kernel*, certains *system calls* étant dépréciés puis supprimés, d'autres étant ajoutés. Packager son application dans un conteneur ne garantie donc pas qu'elle puisse s'exécuter sur n'importe quelle *container-based infrastructure*: il faut que le *host kernel* présente une interface de *system calls* compatible avec celle utilisée par l'application conteneurisée. Il s'agit plus d'une préoccupation de long terme: peut-on exécuter sans risque une image Docker buildée il y a 5 ans sur notre infrastructure où est déployée une des dernières versions du *kernel*. Pas sûr. Comment cela se passe-t-il pour les éléments hardware dépendants ? Quand on a des sources compilées pour x86, doit-on en plus de s'assurer quand on utilise des conteneurs que le *kernel* est le bon, que le *hardware* est aussi le bon ?.\n",
    "\n",
    "Il est ainsi intéressant de noter que la [documentation](https://packaging.python.org/overview/#bringing-your-own-userspace) s'intéressant au *packaging* d'applications Python parle de la conteneurisation comme de la solution permettant de \"*bringing your own userspace*\" et de la virtualisation classique comme de celle permettant de \"*bringing your own kernel*\". \n",
    "\n",
    "#### *Kernel* & *user mode*\n",
    "Sur la machine, le *kernel* a naturellement les priviligèges les plus élevés. Ses tâches sont dites s'exécuter en *kernel mode* (parfois aussi appelé *supervisor mode*). Une tâche s'exécutant en *kernel mode* a notamment priorité dans l'accès au CPU et à accès à l'ensemble des instructions CPU disponibles. Les autres tâches ont des privilèges moins élevés, la hiérarchie des privilèges dépendant du *hardware* et de l'OS. Les applications utilisateurs s'exécutant la plupart du temps avec les privilèges les moins élevés (*user mode*). Entre *kernel* et simples *user applications* on peut trouver des services s'exécutant dans d'autres *modes* aux privilèges intermédiaires comme les *drivers* ou les *hyperviseurs*.\n",
    "\n",
    "A noter que ces hiérarchies dépendent 1) de l'architecture *hardware*, l'architecture x86 proposant ainsi par exemple 4 niveaux de privilèges (les *rings*) 2) de l'OS, dans le cas de l'architecture x86 par exemple, le *kernel* Linux n'utilise que 2 des 4 *rings* proposés, le plus privilégié (0) pour le *kernel* (*kernel mode*) et le moins privilégié (3) pour le reste des applications (*user mode*). Le *kernel* Linux ne définit ainsi que deux *modes*: *kernel* et *user mode*.\n",
    "\n",
    "##### Commande `time`\n",
    "Quand on utilise la commande `time` pour chronométrer l'exécution d'un *process*, sa sortie se décompose en trois: `real`, `user` et `sys`:\n",
    "* `real` correspond au temps phyiquement écoulé entre l'appel et la fin de l'exécution de celui-ci (*wall clock time*)\n",
    "* `user` correspond au temps CPU total passé en *user mode* c'est à dire à exécuter du *library code*.\n",
    "* `sys` correspond au temps CPU total passé en *kernel mode* c'est à dire à exécuter des *system calls*. \n",
    "\n",
    "La somme de `sys` et `user` correspond donc à la totalité du temps CPU consommé par le *process*. Ce total peut:\n",
    "* Être plus faible que `real` car `real` inclut aussi tout le temps passé par l'application à ne pas s'exécuter (attente d'I/O, d'être schedulé, etc.).\n",
    "* Être plus élevé que `real` si par exemple le code s'est exécuté sur différents *cores*. Il semble également que si le programme créé des *forks*, le temps CPU des *child processes* est ajouté à celui de leur parent pouvant s'ils s'exécutent en parallèle dépasser le total `real`.\n",
    "\n",
    "### Les *processes*\n",
    "Un *process* est une abstraction représentant une instance d'un programme en cours d'exécution. Il s'agit concrètement d'un ensemble de représentations concourant à former une description du programme. Le *process* rassemble à la fois une description de l'état d'exécution du programme **et** les ressources lui permettant de s'exécuter. \n",
    "\n",
    "Un process consiste principalement en:\n",
    "* Des ressources mémoire (*user space*), ces ressources se subdivisant principalement en 4:\n",
    "    * Une *text section* dédiée à stocker le code du programme à exécuter\n",
    "    * Une *data section* sert à stocker les variables globales et statiques allouées avant l'exécution du programme.\n",
    "    * Une *heap* est dédiée à l'allocation dynamique de mémoire au cours de l'exécution du programme.\n",
    "    * Une ou plusieurs stacks (une par *thread*) où sont stockées les variables locales dont l'espace est libéré dès qu'elles passent *out of scope*.\n",
    "* Une table *file descriptors* stockée dans le *kernel space* qui inclut notamment les entrées et sorties standard du *process*.\n",
    "* Un ou plusieurs *threads*. Un *thread* correspond à une séquence d'intructions à exécuter et à l'avancement de leur exécution correspond l'avancement de l'exécution du *process*. Chaque *thread* consiste en :\n",
    "    * Une *thread ID*\n",
    "    * Une *stack* prise sur les ressources mémoires du *process* et indépendante des *stacks* des autres *threads*. Le reste des ressources du *process* (*data*, *text*, *heap* et *file descriptors*) sont partagées.\n",
    "    * Un program *counter* gardant la référence d'où se trouve le *thread* dans sa séquence d'instructions à exécuter.\n",
    "    * Un ensemble de registres CPU. Ces informations CPU appelées aussi *CPU context* sont stockées dans le *kernel space* ou le *user space* suivant l'implémentation des *threads*.\n",
    "* Un objet appelé *process control bloc* (PCB) qui est une structure de données rassemblant toutes les données nécessaire à la description et à la gestion du *process* par le *kernel*:\n",
    "    * La *process ID*\n",
    "    * La *process ID* du *parent process*\n",
    "    * Le *process status* (*new*, *ready*/*runnable*, *running*, *waiting*, etc.)\n",
    "    * Le *working directory*\n",
    "    * Le contenu des différents registres CPU et *program counters*\n",
    "    * Les informations relatives au *scheduling*: priorité, pointeurs vers les *scheduling queues*\n",
    "    * Les informations relatives au *memory management*: *page tables*, etc.\n",
    "    * Les informations relatives à l'I/O: fichiers ouverts, etc.\n",
    "    * Différents compteurs: *user* et *kernel CPU time* consommé, etc.\n",
    "    \n",
    "Quand l'exécution d'un *process* est interrompue, son état est sauvegardé dans son PCB. Quand il sera de nouveau schedulé, le *process* sera restauré depuis son PCB au point où il avait été interrompu. De la même manière, l'état de chaque *(kernel) thread* (le *kernel* ne manage que les *kernel threads* et pas les *user threads* , cf. ci-dessous) est décrit et stocké dans (respectivement restauré depuis) dans un *thread control block* (TCB) quand il est interrompue (respectivement reschedulé). L'interruption de l'exécution d'un *thread* par le *scheduler* pour démarrer l'exécution d'un autre *thread* en attente est appelé *context switch*.\n",
    "\n",
    "Les *processes* sont par construction indépendant des uns des autres: ils ne partagent pas de ressources (*process isolation*). La communication entre processes est toutefois possibles par les différents moyens d'IPC (Inter-Process Communication) implémentés par l'OS (*shared memory*, *message passing*, etc.). Les *pipes* UNIX sont un exemple concret d'IPC (ex: `ls | grep *.csv`).\n",
    "\n",
    "#### Parent & child processes\n",
    "Un process peut créer un ou plusieurs autres *processes* qu'on distinguera en appelant le premier *parent process* et les second *child processes*. Sur les système Linux, la création d'un nouveau process passe par le *system call* `fork` d'où le fait que les *child processes* sont parfois appelés *forks*. Le *child process* créé par `fork` est un clone, une copie de son parent au moment du *fork* (données, *threads*, *file descriptors*, etc.). Ce qui est hérité du *parent process* peut toutefois dépendre des implémentations (il existe par exemple des différences entre Windows et Linux) et des options choisies pour le *fork*. Le process créé est complètement isolé et indépendant de son parent qui peut très bien poursuivre sa propre exécution comme attendre (*wait on*) que son ou ses *child processes* aient fini la leur pour poursuivre la sienne.\n",
    "\n",
    "Remarque: On parle parfois comme en Python de *main process* et de *sub-processes* pour parler de *parent process* et de *child processes* respectivement. La librairie Python `subprocess` est ainsi dédiée au démarrage et la gestion de *child processes* depuis un *process* Python alors appelé *main process*. Voir notamment [cet article](https://towardsdatascience.com/unraveling-pythons-threading-mysteries-e79e001ab4c).\n",
    "\n",
    "### Les *threads*\n",
    "Comme vu dans la sections ci-dessus, un *thread* correspond simplement à une séquence d'instructions à exécuter (abréviation de *thread of control*) et qui dispose de ses propres *stack*, registres CPU et *program counter*. Il s'agit de la plus petite entité *schedulable* indépendemment, manipulable par un *scheduler* (celui de l'OS le plus souvent), à laquelle un *scheduler* peut allouer du temps d'exécution. Comme pour les *processes*, il s'agit d'abstractions, d'objets virtuels permettant de se représenter l'exécution d'une séquence d'instructions.\n",
    "\n",
    "Un *thread* est toujour constitutif donc rattaché à un *process* dont il partage les autres ressources avec les autres *threads*. Les différents *threads* d'un même *process* ne sont ainsi pas totalement indépendant comme peuvent l'être différents *processes*. L'état des *threads* d'un *process* décrit l'état de l'exécution de ce dernier. Les *threads* correspondent aux éléments schedulable d'un *process*.\n",
    "\n",
    "On voit à leur simple description que les *threads* sont moins lourds et complexes que les *processes* (leur création en est d'ailleurs d'autant plus facilitée). On comprends pourquoi ils sont parfois appelés *light-weight processes* par opposition aux *heavy-weight processes* qui désignent alors les *processes*.\n",
    "\n",
    "Les *threads* d'un même process partagent des ressources communes comme l'espace d'adressage (*address space*) du *process*. Ces ressources partagées peuvent être mises à profit pour la communication entre *threads* qui est de ce fait plus aisée que la communication entre *processes* qui sont construits comme totalement isolés et qui doit passer par de plus coûteux moyens d'IPC. Ce partage de ressources vient au prix de la possibilité de *race conditions* si aucun moyen de synchronisation de leur accès n'a été mis en place dans le code.\n",
    "\n",
    "### *Multithreading* vs. *multiprocessing*\n",
    "L'exécution concurrente de différentes parties d'un même programme peut grandement en améliorer la performance en faisant progresser différentes tâches en parallèle, mais ces différentes \"parties\" doivent elles s'exécuter dans un *child process* séparé (*multiprocessing*) ou est ce qu'allouer à chacune un *thread* au sein du même *process* suffit (*multithreading*)? \n",
    "\n",
    "Chaque méthode vient avec ses avantages et ses inconvénients. On citera ici les principaux.\n",
    "\n",
    "Le *multithreading*:\n",
    "* A l'avantage d'exiger moins de ressources et d'*overhead* (temps de démarrage) que l'utilisation de *processes*\n",
    "* A l'avantage du fait de zones mémoire partagées, de faciliter la communication entre les différents *threads* du programme.\n",
    "* A le désavantage du fait du partage des ressources de laisser la possibilité à des *races conditions* d'apparaître et dont il faut se préoccuper (*thread safety*).\n",
    "* **Le *multithreading* n'implique pas une exécution parallèle** mais une exécution à minima concurrente. Au delà de la possibilité que notre machine n'ai qu'un seul *core*, des obstacles *software* comme le GIL CPython ou un *mapping many-to-one* entre *user* et *kernel threads* peut avoir pour effet de linéariser l'exécution des différents *threads* et potentiellement annihiler les gains du *multithreading* pour certain types d'applications, notamment celles *CPU bound*. En l'absence de GIL et avec des *mappings one-to-one* ou *many-to-many*, l'exécution physique des différents *threads* d'une application peut devenir parallèle.\n",
    "* A le désavantage que si un *thread* échoue (*fails*), l'ensemble du *process* échoue.\n",
    "\n",
    "Le *multiprocessing*:\n",
    "* A l'avantage de permettre une parallélisation de l'exécution même quand l'exécution parallèle de *threads* au sein d'un même *process* n'est pas possible (cf. GIL Python).\n",
    "* A l'avantage qu'il n'y a pas de ressources partagées entre les différentes \"parties\" du programme et donc pas de risque de *race conditions*.\n",
    "* A l'avantage que le plantage d'une partie du programme ici encapsulée dans un *process* peut ne pas faire planter l'ensemble du programme.\n",
    "* A le désavantage que la communication entre *processes* (IPC) est plus lourde et plus couteuse que dans le cas des *threads*.\n",
    "* A le désavantage d'exiger plus de ressources que dans le cas des *threads* (on ne peut pas créer des *processes* à l'infini. La création d'un nouveau *process* est également plus longue que celle d'un nouveau *thread*.\n",
    "\n",
    "### *Kernel threads* vs *user threads*\n",
    "Les *threads* sont les abstractions sur lesquelles se basent l'exécution concurrentes de tâches. Les *threads* permettent également de mieux se représenter l'exécution du programme: l'exécution du programme correspond à l'exécution de différents *threads* plus ou moins indépendant et n'est plus seulement une succession d'appels de fonction dans un code monolithique. De telles abstractions sont fournies aux programmeurs par des *threading* libraries* qui leur permettent de créer et de manimuler des *threads*. \n",
    "\n",
    "Ce support des *threads* (le fait qu'une telle abstraction soit disponible au programmeur) se fait soit au niveau du *user space* soit au niveau du *kernel* (et donc dans le *kernel space*). On parle alors dans le premier cas de *user threads*, *user-level threads* ou encore de *userland threads*, de *kernel threads* ou *kernel-level threads* dans le second.\n",
    "\n",
    "Dans le premier cas, la *threading library* se situe dans le *user space* tout comme les *threads* qu'elle permet de créer qui sont appelés *user threads*. Elle se charge ensuite de faire les *system calls* approprié auprès du *kernel* qui peut ne pas supporter les *threads*. La plupart des OS supportent aujourd'hui les *threads* mais ils ne se sont pas toujours appuyés sur cette abstraction. Quand le *threading* n'était supporté que dans le *user space*, ces librairies faisaient l'interface entre cette abstraction utilisée par les applications dans le *user space* et le *kernel* qui ne la fournissait alors pas directement.\n",
    "\n",
    "Le *kernel* peut lui aussi supporter une implémentation des *threads*. C'est le cas de la plupart des OS modernes, le *kernel thread* constituant alors la plus petite entité schedulable par le *kernel* sur le CPU. A un *kernel thread* est donc une entité se voyant allouée du temps CPU d'où le fait qu'ils soient parfois désignés sous le nom de *virtual processors* (le *kernel thread* est du point de vue du *user thread* un processeur).  \n",
    "\n",
    "Remarque: Des systèmes utilisent un intermédiaire entre *user* et *kernel threads* appelés *lightweight process* (LWP) qui apparaît aux premiers comme un *virtual processor*. Le LWP vient désigner un *user thread* adossé à un *kernel thread* (*kernel supported user thread*), puisque les *user threads* peuvent être créés sans rapport avec les capacités du *kernel* et donc être beaucoup plus nombreux que les *kernel threads*. Attention tout de même, les concepts pouvant être utilisés de manière légèrement différentes suivant les distributions.\n",
    "\n",
    "En faisant l'hypothèse que notre *kernel* supporte les *threads*, il y a donc un *mapping* à faire entre *user(-level) threads* et *kernel(-level) threads*. Les applications du *user space* n'utilisent en effet pas directement les *kernel threads*. Elles peuvent au maximum n'utiliser que les parties du *kernel* qui leur sont accessibles via les *system calls*. Ces applications n'utilisent directement que les (*user*) *threads* de la *threading librairy* à laquelle elles font appel. D'un côté, le *kernel* ne manage que les *kernels threads*, il n'a aucune conscience de l'existence de *user threads* créés par les différentes *threading librairies* dans le *user space*. Du *user space*, le *kernel* ne perçoit que les *system calls* qui lui sont adressés. De l'autre côté, il n'y a d'exécution physique que l'exécution de *kernels threads*. Pour progresser dans ses tâches, un *user thread* doit pouvoir à un moment se confondre avec/être pris en charge par un *kernel thread*. Du point de vue du *user space*, les *kernel threads* sont des ressources d'exécutions, seuls les *user threads* attachés à un *kernel thread* peuvent progresser dans leur exécution. \n",
    "\n",
    "Les modèles de correspondance entre *kernel* et *user threads* sont appelés *multithreading models* et sont OS dépendant. On en distingue le plus souvent 4:\n",
    "* Le *one-to-one*: à un *user thread* correspond un *kernel thread*. Il n'agit du modèle implémenté par les versions récentes de Linux, Windows et Solaris.\n",
    "* Le *many-to-one*: à un groupe de *user threads* est alloué un *kernel thread*. On a alors potentiellement deux étages de *scheduling*: le *scheduling* de l'accès des *user threads* au *kernel thread* qui est assuré par une composante de la *threading librairy* appelée *thread manager* et le *scheduling* des ressources CPU entre les *kernels threads* par le *kernel* qui bascule le contexte (*context switch*) des uns aux autres. Ce modèle a les principaux désavantages qu'on ne peut pas tirer partie d'une architecture *multi-core* et que s'il advient que le *thread* ayant accès au *kernel thread* réalise un *blocking system call*, l'ensemble des autres *threads* est également bloqué. \n",
    "* Le *many-to-many*: à un groupe de *user threads* correspond un groupe au plus aussi nombreux de *kernel threads* (si aussi nombreux, on est dans le *one-to-one*).\n",
    "* Le *two-stages model* qui mixe *one-to-one* et *many-to-many*.\n",
    "\n",
    "Remarques: \n",
    "* Tous les *kernel threads* ne sont pas forcément associés à des *user threads*, certains sont dédiés à l'exécution de tâches *kernel*.\n",
    "* Les modèles *many-to-many* et *two-level* exignent l'existance de moyens de communication entre le *kernel* et le *(user) thread manager* (qu'on appelle des *scheduler activations* et qui se font via des moyens de communication appelés *upcalls*) afin de pouvoir assurer en permanence qu'un nombre suffisant de *kernel threads* est alloué à l'application.\n",
    "\n",
    "Dans le cas des *mappings* *many-to-many* ou *one-to-one*, il est possible qu'un *process* consistant en plusieurs *user threads* se voit alloué plusieurs *kernel threads*. L'exécution d'un tel programme multithreadé pourra alors se faire de façon parallèle et pas seulement de façon concurrente, l'allocation de plusieurs *kernel threads* permettant l'exécution de *threads* sur autant de *logical cores*. A l'inverse, le recours au *multithreading* avec un mapping *many-to-one* n'apporte que des bénéfices limités puisque les différents *user threads* ne vont s'exécuter qu'alternativement (*interleaving*) sur un seul et même core.\n",
    "\n",
    "Remarque: Dans le cas du *kernel* Linux notamment, il n'y a pas de distinction entre *thread* et *process*, il n'y a que des *tasks* à scheduler.\n",
    "\n",
    "#####  *Kernel threads* vs *user threads*: résumé, avantages & inconvénients\n",
    "Dans le cas des *user threads*, tout le code (*threading library*) et les data structures associées à leur implémentation et leur gestion résident dans le *user space*. Tout appel à l'API résultent un simple appel de fonction dans le *user space* (et non à un *system call*, beaucoup plus \"lourd\"). \n",
    "\n",
    "Les *user threads* ont les avantages:\n",
    "* De pouvoir être schédulés dans le *user space*. Les *user threads* sont managés par la librairie (on dit plus généralement par le *run-time system*). Switcher entre les *threads* ne demande pas les privilèges du *kernel mode*. Le *user* a la main sur le moment où un *thread* particulier est schedulé.\n",
    "* Les *user threads* sont plus légers que les *kernel threads* et donc beaucoup plus rapides à créer et à manipuler. \n",
    "* Le *scheduling* des *(user) threads* peut être *application specific*\n",
    "* Permettent d'utiliser des *threads* même sur les OS ne les supportant pas.\n",
    "\n",
    "Les *user threads* ont les désavantages:\n",
    "* Beaucoup de *system calls* étant bloquants (*blocking*), les avantages apportés par les *user threads* peuvent s'en retrouver limités d'autant (même si les *threading libraries* essayent d'utiliser le plus possible de *non-blocking calls*). Le *user thread* ayant accès au *kernel thread* va le bloquer avec un *blocking system call* et les autres *user threads* devront alors attendre pour s'exécuter. Ce problème n'existe pas dans les modèles *one-to-one* mais dans les *many-to-one* ou *many-to-many*. Dans le second cas, des mécanismes permettent au *kernel* de communiquer (via des *upcalls*) avec le *(user) thread manager* (*scheduler activation*) pour mettre veiller à ce que chaque *process* ait suffisamment de *kernel threads* pour progresser dans ses tâches.\n",
    "* Faire du *multithreading* dans le *user space* peut ne pas suffire à nous apporter les avantages d'une machine *multi-core*. Cela dépend du *multithreading model* du *kernel*.\n",
    "* Le *kernel* n'a pas connaissance des *user thread*: ce n'est pas parce qu'un *user thread* existe qu'il sera automatiquement schedulé. Il y a un manque de coordination avec le *kernel* qui peut ne pas faire les meilleurs choix quant aux *threads* qu'il choisit de scheduler et donc quels *user threads* progressent ou non dans leurs tâches. Une coordination exige des moyens de communication.\n",
    "\n",
    "Dans le cas des *kernel threads*, tout le code (*threading library*) et les data structures associées à leur implémentation et leur gestion résident dans le *kernel space*. Tout appel à l'API se traduit par un *system call* dans le *kernel*. \n",
    "\n",
    "Les *kernel threads* ont les avantages:\n",
    "* Que plusieurs *threads* d'un même *process* (ou de plusieurs *processes*) peuvent être schedulés par le *kernel*. On profite alors d'une exécution parallèle.\n",
    "* Si un *thread* d'un *process* est bloqué par un *blocking system call*, le *kernel* peut scheduler un autre *thread* du même process pour que ce dernier continue de progresser dans ses tâches.\n",
    "\n",
    "Les *kernel threads* ont les désavantages:\n",
    "* De manière générale, les opérations sur les *kernel threads* sont beaucoup plus lentes que leurs équivalents sur les *user threads* (jusqu'à 100x).\n",
    "* Le *switch* d'un *thread* à l'autre ne peut par définition être fait que par le *kernel* (*kernel mode*): le *user* n'a pas la main sur quand il est décidé de passer le contexte d'un *thread* à l'autre. L'application n'a pas la main sur la gestion des *threads*.\n",
    "\n",
    "#### Sur les *threading libraries*\n",
    "Il existe toute une variété de *threading libraries* chacune proposant ses propres fonctionnalités: création et destruction de *threads*, moyens de communication entre *threads*, moyens de sauvegarde et de restauration des *thread contexts*. Certaines vont jusqu'à proposer des moyens de *scheduling* (effectué par un *thread manager*) permettant ainsi d'émuler des environnement *multithreadés* même quand l'OS sous-jacent ne supporte pas le *threading* (n'implémente pas les *(kernel) threads*). De tels *user(-level) threads* ont alors la particularité d'être managés dans le *user space* et non dans le *kernel space*, le type de *scheduling* (*preemptive*, *cooperative* ou mixte) étant alors une propriété de la librairie sur laquelle le programmeur peut avoir la main (contrairement aux politiques de *scheduling* du *kernel*).\n",
    "\n",
    "Remarque: Le terme *threading library* ne sous-entend pas qu'elle se trouve dans le *user space*. Une *threading library* peut se trouver dans le *kernel* et être exposée aux *users* via les *system calls*.\n",
    "\n",
    "Une célèbre librairie de *threading* développée pour Java par l'équipe *The Green Team* de Sun Microsystems proposait ce genre de fonctionnalité. Les *(user) threads* de cette librairie sont ainsi appelés *green threads* dans l'écosystème Java et le terme sert depuis à désigner par extension tout type de *(user) thread* ainsi managé au niveau du *user space*. Les *green threads* ne sont aujourd'hui plus utilisés en Java qui propose une autre implémentation des *threads* appelés *native threads*.\n",
    "\n",
    "Remarque: Ppour les OS supportant le *threading*, la gestion des *kernel threads* par le *kernel* se confond en partie (*many-to-many*) ou en totalité (*one-to-one*) avec celui des *user threads*. Dans le cas *one-to-one* on peut ainsi abusivement dire que les *user threads* sont gérés par le *kernel* ou plus justement, ne plus faire de distinction entre *user* et *kernel thread* et simplement affirmer que les *threads* sont gérés par le *kernel*.\n",
    "\n",
    "Un certains nombre de *threading* librairies ont entre autre choisies de se conformer à un standard. Un des plus connu correspond aux POSIX *threads* appelés aussi *pthreads*. Un POSIX *thread* n'est pas une implémentation d'une *threading library* particulière mais un standard et notamment une API. Par extention, un *pthread* désigne tout *(user) thread* implémentant l'API POSIX Thread. Des implémentations de l'API sont ainsi disponibles sur de nombreux OS *POSIX-conformant* tels que Linux, macOS, Android, FreeBSD, OpenBSD, NetBSD et Solaris, typiquement dans la librairie système `libpthread`. A noter que les *threading libraries* nativement disponible sur Windows ne sont pas *POSIX-conformant* mais que des librairies construites sur les API Windows permettent de contruire des *pthreads* sous Windows. \n",
    "\n",
    "Tous les langages (ou plutôt leurs implémentations) ne proposent ainsi pas les mêmes fonctionnalités quand ils supportent le *threading*. CPython qui supporte ainsi le *threading*, a choisi ne se reposer que sur des mécanismes système pour ses *threads*. En particulier, sur les systèmes POSIX, les *threads* CPython sont (*under the hood*) de simples *pthreads*. De son côté, Java se repose sur d'autres librairies qui lui donnent la possibilité de *scheduler* ses propres *threads* (*green threads*).\n",
    "\n",
    "Remarque: Les *threading librairies* utilisées par un langage peuvent être système (CPython) ou faire partie du langage (Java).\n",
    "\n",
    "#### *Cooperative* vs *preemptive scheduling*\n",
    "On distingue schématiquement deux grands pôles pour les politiques de *scheduling*:\n",
    "* Dans le *cooperative scheduling*, le *thread* qui s'exécute rend volontairement les ressources au *scheduler* quand il n'en a plus besoin. Par exemple, un *user thread* restitue à son *thread manager* le *kernel thread* qui lui était assigné. De ce dernier exemple on comprend que du point de vue de l'OS, le *cooperative multitasking* correspond à l'exécution d'un seul *thread*: c'est l'application via le *thread manager* ou un autre type de mécanisme qui choisit lequel de ses thread peut accéder au *kernel thread* et progresser dans ses tâches. \n",
    "* Dans le *preemptive scheduling*, le *thread* qui s'exécute peut être interrompu et forcé d'arrêter son exécution, le contexte allant être passé à un autre *thread*. Il est fréquent qu'on donne à chaque tâche un certain montant de temps CPU pour s'exécuter, si elle n'a pas fini au bout du temps alloué (*timeout*) elle sera interrompue et forcé de rendre le contrôle. De même quand un *thread* fait une *I/O request*, il est interrompu afin d'éviter de mobiliser inutilement des ressources CPU pendant qu'il attend que sa requête soit exécutée. \n",
    "\n",
    "Le terme de coopératif est parfois employé dans certains contexte (comme l'asyncIO) pour signifier que l'organisation de l'exécution des différentes tâches d'une même application se fait avec une certaine conscience du fonctionnement de celle-ci: on choisit d'exécuter un tâche parce qu'on sait que c'est le bon moment pour le faire par opposition au *scheduler* du *kernel* qui interrompt et reprogramme des tâches sans connaissance du fonctionnement de l'application dans laquelle elles s'exécutent. Il peut alors être plus efficace que le programmeur décide lui même de quand une tâche s'interrompt au profit de telle autre. Le terme de coopératif est alors souvent utilisé pour décrire ce type de programmation. \n",
    "\n",
    "Du point de vue des problèmes fréquemment rencontrés en programmation concurrente, l'apparition de ceux-ci peut ne pas être uniquement le fait de l'organisation du programme par le développeur. Le type de *scheduling* est également impactant:\n",
    "* Switchant le *context* à des moments ne pouvant ni être contrôlés ni être anticipés par le programmeur, le *preemtive scheduling* peut aboutir à des situations de *priority inversions*, *convoy*, etc.\n",
    "* Du fait qu'on attend d'un *thread* qu'il restitue lui même le contrôle de l'exécution, le *cooperative scheduling* peut lui aussi créer des situations de *convoy* ou de *starvation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concurrency/Threads vs processes: Python essential reference p.413-414\n",
    "\n",
    "Dans top on a une vision par process, un process/une app même multithreadée n'occupe qu'une ligne.\n",
    "La colonne %CPU correspond à un nombre de cores utilisé. Une app utilisant 200% CPU, mobilise 2 cores pour son exécution. Sur une machine à 4 cores par exemple sans aucun autre programme majeur en cours d'exécution, cela correspond à une utilisation autour de 50% des ressources CPU de la machine (champ %Cpu(s) du header de top). \n",
    "Par définition le champ %Cpu(s) dans l'entête est toujours inférieur à 100% alors que la colonne %CPU peut monter à des valeurs supérieures à 100% si l'application mobilise en cumulé les ressources de plus d'un core.\n",
    "App multithreadée sur machine multicore: un process utilisant plusieurs cores\n",
    "Deux apps multithreadées lancées en parallèle sur la même machine multicore: deux process se partageant chacune plusieurs cores. \n",
    "Quand un process a plusieurs threads, ces derniers peuvent s'exécuter sur plusieurs cores (donc en parallèle)? C'est l'OS qui décide?. Pas besoin d'être multiprocessé pour exploiter plusieurs cores, être multithreadé peut suffire. Il faut globalement voir si les différentes taches on besoin d'échanger des données (facile si threads)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossaire\n",
    "\n",
    "***I/O bound***: Se dit d'un programme, d'un job, d'une fonction qui passe plus de temps à récupérer ses *inputs* qu'à les processer, la source de données pouvant être l'utilisateur, un fichier, une base de données, le réseau, etc. Dit autrement: les données sont requêtées moins vites qu'elles ne sont consommées. La vitesse d'exécution de tels programmes est limitée (*bound*) par celles des opérations I/O (réseau ou disque) de la machine sur laquelle ils s'exécutent. Exemple de tâches *I/O bound*: téléchargement de fichiers (plus largement les opérations réseau), lectures/écritures disque, requêtage d'une base de données, *web scraping*, etc.\n",
    "\n",
    "Remarque: Le terme d'*I/O bound* inclut le plus souvent celui de *network bound*.\n",
    "\n",
    "Dans le cas de CPython, la présence du GIL n'a que peu d'impact négatif sur la performance d'applications multithreadées *I/O bound*. Le GIL n'empêche pas le *multithreading* d'y apporter des gains de performance. La question se pose davantage pour les applications mêlant *threads* *I/O bound* et *CPU bound*.  \n",
    "\n",
    "***CPU bound***: Se dit d'un programme, d'un job, d'une fonction qui passe plus de temps processer ses données qu'à les récupérer. Dit autrement: les données sont consommées moins vite qu'elles ne sont requêtées. La vitesse d'exécution de tels programmes est limitée (*bound*) par les capacités du ou des CPUs de la machine de la machine sur laquelle ils s'exécutent. On parle aussi de tâches *computationally intensives*. Exemples de tâches *CPU bound*: (dé)compression de fichiers, multiplications de matrices, algorithmes de recherche, traitement d'image, etc.\n",
    "\n",
    "Dans le cas de CPython, la présence du GIL est supprime tous les bénéfices à attendre du *multithreading* de telles applications. La parallélisation de l'exécution d'application *CPU bound* en présence d'un GIL impose de recourrir à d'autres solutions comme le *multiprocessing*.\n",
    "\n",
    "Remarque: Dans le cas d'applications multithreadées avec mélange de threads *I/O* et *CPU bound*, les règles commandant le passage des ressources d'un *thread* à l'autre peuvent parfois conduire à des situations indésirables tels que le *convoy effect* ou la *starvation*. Dans le cas de Python, ces situations sont surtout le produit de l'interaction entre les règles de détention du GIL et le *scheduling* réalisé par l'OS. CPython 3.2 a notamment été l'occasion d'améliorations notables. \n",
    "\n",
    "***Thread safety***: La *thread safety* est une propriété d'un bloc de code (fonction, classe, librairie, etc.) correspondant à sa capacité à pouvoir être exécuté simultanément par plusieurs *threads* partageant un même espace d'adressage (*address space*), ce qui est le cas de lorsque les différents *threads* se rattachent à un même *process*. La *thread safety* est donc un concept de programmation concurrente. Dans un code *thread safe*, il est fait en sorte que les données partagées par les différents *threads* soient manipulées de façon à éviter toute interactions non désirées entre les différents *treads*.\n",
    "\n",
    "Un code n'est pas automatiquement *thread safe* mais peut être rendu *thread safe* (par la protection de ressources partagées par des *locks* par exemple). L'exécution concurrente de code non *thread safe* expose l'utilisateur à l'apparition de *bugs* potentiellement assez difficiles à résoudres appelés *race conditions*.\n",
    "\n",
    "La réalisation de code *thread safe* se repose sur l'utilisation des principes de programmation suivants:\n",
    "* Un premier groupe de principes vise à limiter au maximum le partage de données (*shared state*) entre les différents threads:\n",
    "    * Le code exécuté concurremment doit être réentrant afin d'éviter que sa réentrée dans le même *thread* ou dans un autre *thread* ait un impact lors de la reprise de son exécution. Oblige par exemple à stocker les données partagées (*state*) dans des variables locales à chaque exécution.\n",
    "    * Créer des *thread-local copies* des données partagées.\n",
    "    * Rendre les données partagées immutables (*read-only*) ou implémenter les opérations de mutation de façon à ce qu'elle créent de nouveau objets plutôt que de modifier directement l'existant.\n",
    "* Une deuxième groupe de principes vise à synchroniser l'accès aux ressources partagées:\n",
    "    * L'accès aux données partagées (*shared state*) est protégé par des mécanismes assurant que seul un *thread* peut les lire ou y écrire à la fois. On parle de mécanismes d'exclusion mutuelle (mutex) dont les languages mettent à disposition différentes implémentation de haut niveau: *locks*, *semaphores*, etc appelées *mutex primitives*. Attention, une mauvaise utilisation de ces mécanismes peut créer d'autres problèmes propres à la programmation concurrente: *deadlocks*, *resource starvation*, etc.\n",
    "    * Les opérations d'accès aux données partagées doivent être atomiques. Elle ne peuvent en particulier pas être  interrompues par d'autres *threads* essayant d'accéder à la ressource. Ces préoccupations sont généralement de plus bas niveau et de telles opérations atomiques servent de base aux implémentations des concepts de plus haut niveau que sont les *mutex primitives*.\n",
    "\n",
    "Un code est ainsi rendu *thread safe* soit en limitant voire supprimant la présence de données partagées, soit en rendant séquentielle (*by serializing/linearizing*) l'exécution de parties du code jugées critiques du point de vue de la *thread safety* par des moyens dits de synchronisation (*synchronization*) comme les *locks*/mutexes. On exige alors de ces blocs de code jugés critiques du point de vue de la *thread safety* et dont l'exécution peut potentiellement être simultanée, que leur exécution devienne séquentielle. Attention, la synchronisation de l'exécution de ces blocs de code peut si elle est mal réalisée, être à l'origine de *bugs* et/ou diminuer les gains de performance attendus de l'exécution concurrente.\n",
    "\n",
    "Remarque: L'aquisiton/restitution d'un *lock* est couteuse, ce qui peut faire que l'exécution par un unique *thread* d'un code *thread safe* peut devenir beaucoup moins performante que sa version non *thread safe*. Ce problème est par exemple une des principales difficultés de la suppression du GIL de CPython: toutes les solutions proposées (qui globalement placent des mutexes sur un grand nombre de structure de données) ne parviennent pas à du fait de cet *overhead* être aussi performantes lors de l'exécution sur un seul *thread* que l'implémentation avec GIL.\n",
    "\n",
    "La réalisation de code *thread safe* est aujourd'hui fortement facilité par la mise à dispostion par les languages d'objets appelés *concurrency* ou *mutex primitives* (*lock*, *reentrant lock*, *condition*, *semaphore*, etc.) qui consituent les briques de base nécessaires à la réalisation d'un code *thread safe* et qui sont autant d'abstraction masquant des détails d'implémentation d'assez bas niveau et potentiellement complexes.\n",
    "\n",
    "Remarque: On rencontre parfois la distinction *thread safe* et *conditionally safe*. La premier terme s'entendant alors comme l'absence garantie de *race conditions* dans l'exécution du code, le second admettant la possibilité d'une utilisation *unsafe* de certaines parties du code dans certains contextes mais que des solutions existent pour éviter le problème. C'est alors à l'utilisateur du code de se contrainte dans son usage: son code est *thread safe* à la condition qu'il le *design*, qu'il utilise ce *third-party code* d'une certaine manière. \n",
    "\n",
    "**Réentrance (*reentrancy*)**: La réentrance est comme l'idempotence, une propriété d'une fonction (*routine*). La réentrance désigne la capacité d'une fonction à être appelée alors que ses précédentes invocations n'ont pas terminé leur exécution, l'exécution de ces appels ultérieurs n'ayant pas d'impact lors de la reprise de l'exécution des appels antérieurs. Une procédure est donc réentrante si son exécution suivant son appel (on dit qu'elle est \"entrée\") est interrompue, qu'elle est à nouveau appelée pendant cette interruption (on dit alors qu'elle est \"réentrée\") et que l'exécution de ce second appel n'a pas d'impact sur la poursuite de l'exécution du premier apppel. Dit autrement, l'exécution du code restant à exécuter dans la fonction appelante et interrompue ne dépend pas des éventuels effets produits par l'exécution de la fonction réentrée. L'interruption de l'exécution peut être provoquée par un appel de fonction (la fonction elle-même ou une autre fonction), une interruption *software* ou *hardware* ou un signal.\n",
    "\n",
    "Pour être réentrante, une fonction doit:\n",
    "* Ne pas reposer sur l'utilisation de variable globales/statiques (sauf éventuellement si elles sont *read-only*)\n",
    "* Ne pas modifier son propre code\n",
    "* Ne pas appeler de fonction non-réentrantes: si la fonction partage un ou des états avec ces fontions, elle peut ne plus être réentrante. La réentrance n'est donc pas un problème spécifique à la récursion.\n",
    "\n",
    "Si une de ces trois situations n'est pas respectée, il existe un risque que le second appel (réentré) modifie le contexte d'exécution du premier appel. Le premier appel ne donnera pas forcément alors pas le même résultat si la fonction est réentrée ou non.\n",
    "\n",
    "On comprend alors que la réentrance peut être perçue comme proche de la *thread-safety*:\n",
    "* L'utilisation de resources partagées par plusieurs appels de la même fonction (variables globales) peut comme dans l'exécution concurrente de plusieurs threads avoir un impact sur la cohérence des résultats du programme.\n",
    "* Dans le cas de la réentrance et respectivement de la *thread-safety*, on ne veut pas que l'ordre d'exécution des appels, respectivement des *threads* ait d'impact sur le résultat finalement produit.\n",
    "* On définit souvent la réentrance comme la possibilité d'éxécuter concurremment (*concurrently*) plusieurs appels d'une même fonction sans problème (*safely*). L'utilisation du terme \"concurremment\" forcément proche de la programmation concurrente contribue à créer la confusion entre reentrance et *thread-safety*.\n",
    "\n",
    "La réentrance est cependant une propriété distincte de la *thread-safety*: \n",
    "* La première est d'abord une propriété d'une procédure, la seconde d'un bloc de code pouvant être aussi large qu'une librairie. \n",
    "* Les problèmes liés à la non-réentrance n'a rien a voir avec la possibilité d'exécuter concurremment plusieurs *threads*: on rencontrait et rencontre encore ces problèmes pour des programmes à un seul *thread*. C'est un problème plus ancien que ceux présentés par les systèmes multi-tâches.\n",
    "* On peut être réentrant sans être *thread-safe*: exemple d'une fonction utilisant une variable globale. La fonction stocke la valeur de la variable et la restaure avant de terminer ce qui assure la réentrance. Mais sans *lock*, l'exécution du corps de la fonction n'est pas *thread-safe*.\n",
    "* On peut être *thread-safe* sans être réentrant:\n",
    "    * Si on reprend l'exemple précédent, la variable est toujours globale mais faite *thread-local*: on est thread-safe mais pas réentrant si on modifie cette variable.\n",
    "    * Autre exemple: si le corps de la fonction comprend une section critique lockée et qu'elle est récursivement appelée au sein de ce bloc de code critique, on a créé une situation de *deadlock* (et on est pas *thread-safe*). La fonction réentrée (appelée) va en effet attendre que la fonction appelante et interrompue relâche le *lock* ce qui ne peut se produire avant que la fonction appelée n'ait fini de s'exécuter. Ce problème peut être évité si le *lock* utilisé est lui-même réentrant. La fonction devient alors *thread-safe* et réentrante. On remarque qu'ici, rendre la fonction *thread safe* par l'utilisation d'un *lock* (non réentrant) peut lui faire perdre son caractère réentrant (elle l'était peut être sans le *lock*).\n",
    "\n",
    "**Convoy effect**: Dans le cadre de l'exécution concurrente de plusieurs *threads* en compétition pour l'acquisition de temps CPU, le convoy effect correspond à la situation où un *thread CPU bound* se retrouve à détenir les ressources CPU pendant une période assez longue pendant laquelle plusieurs *threads* ne requérant que peu de temps CPU pour s'exécuter (par exemple des *threads I/O bound*) attendent d'avoir accès à la ressource. L'ensemble du programme ou du système peut s'en retrouvé inacceptablement ralenti. Cette situation se rencontre notamment dans le cas d'algorithmes d'allocation type \"premier arrivé premier servi\" (First Come First Serve - FCFS). Le terme provient de l'image d'un embouteillage créé sur une route à une voie par un véhicule lent derrière lequel patientent de nombreuses voitures potentiellement plus rapides mais incapables de le doubler.\n",
    "\n",
    "Remarque: *convoy effect* et *priority inversion* semblent proches, le premier étant une conséquence possible du second (?).\n",
    "\n",
    "***Resource starvation***: Dans le cadre de l'exécution concurrente de plusieurs *threads* en compétition pour l'acquisition d'une ressource partagée et protégée, la *resource starvation* correspond à la situation où un *thread* attendant de pouvoir acquérir cette ressource ne parvient jamais (potentiellement dans la limite d'un *timeout*) à l'obtenir au moment où elle devient disponible. Le *thread* ne parvient alors jamais à avancer sur ses tâches avec des conséquences potentiellement catastrophiques sur la poursuite de l'exécution de l'ensemble du programme.\n",
    "\n",
    "***Deadlock (interblocage)***: Dans le cadre de l'exécution concurrente de plusieurs *threads* en compétition pour l'acquisition d'une ressource partagée et protégée, un *deadlock* correspond à la situation où deux *threads* détentant chacun un *lock* attendent chacun que l'autre relâche son *lock* pour progresser dans ses tâches. La situation est alors indéfiniment bloquée.\n",
    "\n",
    "***Priority inversion***: Dans le cadre de l'exécution concurrente de plusieurs *threads* en compétition pour l'acquisition de temps CPU, la *priority inversion* correspond à la situation où un *thread* de priorité élevée attend qu'un *thread* de priorité faible ait fini de s'exécuter pour poursuivre son exécution. Une telle situation peut entrainer une forte latence voire mener à un *deadlock*.\n",
    "\n",
    "***Race condition***: Une *race condition* (parfois traduit \"situation de compétition\") est dans un système multi-acteur, une situation dans laquelle le résultat produit par celui-ci est dépendant de l'ordre dans lequel agissent les acteurs du système. Cet ordre d'exécution peut être ni contrôlable ni déterministe ce qui peut mener pour certains ordonnancements particuliers à des situations de blocages ou de panne particulièrement difficiles à identifier et à reproduire. Les *race conditions* se recontrent ainsi en informatique (mais pas que) en programmation concurrente où les acteurs sont des *threads* ou des *processes* (dont le *scheduling* est intrinsèquement non déterministe) mais aussi dans les systèmes distribués.\n",
    "\n",
    "En informatique, les *race conditions* se recontrent surtout lorsque l'exécution des différents acteurs en concurrence (*threads* ou *processes*) dépend de données partagées (*shared state*). La solution est alors de rendre les opérations sur ces données partagées mutuellement exclusives: seul un acteur peut agir sur ces données à la fois sans être interrompu. Les portions de code où interviennent ces opérations sont appelées section critiques. Ne pas se plier à cette simple règle revient à prendre le risque de corrompre ces données partagées.\n",
    "\n",
    "***Mutual exclusion (mutex)***: L'exclusion mutuelle est en programmation concurrente une solution apportée à la possibilité d'existence de *race conditions* lorsque les différents acteurs contribuant à l'exécution du programme partagent des ressources communes. \n",
    "\n",
    "L'exclusion mutuelle est une caractéristique d'un bloc de code. Dans un bloc de code exécuté en exclusion mutuelle, quand un thread entre dans cette partie du code, aucun autre thread ne peut y entrer ou interrompre le premier tant que celui-ci n'en est pas sorti. On parle désigne un tel bloc de code par le terme de section critique (*critical section*). Les sections critiques sont typiquement les sections du code où se fait l'accès aux ressources partagées.\n",
    "\n",
    "Plus largement, une solution acceptable à l'accès concurrentiel à des ressources partagées doit implémenter l'exclusion mutuelle, ne pas conduire à de *deadlocks* c'est à dire que si d'autres *threads* demandent à entrer dans la section critique, au moins un d'entre eux doit pouvoir y parvenir, aucun thread ne doit rester bloqué de façon permanente dans la section critique. On peut ajouter à celà une condition plus forte: tout *thread* demandant à pouvoir entrer dans la section critique doit pouvoir finalement y arriver. Cette condition nous garde de la situation de *ressource starvation*.\n",
    "\n",
    "Il existe de nombreuses solutions permettant d'implémenter l'exclusion mutuelle aussi bien à un bas niveau (suppression des interruptions, *busy-waiting*, opérations d'accès à des ressources rendues atomique) qu'à plus haut niveau, ces solutions appelées *concurrency primitives* s'appuyant sur et abstrayant les implémentation de l'exclusion mutuelle à plus bas niveau. Parmis ces solutions on trouve les *locks* (parfois appelées *mutexes* par abus de langage), les *recursive/reentrant locks*, les *semaphores*, etc. \n",
    "\n",
    "***Lock***: En programmation concurrente, un *lock* (appelé aussi *mutex*) est un mécanisme de synchronisation permettant d'implémenter le principe d'exclusion mutuelle. Un *lock* permet de rendre mutuellement exclusive l'exécution d'un bloc de code pouvant générer des *race conditions* (*critical section*). Pour y entrer, un *thread* doit demander le *lock*. Soit celui-ci est disponible et le *thread* l'acquier, soit il ne l'est pas et le *thread* demandeur doit attendre qu'il le soit à nouveau c'est à dire seulement une fois que le *thread* actuellement dans la section critique en est sorti. Une fois sorti de la section critique, le *thread* doit restituer le *lock* qui peut alors être acquis par un autre *thread*.\n",
    "\n",
    "On distingue un type particulier de *locks* appelés *recursive locks* ou *reentrant locks* qui ont la capacité de pouvoir être acquis plusieurs fois par le même *thread*. Ils sont particulièrement utiles dans dans le cas d'appels *stackés* de fonctions où plusieurs d'entre elles veulent accéder à la même ressource et cherchent donc à chacune à accéder au même *lock*. Dans de tels cas, l'utilisation d'un *lock* simple aboutirait à un *dead lock*: la fonction appelée attend (indéfiniment) d'acquérir le lock détenu par la fonction appelante (ou une autre fonction située plus haut dans la *call stack*) dont l'exécution ne reprendra qu'une fois celle de la fonction appelée terminée.\n",
    "\n",
    "**Atomicité (*atomicity*)** : L'atomicité est en programmation concurrente une propriété d'une opération ou d'un ensemble d'opérations qui qui s'exécutent entièrement sans pouvoir être interrompues. De telles opérations sont qualifiées d'atomiques (on ne peut pas les diviser en sous-opérations par des interruptions). \n",
    "\n",
    "Ce concept est proche mais distinct de la propriété d'atomicité pour décrire des transactions en informatique, notamment les transactions effectuées avec une base de données (le A des principes ACID). L'atomicité prend ici un sens un peu plus large: soit la transaction se fait en entier, soit elle ne se fait pas du tout et on ajoute que les données doivent être restaurées dans leur état précédant la transaction. Cette propriété est exigée dans toutes les situations: panne de courant, de *hardware*, etc.\n",
    "\n",
    "***busy-waiting***: Technique par laquelle un programme/*process* contrôle périodiquement la validité d'une condition. Le terme de *busy-waiting* est négativement connoté et associé à de faibles performances du fait que le changement de condition peut intervenir un très faible nombre de fois rapporté au nombre de contrôles, gaspillant autant de cycles CPU.\n",
    "\n",
    "----\n",
    "\n",
    "programmation concurrente https://fr.wikipedia.org/wiki/Programmation_concurrente\n",
    "concurrency primitives / abstractions: toutes sont globalement des locks (?), certaines inclus le fait de prévenir les autres threads, etc. (?). Permet d'économiser de l'overhead (?): threads qui attendraient avant de redemander le lock ?\n",
    "semaphore\n",
    "mutual exclusion problem / mutex\n",
    "lock\n",
    "X deadlock\n",
    "X race condition\n",
    "thread : abstraction, représentation de ...\n",
    "process vs subprocess\n",
    "fork\n",
    "X* I/O bound\n",
    "X* CPU bound\n",
    "X thread-safety\n",
    "X reintrancy\n",
    "Python GIL\n",
    "X atomic operations\n",
    "Quand est-ce que le multithreading / multiprocessing apporte-t-il un gain, quand n'en apporte-t-il pas? blocking IO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rassembler les problèmes communs de prog concurrente : cf. wikipedia FR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le *Global Interpreter Lock* CPython\n",
    "Bien que CPython fournisse notamment via le module `threading`, une interface permettant à un programmeur de manipuler des *threads* (*thread programming*) et par ce même moyen de créer des applications multithreadées, CPython (comme d'autres implémentations) contraint l'exécution de ces *threads*. L'interpréteur est en effet protégé par un *lock* appelé *Global Interpreter Lock* (GIL) qui l'empêche d'interpréter et donc d'exécuter le code de deux *threads* ou plus à la fois. Dit autrement, seul le *thread* détenant le GIL peut être exécuté par l'interpréteur (?) avec pour conséquence évidente que seul un *thread* Python ne peut être interprété et donc exécuté à la fois. La présence d'un GIL rend donc l'exécution de plusieurs *threads* non pas parallèle mais seulement concurrente.\n",
    "\n",
    "Le principal impact négatif du GIL est que la performance d'une application *CPU bound* ne peut pas être améliorée en recourant au *multithreading*. La contrainte du GIL n'apparaît donc que dans ce contexte: un développeur ne construisant que des programmes *single threaded* peut programmer une vie entière sans avoir à se soucier voire à entendre parler du GIL.\n",
    "\n",
    "Remarque: A chaque *process* Python correspond à un interpréteur et que le programme consiste en un ou plusieurs *threads*, il n'y a donc qu'un seul GIL par *process*/programme en train de s'exécuter.\n",
    "\n",
    "### Pourquoi le GIL ?\n",
    "\n",
    "#### Absence de *thread safety* du *memory management* CPython\n",
    "Dans le cas de CPython, la principale motivation à l'origine de l'utilisation d'un GIL est que le type de *memory management* choisi n'est pas *thread safe*. CPython utilise en effet comme méthode de *memory management* le *reference counting*: pour chaque object Python, on garde le nombre de références pointant vers cet objet. Quand le compteur d'un objet atteint zéro c'est que plus aucune entité du programme n'a besoin de lui, on libère alors l'espace mémoire utilisé par ce dernier.\n",
    "\n",
    "En cas d'exécution concurrente de plusieurs threads et sans plus de précautions, de tels compteurs peuvent être l'objets de *race conditions*. Dit autrement, leur mise à jour simultanée peut produire des incohérences. La survenue de *race conditions* peut se traduire par deux conséquences également fâcheuses. Le compteur peut en effet soit ne jamais atteindre la valeur zéro, l'espace mémoire pris par l'objet n'est alors jamais libéré créant une fuite mémoire (*memory leak*), soit atteindre la valeur zéro alors que des références à l'objet existent toujours et le supprimer de la mémoire alors que des parties du programme s'attendent encore à pouvoir l'y trouver.\n",
    "\n",
    "Une approche naïve consisterait à protéger tous ces compteurs par des *locks* ou au moins ceux des objets partagés par les différents *threads*. Au dela des problèmes de *deadlock* pouvant apparaître, les très nombreuses acquisitions/restitutions de *locks* que cela impliquerait représentent un coût et un impact sur la performance tel qu'il a été jugé inacceptable.\n",
    "\n",
    "Le GIL vient résoudre ce problème: on ne pose qu'un seul *lock* mais sur l'interpréteur. La présence d'un unique *lock* élimine la possibilité de *deadlocks*, les coûts d'acquisition/restitution des *locks* sont faibles. Le recours à un GIL a le réel avantage de la simplicité qui se paye cependant du prix élevé qu'elle a de rendre séquentielle l'exécution du *bytecode* et donc de rendre de facto *single threaded* tout programme Python *CPU bound*. \n",
    "\n",
    "#### Absence de *thread safety* des extensions C\n",
    "CPython et sa librairie standard se reposent sur l'utilisation de nombreuses extensions C qui ont contribué et contribuent encore largement à son attractivité et sa large adoption. Beaucoup de ces librairies ne sont pas *thread safe* et la présence du GIL a rendu leur intégration beaucoup plus aisée.\n",
    "\n",
    "#### Le GIL est-il spécifique à CPython ?\n",
    "Le GIL n'est unique ni à Python ni à CPython. CPython et PyPy recourent à un GIL mais pas Jython par exemple. De même, parmi les autres langages présentant des implémentation avec GIL, on peut citer Ruby MRI pour Ruby.\n",
    "\n",
    "Un GIL est plus largement un mécanisme de synchronisation pouvant être utilisé dans la conception d'interpréteurs afin d'interdire à ceux-ci d'exécuter concurremment du code qui n'est pas *thread safe*. Le *memory management* de CPython n'est ainsi pas *thread safe*, l'exécution de code par l'interpréteur (C)Python n'est donc pas *thread safe*. De même de nombreuses extensions C sur lequelles s'appuient de nombreuses fonctionnalités du langage et la librairie standard CPython ne sont pas *thread safe*. Le GIL a été la principale solution apportée à cette absence de *thread safety*.\n",
    "\n",
    "Comme on l'a vu plus haut, le recours à un GIL permet également de simplifier considérablement le design de l'implémentation du langage: on a pas besoin de recourir à une abondance de *locks* dont la gestion peut se révéler ardue. Cette très faible utilisation des *locks* donne une prime de performance aux langages à GIL dans l'exécution de programmes si bien que les langages sans GIL doivent trouver des moyens de compenser cet écart de performance (emploi de *JIT compilers*, etc.).\n",
    "\n",
    "#### Si l'implémentation que j'utilise possède un GIL, ai-je vraiment besoin de recourrir à des mécanismes de synchronisation ?\n",
    "Oui. Le GIL protège seulement l'accès à l'interpréteur et n'empêche pas la possible apparition d'incohérences lors de l'*update* de données partagées. Prenons par exemple une ressource (ex: une variable) partagée par différents *threads* chacun exécutant un code contenant une section où on lit cette ressource, la modifie, et y écrit la modification (section qu'on appelerait critique). Imaginons à présent que lors de l'éxécution concurrente de ces *threads*, l'OS réalise un *context switch* d'un premier *thread* vers un autre après la lecture de la ressource partagée par le premier mais avant l'écriture de ses modifications. Il est ensuite laissé le temps au second *thread* d'exécuter sa section critique avant que le contexte ne soit rebasculé sur le premier *thread* qui va pouvoir écrire ses modifications calculées basées un état qui n'est désormais plus celui dans lequel se trouve la ressource avec pour effet possible l'apparition d'une incohérence.     \n",
    "\n",
    "Prendre par exemple le problème classique de l'*update* de la balance d'un compte en banque où le premier *thread* retire quand l'autre verse sur le compte.\n",
    "\n",
    "Malgré la présence du GIL, il faut ici quand même synchroniser l'exécution des sections de code jugées critiques (où utiliser d'autres mécanismes visant de la même façon à nous préserver des incohérences, ex: *software transactional memory*).\n",
    "\n",
    "### Conséquences de la présence d'un GIL\n",
    "La principale conséquence du GIL est que les programmes Python multithreadés ne peuvent pas bénéficier des gains de performances apportés par l'utilisation de plusieurs CPUs/*cores*: du fait qu'un seul *thread* Python ne peut être exécuté à la fois, le programme qu'il consiste d'un ou plusieurs *threads* n'est exécuté que sur un seul CPU/*core*.\n",
    "\n",
    "En réalité, le GIL ne constitue une limitation que pour les programmes *CPU bound*. La présence du GIL n'impacte pas notablement l'exécution de programmes multithreadés réalisant essentiellement des opérations d'I/O (*I/O bound*). Lors de ces opérations, les *threads* impliqués passent l'essentiel de leur temps à attendre (*wait*) que l'opération d'I/O se réalise. Ils peuvent alors rendre le GIL (en CPython toutes les opérations d'I/O considérées bloquantes (*blocking*) entrainent une restitution du GIL) et ainsi permettre à d'autres *threads* de poursuivre leur exécution limitant d'autant l'impact du GIL sur la performance d'ensemble du programme. C'est pourquoi l'usage du *multithreading* reste recommandé pour la réalisation de programmes Python *I/O bound*.\n",
    "\n",
    "L'existence du GIL n'impacte réellement que les programmes *CPU bound* dont la performance ne peut être améliorée par *multithreading*. Accroître la performance de ces programme exige, amélioration de l'algorithme mise à part, de contourner le GIL.\n",
    "\n",
    "Etant donné que seul le *thread* détenant le GIL a le droit de s'exécuter, sans plus de règles régulant la détention de celui-ci, le *thread* détenant le GIL peut sans plus de règles le conserver jusqu'à avoir exécuté l'ensemble de ses tâches. On aperçoit vite le potentiel catastrophique d'une telle absence de règles pour la performance d'applications multithreadées. \n",
    "\n",
    "Chaque implémentation de CPython vient avec un jeu de règles régulant la détention du GIL. Ces règles définissent les conditions dans lesquelles le GIL est relâché (par exemple lors de l'exécution d'une opération d'I/O, au bout d'un certain temps d'exécution, etc.), acquis ou potentiellement réacquis par un *thread*. Ces règles jouent un rôle critique dans la compréhension des performances (bonnes ou mauvaises) de différents types d'applications (*threads CPU bound only*, compétition *threads I/O bound* et *CPU bound*, etc.) dans différents contextes (*single core* ou *multi-core*). \n",
    "\n",
    "#### Règles de détention du GIL\n",
    "Dans le cas de CPython, les règles de détention du GIL on été entièrement revue avec CPython 3.2. \n",
    "\n",
    "##### Règles de détention du GIL avant CPython 3.2\n",
    "Dans l'ancien GIL, un *thread* pouvait détenir le GIL le temps d'exécuter un maximum de 100 instructions de *bytecode* appelés *ticks*. Une fois fini ou une fois passés 100 *ticks*, le *thread* rendait alors le GIL tout faisaint notifier les autres *threads* en attente que le GIL était disponible. \n",
    "\n",
    "Ce système simple était cependant à l'origine de situations problématiques sur les machines *multi-cores* où plusieurs *threads* d'un même programme finissent par se disputer le GIL (*GIL contention*). Prenons par exemple un *thread CPU bound* détentant le GIL et un *thread I/O bound* tentant de l'acquérir:\n",
    "* A chaque fois que le *thread* détenant le GIL le restitue, il signale toujours à l'autre *thread* que le GIL est disponible. La première dimension du problème réside dans le fait qu'étant sur un système *multi-core*, le premier *thread* est maintenu actif par l'OS et récupère le GIL pendant le temps que le *thread* notifié passe de l'état *waiting* à *runnable* (*wake up*). Une fois \"réveillé\", le GIL n'est plus disponible. Cette situation peut perdurer longtemps pouvant créer des situations de *starvation* ou de *convoy* (un *thread* jugé de plus haute priorité peut ne pas arriver à s'exécuter dans un temps raisonnable du fait de la monopolisation des ressources - l'interpréteur - par un *thread* de plus basse priorité).\n",
    "* La deuxième dimension du problème provient que disposant de plusieurs *cores*, l'OS va scheduler les deux *threads* pour l'exécution (par exemple, un paquet à traiter par le second *thread* vient d'arriver) cette exécution simultanée n'est cependant pas possible à cause de la présence du GIL. Le *thread* ne détenant pas le GIL va alors à chaque fois qu'il est schedulé échouer à acquérir le GIL. L'OS ne sachant rien du GIL, cette situation peut se représenter un nombre très élevé de fois, c'est la \"GIL *battle*\". Le problème est que chaque tentative est associé à un certain nombre d'appels système, les démultiplier c'est consacrer une part croissante de ressources à des usages improductif et donc dégrader la performance du programme et du système.\n",
    "\n",
    "Remarque: L'interpréteur CPython ne prend pas à sa charge l'ordonnancement (*scheduling*) des *threads* et la délègue intégralement à l'OS. On a vu ci-dessus que cette délégation complète du *scheduling* à l'OS et la présence du GIL peuvent dans certaines situation interagir négativement.\n",
    "\n",
    "##### Le nouveau GIL (depuis CPython 3.2)\n",
    "CPython 3.2 introduit un tout nouveau GIL. Les *ticks* disparaissent, un *thread* peut désormais détenir le GIL aussi longtemps que nécessaire. Cependant, si un autre *thread* demande à acquérir le GIL, l'actuel détenteur du GIL a maximum 5ms (par défaut) pour le restituer. Une fois restitué, sa disponibilité est signalée par l'OS au prochain *thread* qu'il juge prioritaire pour s'exécuter (ma compréhension, puisque le *thread* qui a fait le premier fait la demande d'aquisition n'est pas garanti d'obtenir le GIL, je ne sais pas si c'était dejà comme cela avant). De l'autre côté, le thread ayant restitué le GIL doit attendre un signal lui signifiant que le GIL a été acquis par un autre *thread*: il ne peut plus réaquérir immédiatement le GIL, il n'y a plus de possibilité de *GIL battle*.\n",
    "\n",
    "Les deux principaux problèmes de l'ancien GIL sont résolus: la possibilité pour un *thread* de réacquérir immédiatement le GIL, le monopolisant de facto et la possibilité de d'existence de threads échouant un très grand nombre de fois à acquérir le GIL (GIL *contention/battle*). La critique la plus saillante adressée à ce nouveau GIL concerne le fait que la demande d'acquisition du GIL n'est pas préemptive: on peut attendre jusqu'à 5ms d'obtenir (éventuellement) le GIL, ce qui ajoute autant de latence à des tâches d'I/O et qui peut ne pas être considéré comme acceptable.  \n",
    "\n",
    "La connaissance des règles de détention du GIL ne suffisent donc pas à la compréhension de l'exécution d'une application multithreadée. Il faut également intégrer l'interaction de ces mécanismes avec l'OS. Pour plus de détails, voir notamment les [présentations de David Beazley](http://www.dabeaz.com/GIL/) consacrées à l'ancien et au nouveau GIL CPython.\n",
    "\n",
    "### Contourner le GIL\n",
    "Parmi les différentes solutions offertes à un programmeur pour améliorer la performance de son application malgré le GIL on trouve: \n",
    "* **L'utilisation du *multiprocessing***: On permet aux parties critiques de s'exécuter de façon parallèle et on fait réaliser cette exécution par un *process pool* constitué de *processes*/d'interpréteurs Python indépendants pouvant chacun s'exécuter sur un CPU/*core* différent. Malgré les inconvénients associés à l'utilisation de *processes* plutôt que de *threads* (temps de démarrage et besoin de ressources accru, IPC *overhead*), on parvient ici à tirer partie de l'existence de plusieurs CPUs/*cores*. Toutefois, du fait des resources mobilisés à la création de chaque nouveau *process*, il existe une limite à la multiplication des *processes* pour l'exécution parallèle. Cf. les modules `multiprocessing` ou `concurrent.futures`.\n",
    "* **Ecrire les parties critiques en C**: CPython inclut une API C (la *CPython C API*) qui permet à un programmeur de développer des modules en C appelés extensions C (*C extensions*) qu'on peut ensuite utiliser directement depuis Python et/ou qui peuvent elles-mêmes directement faire appel à Python. Si l'extension C entend être exécutée concurremment, il faut s'assurer d'encadrer tout appel à Python depuis C via l'API (utilisation de *data structures* Python, appel à des fonctions Python, etc.) par l'acquisition et la restitution du GIL sous peine de s'exposer à des *race conditions*. On voit alors que recourir aux extensions C n'est pas une condition suffisante à l'amélioration des performance: le recours à la *CPython C API* nous soumet toujours à la contrainte du GIL. Pour un réel gain de performance, la partie critique ne devra pas recourir à la *CPython C API*. Le *thread* C associé n'aura alors pas besoin de détenir le GIL pour exécuter cette partie du code. `numpy` est un exemple de package dont la performance remarquable repose sur l'utilisation de telles extensions C.  \n",
    "* **Utiliser une autre implémentation de Python**: Certaines comme IronPython ou Jython n'ont pas de GIL, d'autres comme PyPy ont un GIL mais incluent des *features* de nature à améliorer la performance comme un *JIT-compiler*.\n",
    "\n",
    "Remarques: \n",
    "* L'exécution par plusieurs *processes* impose un certain nombre de restrictions au code à paralléliser: celui-ci doit être entièrement contenu dans une fonction: les *closures*, lambdas et objets *callables* ne sont pas acceptés. Des informations devant être échangées au moins entre le *parent process* et les autres, les arguments et valeurs retournées par la fonction doivent être picklelizables. Cette sérialisation vient s'ajouter à l'*IPC overhead*, coûts se devant être négligeables un fois rapportés à la tâche à exécuter.\n",
    "* On voit donc que le GIL est un obstacle au parallélisme. La réalisation d'un \"vrai\" parallélisme en présence d'un GIL impose l'utilisation de plusieurs *processes*.\n",
    "\n",
    "#### *Multithreading* ou *multiprocessing* ?\n",
    "Pour résumer, si l'alternative est utiliser plusieurs *threads* ou *processes*:\n",
    "* Si les opérations à réaliser sont *I/O bound*: utiliser des *threads*.\n",
    "* Si les opérations à réaliser sont *CPU bound* mais réalisées par des libraires externes écrites en C (*C extensions*): utiliser des *threads* (suppose que l'extension C ne s'appuie pas sur la CPython C API pour réaliser la tâche).\n",
    "* Si les opérations à réaliser sont *CPU bound* et écrites en Python: utiliser des *processes*.\n",
    "\n",
    "### Vers une possible suppression du GIL CPython ?\n",
    "Pour son principal défaut ne de pas permettre l'amélioration des performance d'applications *CPU bound* par *multithreading*, le GIL CPython est régulièrement débattu mais ne semble pas prêt de disparaître pour plusieurs raisons parmi lesquelles:\n",
    "* C'est difficile, en particulier pour s'assurer de la *thread safety* de l'ensemble de la librairie standard.\n",
    "* Cela poserait des problèmes de rétrocompatibilité: la *thread safety* de certains programmes est assurée par le GIL, les utiliser avec la nouvelle version sans GIL peut générer des problèmes absents n'existant pas sur les versions antérieures. Cela inclut notamment de nombreuses *C extensions* qui ont fait le succès de Python. \n",
    "* L'absence de GIL assure d'une bonne performance les programmes *single-threaded* (et *multi-threaded I/O bound*). Guido van Rossum avait ainsi déclaré un article intitulé “It isn’t Easy to remove the GIL” en septembre 2007 qu'il ne soutiendrait pas toute suppression du GIL qui ferait baisser la performance de tels programmes. Aucune tentative n'a jusqu'ici rempli ce critère.\n",
    "* Supprimer le GIL revient forcément à compliquer l'implémentation et on craint que cette complexification rende plus difficile de maintenir et faire evoluer le langage ainsi en même temps qu'elle découragerait de nombreux développeurs d'y participer.  \n",
    "\n",
    "Une fois constaté le manque de performance de son application, il faut rester *fair-play*: avant de blâmer le GIL, vérifier que c'est bien lui qui est en cause et même dans les cas où les limitations de l'application trouvent bien leur origine dans la présence du GIL, les solutions ne manquent pas.\n",
    "\n",
    "#### PEP 554\n",
    "Le [PEP 554](https://www.python.org/dev/peps/pep-0554/) qui pourrait être intégré à Python 3.10 propose qu'un même *process* Python puisse inclure plus d'un interpréteur, ceux-ci pouvant être manipulés par le programmeur via un module `interpreters` dédié (~*interpreter programming*). Comme dans le cadre *multiprocessing* où on distingue le \"*main*\" *process* en désignant les autres sous les termes de *sub-process*, on parlerait ici de \"*main*\" *interpreter* et de *sub-interpreters*. Cela ne signe pas la fin du GIL: chaque interpréteur reste protégé par un GIL mais une telle *features* offre de nouvelles possibilités d'exécution parallèle sans avoir à recourir au *multiprocessing*.\n",
    "\n",
    "Voir aussi ce [post de blog](https://medium.com/hackernoon/has-the-python-gil-been-slain-9440d28fa93d).\n",
    "\n",
    "### Références\n",
    "Pour tout ce qui se rapporte au GIL, voir notamment les publications de David Beazley:\n",
    "* La partie de son blog [dédiée au GIL](http://www.dabeaz.com/GIL/)\n",
    "* Les parties dédiées à la concurrence dans les ouvrages qu'il a (co-)écrits:\n",
    "    * Python Essential Reference\n",
    "    * Python Cookbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libs python\n",
    "`concurrent.futures` semble faire pareil que `multiprocessing` mais avec de meilleures abstractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async IO\n",
    "\n",
    "Les opérations d'I/O sont plus lentes de plusieurs ordres de grandeur qu'un cycle CPU. Un cycle CPU a en effet une durée inférieure à la nanoseconde, les opérations d'I/O sont elles de l'ordre de la dizaine de microsecondes pour les disques SSD et de la millisecondes pour les disques durs et les opérations réseau (voire beaucoup plus pour ces dernières).\n",
    "\n",
    "Ces différences se traduisent d'autant plus en *bottlenecks* de performance (on se dit alors *I/O bound*) que certaines de ces opérations sont bloquantes (*blocking*): l'application est mise en pause par l'OS jusqu'à ce que l'opération soit terminée. S'assurer d'une bonne performance pour des opérations réalisant un grand notre d'opérations I/O est donc un enjeu majeur et peut requérir l'utilisation de techniques spécifiques.\n",
    "\n",
    "### *Blocking* I/O\n",
    "\n",
    "#### *File descriptor*\n",
    "Toute opération I/O fait à un moment usage d'une abstraction appelée *file descriptor*. A chaque ressource I/O ouverte (fichier, répertoire, *pipe*, *socket* réseau, *special file*, etc.) correspond un *file descriptor*. Ainsi et contrairement à ce que le terme pourrait laisser penser, un *file descriptor* ne représente pas que des entités présentes sur le *file system*. A chaque *process* est associée une liste de *file descriptors* correspondant à l'ensemble des ressources I/O ouvertes par ou pour ce *process*. Quand un *process* cherche à lire de ou à écrire des données vers une ressource I/O, il le fait en passant le *file descriptor* de la ressource au *system call* approprié. Les données sont ensuite lues/écrites par le *kernel* pour le compte du *process*, ce dernier n'ayant jamais directement accès à la ressource.\n",
    "\n",
    "Remarque: Une *socket* est une abstraction représentant une extrémité du réseau (*network endpoint*) de laquelle on peut envoyer et recevoir des données. A une *socket* correspond un protocole, une interface réseau (adresse IP) et un port. Localement, les *sockets* fournissent une solution d'*inter-process communication* (IPC).\n",
    "\n",
    "Un *file descriptor* est concrètement un simple entier positif. L'ensemble des *file descriptors* d'un même *process* constituent la *file descriptor table* qui est gérée par le *kernel*. A toutes les entrées de cette table correspondent une entrée dans la table recensant l'ensemble des \"fichiers\" ouverts à l'échelle du système, la *file table*. Tout *process* qui n'est pas un *daemon* est par exemple doté de trois *file descriptors*: `0`, `1` et `2` correspondant respectivement aux flux standards (*standards streams*) `stdin`, `stdout` et `stderr`. Plus généralement, la liste des *file descriptors* d'un *process* est accessible sous `/proc/<PID>/fd`.\n",
    "\n",
    "#### *File descriptor blocking mode*\n",
    "Par défaut, tous les *file descriptors* des systèmes Unix sont créés en *blocking mode*. Cela signifie que certains *system calls* (comme par exemple `read`) sur ces *file descriptors* seront bloquant. Lors de tels appels, le *kernel* va passer le *thread* appelant en état *waiting*/*sleeping* et ne reprendra son exécution qu'une fois l'action réalisée. L'exécution du programme est en attendant suspendue, bloquée, d'où le terme de *blocking* utilisé pour ces opérations.\n",
    "Un appel à `read` sur `stdin` va par exemple suspendre l'exécution du programme jusqu'à ce que des données soient disponibles sur ce flux, c'est à dire jusqu'à ce que l'utilisateur saisisse des caractères à l'aide de son clavier. De même, un appel à `read` sur une *socket* TCP va bloquer le programme jusqu'à ce que l'ensemble des données aient été reçues de la contrepartie de l'autre côté de la connection. \n",
    "\n",
    "Ce blocage est notamment un problème pour les *single-threaded applications* qui ont à effectuer un grand nombre de ces *blocking I/O operations*. Un serveur à un seul *thread* est ainsi obligé d'attendre d'avoir reçu les données de la dernière connection avant de pouvoir traiter la suivante. Ces blocages peuvent également dégrader les performances des applications multithreadés, certains *threads* pouvant avoir à attendre d'autres *threads* bloqués par de l'I/O ou plus simplement, une partie des significative ressources de l'application peuvent se retrouver bloquées par de l'I/O.\n",
    "\n",
    "Deux principales solutions, souvent utilisées ensembles, ont été apportées à ce problème:\n",
    "* Le mode *non blocking* des *file descriptors*\n",
    "* Le multiplexage des I/O *system calls*\n",
    "\n",
    "### *Non-blocking I/O*\n",
    "\n",
    "#### *Non-blocking file descriptor*\n",
    "Le mode d'un *file descriptor* peut être fixé à *non-blocking* (il s'agit concrètement d'ajouter le *flag* `O_NONBLOCK` au *file descriptor*), procéder à un *system call* qui aurait normalement bloqué sur ce *file descriptor* retourne désormais une valeur et un message d'erreur spécial indiquant qu'il \"aurait normalement bloqué\" (`EWOULDBLOCK`).\n",
    "\n",
    "L'existence de cette possibilité, de cette primitive, n'est toutefois pas suffisante pour faire efficacement de l'I/O sur plusieurs *file descriptors*. Un approche naïve consisterait à périodiquement contrôler la disponibilité de données auprès de chaque *file descriptors* et d'attendre un temps jugé suffisant entre deux contrôles. Une telle approche est très inefficaces pour au moins les deux raisons suivantes:\n",
    "* Si les données mettent du temps à arriver, le programme va se réveiller et utiliser des ressources CPU pour rien un grand nombre de fois (*busy-waiting*).\n",
    "* Une fois les données disponibles, elles ne seront traitées qu'au prochain réveil de l'application avec un impact négatif sur le temps de réponse de l'application (*lantency*).\n",
    "\n",
    "#### I/O *multiplexing*\n",
    "Pour répondre au problème posé au paragraphe précédent, le programmeur peut utiliser des *multiplexing system calls* (`select`, `poll`, `epoll`, `kqueue`, etc.) qui permettent de surveiller un ensemble de *file descriptors* jusqu'à ce qu'au moins l'un d'entre eux soit \"prêt\" (*ready*) pour une ou plusieurs opérations qui nous intéressent (lecture, écriture, exception). Le terme *multiplexing* provient du fait qu'on \"traite\" plusieurs *file descriptors* pour différents types d'*I/O operations* avec un seul *system call*.\n",
    "\n",
    "Noter qu'un appel à *multiplexing system call* peut être bloquant (*blocking*) et ce caractère bloquant est le plus souvent indépendant du caractère bloquant ou non des *file descriptors* surveillés. `select` possède par exemple un argument `timeout` qui permet de spécifier combien de temps au maximum on doit attendre qu'au moins un *file descriptor* soit \"prêt\" pour les opérations qu'on surveille. Pendant tout ce temps passé à attendre, l'appel à `select` est bloquant.\n",
    "\n",
    "### *Synchronous* vs. *asynchronous* vs. *blocking* vs. *non-blocking*\n",
    "Il existe un assez grande confusion quant à la distinction certains de ces termes et il est difficile de trouver des définitions à la fois claire et soulignant les relations et les différences entre les différents termes. Je me suis arrêté aux définitions suivantes.\n",
    "\n",
    "Les termes de synchone et d'asynchrone désignent un type de programmation (*(a)synchronous programming*). La programmation asynchrone se distingue de la synchrone en ce que des éléments du programme peuvent s'exécuter en dehors du flot de contrôle (*control flow*) du programme. De plus, l'achèvement d'une tâche asynchrone n'est pas forcément attendu en un point précis de la structure du programme (au quel cas celui-ci bloquerait jusqu'à être notifié de l'événement que constitue la complétion de la tâche). Celà peut être le cas, on des équivalents de `join` pour les tâches exécutées de façon asynchrone (cf. `concurrent.futures`) ou non. Par exemple dans le cas de l'*asyncIO*, l'achèvement d'une tâche asynchrone n'est qu'un événement à prendre en charge par l'*event loop* du programme qui est construite pour ne pas avoir à l'attendre. L'exécution asynchrone d'une tâche suppose qu'elle restitue le contrôle après avoir été lancée. Dans le cas d'opérations d'I/O en particulier, celà nécessite l'utilisation de *non-blocking requests*. C'est pour celà que dans le contexte de l'I/O qui est un des principaux domaine où est utilisée la programmation asynchrone, les termes de *synchrone*/*asynchrone* et de *blocking*/*non blocking* sont utilisés de manière interchangeable ou que pour simplifier, le *synchronous programming* se comprend comme le recours à des requêtes bloquantes (*blocking requests*) là où l'*asynchronous programming* est vue comme se reposant sur l'utilisation de *non-blocking requests*.\n",
    "\n",
    "Comme déjà évoqué, une *blocking request* se définit par le fait qu'elle ne restitue pas le contrôle (*yield the flow of control*) tant que l'operation à effectuer (en général d'I/O) n'est pas terminée. L'exécution du *thread* appelant est ainsi suspendue: il est mis en sommeil par l'OS pendant un certain temps.\n",
    "\n",
    "Cette attente peut ne pas être un problème si le programme ne peut de toute façon rien faire en attendant que l'opération bloquante soit achevée. Pour certains types d'applications comme par exemple les GUI, les applications réseau (*network programming*) comme pour un serveur recevant des requêtes de multiples clients, il peut être intéressant voire indispensable que le programme puisse continuer à progresser sur d'autres tâches tout attendant de recevoir ses données d'un ou plusieurs autres utilisateurs.\n",
    "\n",
    "Deux principales solutions se présentent alors: \n",
    "* Le *multithreading*: par l'utilisation de plusieurs threads, on est toujours garantis qu'une partie du programme n'attendra pas qu'une opération dont elle ne dépend pas se termine pour poursuivre son exécution. Il s'agit de l'option choisie quand le programme recourt à des *blocking requests*, c'est à dire en *synchronous programming*. Cette solution se recontre souvent dans le domaine du *network programming*.\n",
    "* Le recours à l'*asynchronous programming*. Du fait de l'utilisation de *non-blocking requests*, on n'attend jamais de récupérer le *flow of control*. Bien que l'*asynchronous programming* puisse se révéler complexe, il a l'avantage que l'application puisse consister en un seul *thread*, ce qui élimine les problèmes propres à la *concurrency* (rien n'empêche de faire à la fois du l'*asynchronous programming* et du *multithreading*). L'*asynchronous programming* sous-entend l'existence de mécanismes permettant de notifier à l'application l'achèvement des requêtes qu'elle a initié (*signaling*, *callbacks*). Cette solution est souvent privilégiée dans la conception de GUI.\n",
    "\n",
    "Remarque: Retour sur le *non-blocking file descriptor*: Il ne suffit pas de régler ses *file descriptors* à *non-blocking* pour faire de l'asynchrone. On l'a vu, utiliser un *non-blocking file descriptor* oblige à s'enquérir de son état. On peut le faire efficacement (*multiplexing system call*) ou non (*busy-waiting*) mais dans les deux cas on attend, l'exécution du programme étant *de facto* bloquée. On précise que là où les *multiplexing system calls* sont explicitement bloquants, le *busy-waiting* revient au même.\n",
    "\n",
    "### Exemple Python \n",
    "Les exemples suivants sont tirés de [ce post de blog](https://luminousmen.com/post/asynchronous-programming-blocking-and-non-blocking) et s'intéressent à la simple réalisation d'un client envoyant des requêtes et d'un client y répondant.\n",
    "\n",
    "#### *Blocking socket*\n",
    "Client example script:\n",
    "```python\n",
    "# client.py\n",
    "import socket\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "def main():\n",
    "    host = socket.gethostname()\n",
    "    port = 12345\n",
    "\n",
    "    # create a TCP/IP socket\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "        while True:\n",
    "            sock.connect((host, port)) # establish connection with the server\n",
    "            while True:\n",
    "                data = str.encode(sys.argv[1])\n",
    "                sock.send(data)\n",
    "                time.sleep(0.5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    assert len(sys.argv) > 1, \"Please provide message\"\n",
    "    main()\n",
    "```\n",
    "\n",
    "Server example script:\n",
    "```python\n",
    "# server.py\n",
    "import socket\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    host = socket.gethostname()\n",
    "    port = 12345\n",
    "    \n",
    "    # create a TCP/IP socket (blocking socket by default)\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "        # make the socket a listening socket\n",
    "        sock.bind((host, port))  # associate the socket with a specific network interface and port number\n",
    "        sock.listen(5)  # listen for incoming connections, setting backlog to 5 connections\n",
    "        print(\"Server started...\")\n",
    "\n",
    "        while True:\n",
    "            conn, addr = sock.accept()  # block and wait for an incoming connection\n",
    "            # return a new socket object (\"server socket\") and the client's address\n",
    "            # the server socket is used to communicate with the client\n",
    "            print('Connected by ' + str(addr))\n",
    "            while True:\n",
    "                data = conn.recv(1024)  # read whatever data the client sends, blocking\n",
    "                if not data: \n",
    "                    break\n",
    "                print(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "Parmi tous les reproches qu'on peut faire à ce très simple serveur, le plus évident est qu'il n'est capable de traiter qu'un seul client, qu'une seule connection à la fois. De plus, toutes les tentatives de connection en plus de la capacité de la *backlog* (ici 5) seront refusées et leur données ni reçues ni traitées.\n",
    "\n",
    "On peut rendre notre serveur capable d'accepter plus d'une connection à la fois en rendant la *listening socket non-blocking* et en utilisant par exemple `select.select`.\n",
    "\n",
    "```python\n",
    "import select\n",
    "import socket\n",
    "\n",
    "\n",
    "def main():\n",
    "    host = socket.gethostname()\n",
    "    port = 12345\n",
    "\n",
    "    # create a TCP/IP socket\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "        sock.setblocking(False) # make listening socket non-blocking\n",
    "        sock.bind((host, port))  # associate the socket with a specific network interface and port number\n",
    "        sock.listen(5)  # listen for incoming connections, setting backlog to 5 connections\n",
    "        print(\"Server started...\")\n",
    "\n",
    "        inputs = [sock] # waitable objects from which we expect to read\n",
    "        outputs = []\n",
    "\n",
    "        while inputs:\n",
    "            # block until at least one of the monitored objects is ready (timeout=None)\n",
    "            readables, writables, exceptionals = select.select(inputs, outputs, inputs)\n",
    "\n",
    "            for readable in readables:\n",
    "                if readable is sock:\n",
    "                    conn, addr = readable.accept() # listening socket waiting for a connection, non-blocking \n",
    "                    inputs.append(conn) # should we make conn explicitly non-blocking?\n",
    "                else:\n",
    "                    data = readable.recv(1024) # server socket ready for reading\n",
    "                    if data:\n",
    "                        print(data)\n",
    "                    else: # if no data, client socket has been closed\n",
    "                        inputs.remove(readable) # server socket won't be monitored anymore\n",
    "                        readable.close() # close server socket\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "```\n",
    "\n",
    "En rendant la *listening socket non blocking* et en s'aidant de `select`, on est désormais capable de traiter plusieurs connections avec un seul *thread* et de réduire les temps d'attente au strict minimum: dès qu'une *server socket* est prête pour être lue, elle est traitée. On remarque que le `while inputs` ressemble à une *event loop* qu'on trouverait dans un framework d'*async I/O*.\n",
    "\n",
    "\n",
    "Une autre solution pour implémenter un serveur capable de traiter de multiples requêtes est de confier l'exécution de chacune d'entre elles à un *thread* (*multithreading*): chaque *thread* est en charge d'une *server socket*, les opérations bloquantes ne bloquent alors que le thread concerné et non l'ensemble de l'application.\n",
    "\n",
    "```python\n",
    "import select\n",
    "import threading\n",
    "import socket\n",
    "\n",
    "\n",
    "def handler(client):\n",
    "    while True:\n",
    "        data = client.recv(1024)\n",
    "        if data:\n",
    "            print(data)\n",
    "        \n",
    "    client.close()\n",
    "\n",
    "def main():\n",
    "    host = socket.gethostname()\n",
    "    port = 12345\n",
    "\n",
    "    # create a TCP/IP socket\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "        sock.bind((host, port))  # associate the socket with a specific network interface and port number\n",
    "        sock.listen(5)  # listen for incoming connections, setting backlog to 5 connections\n",
    "        print(\"Server started...\")\n",
    "\n",
    "        while True:\n",
    "            client, addr = sock.accept()\n",
    "            threading.Thread(target=handler, args=(client,)).start()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "On remarque que la *listening socket* n'a pas à être non bloquante.\n",
    "\n",
    "On peut évidemment implementer la même idée non pas avec de *threads* mais avec des *processes*, l'arbitrage faisant mettant en balance les inconvénients classiques de chaque méthode: *multithreading* (problèmes de synchronisation) vs. *multiprocessing* (IPC, coût plus élevé).\n",
    "\n",
    "La solution consistant à faire s'exécuter chaque tâche à exécuter concurrement dans un *thread* connait cependant des limites (qui arrivent d'autant plus rapidement si on a choisi d'utiliser des *processes*):\n",
    "* Dans le cas de Python on rappelle que de fait de la présence d'un GIL, l'utilisation du *multithreading* n'apporte pas de réelle concurrence. Les gains de performance sont d'autant plus visibles que les tâches exécutées dans les différents *threads* sont *I/O bound*.\n",
    "* La gestion d'un *thread* a un coût. Ces coûts (empreinte mémoire, temps de *scheduling*, temps de *context switch*) peuvent devenir substantiels avec un très grand nombre de *threads*.\n",
    "\n",
    "Une dernière alternative est l'*asyncIO* qui consiste à restructure sa *multithreaded I/O bound application* en un *asynchronous event-handling system* où une *event loop* gère un ensemble d'événements produits par l'exécution de tâches asynchrones qu'elle dispatche vers une large collection de *handlers*.   \n",
    "\n",
    "## L'AsyncIO\n",
    "L'*asynchronous I/O* (*asyncIO*) est une technique / un style de programmation (*event-driven programming*) utilisée pour le traitement (*handling*) de multiples tâches *I/O bound*. Cette technique est souvent utilisée dans le développement d'applications web (*networked applications*) et leur permet notamment de pouvoir aisément gérer un très grand nombre de requêtes (*scale*). \n",
    "\n",
    "Dans l'*asyncIO*, une *I/O operation* est modélisée comme un événement. Par exemple, le fait que des données aient été reçues sur une *socket* et que celle-ci soit prête à être lue est vu comme un \"receive\" *event* qui est ensuite dispatché vers le traitement (*handler*) approprié. Le traitement d'un *event* par un *handler* est une tâche (*task*). \n",
    "\n",
    "### *Event loop*\n",
    "Une application développée pour faire de l'*asyncIO* s'organise autour d'une *(main) event loop*. L'*event loop* correspond au *scheduler* des tâches à effectuer par l'application. C'est elle qui se charge de collecter les événements auprès de l'OS, de placer les tâches associées dans une file (*queue*) et de les exécuter. L'*event loop* tourne le plus souvent aussi longtemps que vit l'application. \n",
    "\n",
    "Il s'agit du plus haut niveau de contrôle dans l'application. C'est à l'*event loop* que les tâches (asynchrones) restituent le contrôle quand elles en arrive à lancer une *I/O operation*. Si l'*event loop* se retrouve à exécuter des tâches qui conservent le contrôle trop longtemps (tâche *CPU bound*, tâche réalisant un *blocking call*), l'ensemble des autres tâches doit attendre pour s'exécuter et on perd le principal intérêt de l'*asyncIO*. \n",
    "\n",
    "Les application *asyncIO* sont en effet le plus souvent ***single threaded*** c'est à la fois leur force puisque cela les préserve de la complexité du *multithreading* mais aussi leur faiblesse puisque **leurs tâches ne sont en réalité pas exécutées concurremment** et leur performance peut très vite souffir si elles ne sont pas programmées correctement. Il faut donc être très vigilant au code employé pour l'exécution des tâches afin que celles-ci ne monopolisent pas le contrôle. En particulier, il est recommandé de ne développer de telles applications qu'avec un écosystème de librairies compatibles. Avec la librairie `asyncio` de Python par exemple vient tout un écosystème de librairies spécialisées compatibles: `aiohttp`, `aiomysql`, etc.\n",
    "\n",
    "Par la restitution volontaire du contrôle qu'elle implique, on voit que la programmation asynchone est une forme de *cooperative multitasking* se faisant dans le *user space* puisque le scheduling des tâches est fait par l'application. En programmation asynchrone, le transfert du contrôle de l'exécution se fait en des points de bascule déterminés et non à intervalles indéterminés comme en programmation synchrone où c'est l'OS qui bascule le contexte d'un *thread* et potentiellement d'une tâche à l'autre sans aucun considération du point où l'application se trouve dans son exécution, cette dernière n'ayant aucun contrôle sur la bascule.\n",
    "\n",
    "#### Modélisation des tâches\n",
    "Dans le cadre de l'exécution concurrente de différentes tâches asynchrones, chaque tâche peut être vue comme une première partie \"conventionnelle\" s'exécutant de façon synchrone où la tâche détient le contrôle, un appel à une fonction s'exécutant asynchrone où la tâche rend le contrôle et une partie dédiée à traiter du résultat de la partie asynchrone. \n",
    "\n",
    "De telles tâches peuvent être implémentées et exécutées de différentes façons:\n",
    "* De façon synchrone: le code va bloquer, l'ensemble de la tâche est exécutée dans son propre *thread*. Le *multithreading* permet alors de préserver la performance de l'application, gains potentiellement plus plus limités en Python en présence d'un GIL. Certains langages offrent la possibilité de *scheduler* ces *threads* (chose qui n'aurait que peu de sens avec un GIL comme avec Python), l'application décidant alors du transfer du contrôle d'un *thread* à l'autre et réalisant un *user level cooperative multitasking*.\n",
    "* La tâche est implémentée avec une coroutine, l'appel ordinairement bloquant donne lieu à une restitution du contrôle. Une fois la partie asynchrone exécutée, l'*event loop* récupère l'évènement associé et ajoute la coroutine interrompue à sa *task queue*. La coroutine sera réentrée à sa sortie de la *task queue*. L'utilisation de coroutines qui permettent une restitution du contrôle à certains moments précis de l'exécution correspond à une réalisation de *user level cooperative multitasking*. L'utilisation de coroutines et d'une *event loop* ne permet pas de faire s'exécuter les tâches de façon concurrentes: m'application reste *single-threaded*.\n",
    "* On dissocie l'exécution de la tâche asynchrone de la suite à donner au résultat de son exécution. Soit on récupère l'événement de complétion de l'exécution et l'*event loop* se charge de le dispatcher au bon *handler* et d'exécuter ce dernier, soit l'évènement vient associé à un *callback* qui sera exécuté par l'*event loop*. Tout ce qui est exécuté par l'*event loop* ne lui restitue le contrôle qu'après s'être entièrement exécuté.  \n",
    "\n",
    "Remarque: les *futures* (appelée *promises* dans d'autres langages*) sont un object représentant l'exécution de code asynchrone (il sont une méthode permettant de savoir si l'exécution est finie, de récupérer son résultat, etc.). De tels objets peuvent notamment être utilisés (par l'*event loop*) par exemple pour monitorer les différentes tâches asynchrones en cours d'exécution et réagir à leur complétion. Python permet par exemple d'attacher un *callback* à une *future*.  \n",
    "\n",
    "#### Exemple d'*event loop*: l'*event loop* JavaScript\n",
    "Cette partie est inspirée de cette [vidéo](https://www.youtube.com/watch?v=8aGhZQkoFbQ&ab_channel=JSConf) particulièrement claire.\n",
    "\n",
    "En JavaScript, l'exécution d'une tâche asychrone se déroule schématiquement comme suit: exécution de code synchrone avant l'appel à la fonction asynchrone, appel de la fonction asynchrone à laquelle on passe un *callback*, exécution de la partie asynchrone de la tâche à l'extérieur du *runtime* JavaScript, exécution du *callback*. \n",
    "\n",
    "Pour être plus précis, l'*event loop* gère très schématiquement une *call stack* et une file de tâches/*callbacks* (*task queue*/*callback queue*) à exécuter. La *call stack* correspond à la *call stack* d'une tâche. Dès qu'elle est vide, celà signifie qu'une tâche a fini de s'exécuter, l'*event loop* lance alors la prochaine tâche présente dans la file. Une tâche asynchrone se termine par l'appel d'une fonction asynchrone à laquelle est passé un *callback*. L'appel de cette fonction clot l'exécution de la tâche dans le *runtime* JavaScript, la partie asynchrone s'exécutant ailleurs. Une fois achevée, le *callback* est ajouté à la *task queue* du *runtime* JavaScript et y sera exécuté quand viendra son tour. L'événement (fin de l'exécution asynchrone) amène son propre *handler* (le *callback*). \n",
    "\n",
    "Afin de pouvoir traiter efficacement un grand nombre d'opérations I/O, l'asyncIO a deux piliers: \n",
    "\n",
    "L'asynchronie: Les événements associés à un file descriptor se produisent en dehors de flot d'exécution du programme: l'application ne réagit qu'à des événements se produisant sur ces file descriptors mais n'attend pas jusqu'à ce qu'un événement se produise. On n'est alors plus dans le cas où interagir avec un seul *file descriptor* peut suffire à bloquer le programme. On n'est alors plus obligé pour s'assurer de ne pas bloquer l'ensemble du programme et donc de s'assurer une performance acceptable, de recourrir à l'utilisation d'un *thread* par opération. Le *main thread* du programme n'étant potentiellement jamais bloqué et ne se focalisant que sur le traitement d'événements, celui-ci peut suffire à assurer une bonne performance à l'application.\n",
    "\n",
    "Pour arriver à ce résultat, l'async IO s'appuie sur l'utilisation de non blocking system calls pour ses opérations d'I/O: ces appels restituent (*yield*) immédiatemment le contrôle au code appelant qui peut poursuivre son exécution. \n",
    "\n",
    "Dans un programme développé avec de l'*asyncIO*, c'est en général le main thread du programme qui prend en charge et gère le traitement des événements. On parle le plus souvent de (*main*) *event loop*. Cette dernière gère une file (*queue*) d'événement dont il faut exécuter le traitement (le plus souvent une *callback function*). Elle peut les exécuter elle même où les faire s'exécuter dans des threads ou processes séparés. \n",
    "\n",
    "On voit apparaître la notion de cooperative multitasking: les taches correspondant à l'exécution du programme qui dépendent d'un événement (le code de la tâche va à un moment dépendre du résultat d'une opération qui ne sera disponible que dans le futur) restituent (*yield*) le contrôle (au main thread qui correspond à l'exécution de l'event loop par exemple) au moment où elles sont contraintes d'attendre (*await*) la survenue de celui-ci. A cette restitution volontaire du contrôle par les tâches dépendant d'un événement et le fait que la main loop soit dans une postition qui lui permette de scheduler les différentes tâches justifient qu'on parle de l'asyncIO comme d'une forme de cooperative multitasking. \n",
    "\n",
    "Attention: Quand on recourt un framework d'asyncIO, il faut s'assurer que les package qu'on souhaite utiliser sont compatibles avec une utilisation asynchrone (notamment ne pas faire *blocking calls*) sous peine d'anihiler tout ou partie des gains apportés par ce style de programmation. \n",
    "\n",
    "On a vue que pour préserver la performance de l'application, une tâche dépendant d'un événement doit pouvoir restituer le contrôle au code qui l'a lancé. A l'opposé, il faut qu'une fois l'événement attendu survenu, pouvoir reprendre l'exécution là où elle s'était interrompue. On reconnait ici la notion de coroutine qui sont des *subroutines* (fonctions) en plusieurs points desquelles ont peut entrer, sortir ou reprendre l'exécution. \n",
    "\n",
    "Il est ainsi possible d'illustrer le fonctionnement de l'asyncIO en se reposant sur des générateurs Python (il serait en revanche compliqué de développer une vraie applications d'asyncIO avec des générateurs) dont le *statement* `yield` explicite bien la notion de restitution du contrôle. L'utilisation de générateurs et des de leurs méthodes `next` et `send` permet d'implémenter des coroutines en Python. Dans le contexte de Python, on appelle en fait coroutine les fonction destinées à s'exécuter de façon asynchrone (c'est à dire dont l'exécution dépend à un moment d'un événement). Ces fonctions particulières peuvent depuis Python 3.5 être créées à l'aide de statements dédiés, l'ancienne syntaxe restant intéressante du point de vue de ce quelle exprime, notamment la restitution du contrôle via le *statement* `yield from`):\n",
    "\n",
    "```python\n",
    "# New coroutine / asynchronous function syntax available in Python 3.5+\n",
    "async def my_coroutine():\n",
    "    # ...\n",
    "    await another_coroutine()\n",
    "    # ...\n",
    "\n",
    "\n",
    "# Old syntax, generator-based coroutines are deprecated since Python 3.8 and will be removed in Python 3.10\n",
    "import asyncio\n",
    "\n",
    "@asyncio.coroutine\n",
    "def my_coroutine():\n",
    "    # ...\n",
    "    yield from another_coroutine()\n",
    "    # ...\n",
    "```\n",
    "\n",
    "Remarque: Le *statement* `yield from` a été introduit en Python 3.3 avec d'autres objectifs (permettre à un générateur de déléguer partie de ses opérations à un autre générateur) mais a servi de base à l'implémentation de l'asyncIO en Python. Le package `asyncio` a été ajouté à la librairie standard de Python 3.4.\n",
    "\n",
    "Comme dit en introduction l'*asyncIO* est une technique de programmation dont l'implémentation fait intervenir différents composants: main loop, callbacks/handlers, event demultiplexer (composant chargé de collecter les événements, par exemple `select` qui retourne les file descriptors prêts pour écriture/lecture). Il existe en Python comme dans d'autres languages, différents frameworks d'asyncIO qui viennent abstraire le plus possible ces détails d'implémentation afin de rendre efficiente l'utilisation de l'asyncIO et de minimiser l'impact sur la lisibilité du code. `asyncio` est ainsi un framework d'asyncIO intégré à la librairie standard de Python (mais il en existe d'autres, `curio`, `trio` par exemple) autour duquel se développe un écosystème de package compatibles comme `aiohttp`, `aiomysql`, etc.\n",
    "\n",
    "\n",
    "Le code d'une application faite avec asyncio est potentiellement très simple. On remarque notamment que toute la logique chargée de la collecte des événements et de la reprise de l'exécution des coroutines est cachée à l'utilisateur. L'implémentation de toute la logistique exigée par l'exécution asynchrone/la gestion d'événement peut d'ailleurs être assez complexe. \n",
    "\n",
    "Remarque: Dans le *design* d'applications *event-driven*, on distingue semble-t-il deux grands modèles/*design pattern*, le *Reactor* et le *Proactor*. Le premier est employé que l'OS sous-jacent supporte l'asynchronie et la gestion d'événements ou pas. Dans ce dernier cas là, on trouvera forcément à un moment dans un mécanisme (bloquant) de pooling permettant de récupérer les événements. Le second est semble-t-il employé quand l'OS supporte l'asynchronie et fournit des systèmes de notifications d'événements (*event notification*) (voir par exemple `eventfd2` pour le kernel Linux).\n",
    "\n",
    "`asyncio` n'est pas l'unique moyen d'exécuter des tâches de façon asynchrone. Les librairies `multiprocessing` et `concurrent.futures` de la librairie standard rendent en particulier cela possible. Il n'y a ici ni `async`/`await`, ni coroutines, mais seulement d'éventuels *callbacks* et des objets spéciaux appelés *futures* (*promisses* dans d'autres language) représentant et permettant de suivre le status de chaque exécution asynchrone. La logistique propre à l'asynchronisme apparaît ici plus explicitement dans le code. On prend aussi la mesure d'un des avantages d'`asyncio`: l'utilisation de coroutines permet d'avoir un code en apparence très proche et donc presque aussi facile à lire que son équivalent synchrone. Dans le cas de l'asynchronime fait avec `multiprocessing`/`concurrent.futures`, ce qui se trouverait après `await` dans `asyncio` doit être placé dans un *callback*. \n",
    "\n",
    "Remarque: Peut on dire que dans un framework comme `asyncio`, les callbacks correspondent à la réentrée dans la coroutine interrompue ? Ou pas de callback: on associe file descriptor et coroutine et quand on récupère un événement pour ce file decriptor (soit par l'application via pooling soit par l'OS), on réentre la coroutine.\n",
    "\n",
    "### Quand est-il avantageux d'utiliser de recourir à l'asyncIO ?\n",
    "Plus généralement que l'asyncIO, la programmation asynchrone est privilégiée sur la programmation synchrone dans l'implémentation de l'exécution concurrente de tâches si:\n",
    "* Les tâches à réaliser sont nombreuses et qu'il est très probable qu'il y en aura toujours une à faire progresser.\n",
    "* Les tâches rentiennent le contrôle pendant de longues périodes, temps pendant lequel dans un programme synchrone, les autres tâches ne peuvent pas progresser. \n",
    "* Les tâches à réaliser sont largement indépendantes: une tâche n'a pas besoin d'en attendre une autre pour s'exécuter (pas de besoin de synchronisation, sinon utiliser des *threads*).\n",
    "\n",
    "Les premières tâches qualifiées par ces critères venant à l'esprit sont les opérations I/O d'où une utilisation généralisée de la programmation asynchrone pour les applications ayant à gérer un très grand nombre d'*I/O operations* comme les *networked operations* au point qu'on finisse par parler d'asyncIO. \n",
    "\n",
    "Quelle style de programmation concurrente finalement privilégier **en Python** suivant la nature des tâches à prendre en charge: \n",
    "* Tâches principalement *CPU bound*: ***multiprocessing***. La présence du GIL ne permet pas de gain pour ce type de tâche par recours au *multithreading*. Ne pas oublier que par défaut, la programmation asynchrone est *single threaded*: une tâche *CPU bound* va monopoliser le contrôle sur une période pendant laquelle les autres tâches ne pourront pas progresser sauf à être réalisée dans un *process* séparé. \n",
    "* Tâches d'I/O relativement rapides et pas trop nombreuses: ***multithreading***. Sous-entend d'allouer un *thread* à chaque tâches afin de ne pas être pénalisé par les *blocking I/O operations*. Comme on l'a vu, quand le nombre de tâches à exécuter devient trop grand, les coûts associés à la gestion de d'autant de *threads* augmente et peu devenir pénalisant.\n",
    "* Tâches d'I/O relativement lentes et nombreuses: ***asyncIO****.\n",
    "\n",
    "En cas de mixité des types de tâches, il est notamment possible de combiner *asyncIO* et *multiprocessing*. Voir notamment ce talk: [John Reese - Thinking Outside the GIL with AsyncIO and Multiprocessing](https://www.youtube.com/watch?v=0kXaLh8Fz3k&feature=youtu.be&t=0s&ab_channel=PyCon2018).\n",
    "\n",
    "\n",
    "TODO\n",
    "Code samples\n",
    "Beazley\n",
    "Notes asyncIO notebook\n",
    "ordonner\n",
    "doc Python asyncio\n",
    "Repasser sur ce qu'on a lu / vu\n",
    "Glossaire: event loop, futures, callbacks, coroutine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object task at 0x7f171d0c5af0>\n",
      "<generator object task at 0x7f171d0c5830>\n"
     ]
    }
   ],
   "source": [
    "def task():\n",
    "    n = random.randint(1, 5)\n",
    "    for i in range(n):\n",
    "        yield i \n",
    "    \n",
    "def b():\n",
    "    n_tasks = 2\n",
    "    for _ in range(n_tasks):\n",
    "        q.append(task())\n",
    "        \n",
    "    while q: \n",
    "        print(q.pop())\n",
    "    \n",
    "b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import queue\n",
    "\n",
    "q = queue.deque()\n",
    "\n",
    "def task():\n",
    "    n = random.randint(1, 5)\n",
    "    for i in range(n):\n",
    "        yield i \n",
    "        \n",
    "def main():\n",
    "    n_tasks = 2\n",
    "    for task_id in range(n_tasks):\n",
    "        q.appendleft((task_id + 1, task()))\n",
    "        \n",
    "    while q:\n",
    "        tsk_id, tsk = q.pop()\n",
    "        try:\n",
    "            print(f'Task #{tsk_id}: Executing step #{next(tsk)}')\n",
    "            q.appendleft((tsk_id, tsk))\n",
    "        except StopIteration:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task #1: Executing step #0\n",
      "Task #2: Executing step #0\n",
      "Task #1: Executing step #1\n",
      "Task #2: Executing step #1\n",
      "Task #1: Executing step #2\n",
      "Task #1: Executing step #3\n",
      "Task #1: Executing step #4\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function new_event_loop in module asyncio.events:\n",
      "\n",
      "new_event_loop()\n",
      "    Equivalent to calling get_event_loop_policy().new_event_loop().\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(asyncio.new_event_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque: on peut définir une coroutine sans `await`, `async` ne sert qu'à définir une fonction pouvant être exécutée de façon asynchrone par une event loop. Si elle ne comprend pas de `await` celà veut simplement dire qu'elle ne retournera le contrôle à l'event loop qu'une fois qu'elle se sera complètement exécutée. Attention à ne pas y mettre du code CPU bound qui pourrait retarder d'autant l'exécution de toutes les autres tâches gérées par l'event loop. Dans ce cas, faire s'exécuter la fonction dans un thread/process séparé.\n",
    "\n",
    "`await`: si inclus dans le corps d'une coroutine enregistré comme Task dans la main loop, point auquel le contrôle est repassé à la main loop, l'exécution de la coroutine par le main thead n'était pas reprise avant que l'awaitable n'a pas terminé (autre coroutine/Task, future). Si on `await` un awaitable sans qu'il appartienne à une coroutine gérée par la main loop, le programme va bloquer jusqu'à complétion de l'awaitable.\n",
    "\n",
    "De même que `yeild` doit se comprendre comme *yield back the flow of control*. `await` doit de comprendre comme *await completion of coroutine/task/future*. Soit `await` est dans le corps d'une coroutine gérée par la main loop et son appel va alors restituer le contrôle à celle ci. Soit `await` est appelée par le main thread, n'a donc personne à qui restituer le contrôle et va se comporter de manière bloquante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task #1: Doing things before I/O\n",
      "Task #1: Doing things after I/O\n",
      "Task #2: Doing things before I/O\n",
      "Task #2: Doing things after I/O\n",
      "non_concurrent_main executed in 0.80 seconds.\n",
      "Task #1: Doing things before I/O\n",
      "Task #2: Doing things before I/O\n",
      "Task #1: Doing things after I/O\n",
      "Task #2: Doing things after I/O\n",
      "concurrent_main executed in 0.60 seconds.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def task(id_, delay):\n",
    "    print(f'Task #{id_}: Doing things before I/O')\n",
    "    await asyncio.sleep(delay*0.1)\n",
    "    print(f'Task #{id_}: Doing things after I/O')\n",
    "\n",
    "# Remarque: async def décrit une coroutine function dont l'appel retourne un coroutine object qui est awaitable\n",
    "\n",
    "async def no_await():\n",
    "    time.sleep(0.1)\n",
    "\n",
    "async def non_concurrent_main():\n",
    "    tasks = [task(1, delay=2), task(2, delay=5), no_await()]\n",
    "    \n",
    "    for tsk in tasks:\n",
    "        await tsk\n",
    "\n",
    "async def concurrent_main():\n",
    "    tasks = [asyncio.create_task(task(1, delay=2)), asyncio.create_task(task(2, delay=5)), no_await()]\n",
    "    # coroutine are added to the main loop to be scheduled and run as asyncio.Tasks\n",
    "    # await ne suffit pas: on rend la main à un scheduler, il faut y être attaché\n",
    "    # voir aussi asyncio.wait et asyncio.as_completed\n",
    "    for tsk in tasks:\n",
    "        await tsk\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "for main_func in (non_concurrent_main, concurrent_main):\n",
    "    start_time = time.perf_counter()\n",
    "    await main_func() # blocking\n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"{main_func.__name__} executed in {elapsed:0.2f} seconds.\")\n",
    "\n",
    "# Different de await une coroutine (blocking) qu'une asyncio.Task (non blocking)\n",
    "# Normalement on peut s'en tirer sans aucune référence à l'envent loop: c'est transparent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design patterns propres au concurrency programming:https://en.wikipedia.org/wiki/Software_design_pattern#Concurrency_patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda Python 3.8.5",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
