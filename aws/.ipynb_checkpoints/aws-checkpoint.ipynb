{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liens:\n",
    "\n",
    "GK4502 Booster\n",
    "\n",
    "* [Zoom](http://meet.gklearn.fr/d141037)\n",
    "* [Emargement (et accès aux supports)](https://emargement.gklearn.fr/portail.php) - Mdp: ipatef\n",
    "* [Support de cours](https://bookshelf.vitalsource.com/#/)\n",
    "* [Labs](https://fr-gk-emea.qwiklabs.com/)\n",
    "* Examens blancs:\n",
    "    * [LMS GKLearn](https://lms.gklearn.online/) - Id: email pro, Mdp: Booster@123\n",
    "    * [Examtopics](https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate-saa-c02/view/) - Code examen: SAA-C02\n",
    "* [Site de certification](https://www.aws.training/certification) - Voucher: AWAV720C8D19 (Expiration: 01/12/2021)\n",
    "* [AWS official documentation index](https://docs.aws.amazon.com/index.html)\n",
    "* [Blog Jayendra Patil](https://jayendrapatil.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 1: Introduction\n",
    "\n",
    "### AWS Well-Architected Framework\n",
    "[AWS Well-Architected Framework white paper](http://d1.awsstatic.com/whitepapers/architecture/AWS_Well-Architected_Framework.pdf)\n",
    "\n",
    "Les 5 piliers du AWS Well-Architected Framework:\n",
    "* Security\n",
    "* Reliability (résiliance): notamment on automatise la configuration pour éviter les erreurs. Solution à construire minimum multi-AZ, voire multi-Regions. \n",
    "* Cost optimization\n",
    "* Operational excellence\n",
    "* Performance efficiency\n",
    "\n",
    "Remarque: Les AZ d'une même Region sont connectées entre elles à l'aide d'une infrastructure réseau privée. Chaque Region est connectée avec une variété d'ISP mais sont aussi connectées entre elles via une *global infrastructure backbone* privée qui procure une plus faible latence et de façon plus fiable que d'en passer par l'internet public pour les communications inter-Region. Les *edge locations* consistue un réseau de plus de 190 *points of presence* permettant de délivrer du contenu au plus près des utilisateurs et donc avec la plus faible latence possible. Les *edge locations* supportent notamment les services CloudFront et Route 53. \n",
    "\n",
    "*Fully managed* implique aussi le plus souvent *high availability*, *resilient* et *scalable*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 2: The simplest architectures\n",
    "\n",
    "### AWS S3\n",
    "[AWS S3 official documentation](https://docs.aws.amazon.com/s3/index.html)\n",
    "\n",
    "Fully managed service, serverless (à aucun moment on a à s'occuper de provisionning de machines), region scoped. \n",
    "\n",
    "Blob: binary large object. S3 \n",
    "Réplication: au sein de la région, au moins une fois par AZ\n",
    "\n",
    "Taille maximale d'un objet: 5TB\n",
    "\n",
    "S3 est particulièrement adapté aux cas avec une grande diversité de données, de nombreux utilisateurs avec des pics dans les accès aux données (fully managed) mais toujours dans des configurations WORM (Write Once Read Many). S3 n'est en revanche la solution la plus adatée aux données changeant fréquemment ou demandant un block storage. S3 n'est pas non plus un service d'archivage ou de stockage de long-terme.\n",
    "\n",
    "`https://[bucket-name].s3.amazonaws.com/[object-key]`: on accède au contenu par requêtes HTTP(S), full REST (POST, GET, PUT, etc.). Il y a génération d'un alias DNS / L'URI via laquelle la resource est requêtable/accessible contient le bucket name d'où l'exigence qu'il soit unique worldwide.\n",
    "\n",
    "Remarque: Chaque object stocké sur S3 se retouve associé à une unique URL HTTP dont on exige qu'elle soit aussi *DNS compliant*. Une conséquence est qu'un site web statique peut être directement hébergé sur S3 qui offre une fonctionnalité dédiée (avec les avantages qu'offre S3: *managed service*, *highly available* et *low cost*). Ce cas particulier est aussi un exemple de cas où donne un accès public au données stockées. \n",
    "\n",
    "### Contrôle d'accès\n",
    "\n",
    "#### *Block all public access*\n",
    "Le contenu d'un bucket est privé par défaut. L'option de bucket \"block all public access\" permet de bloquer tout accès au public et notamment les permissions accidentellement données. Doit être désactivée si on souhaite donner accès direct aux ressources du bucket à des utilisateurs publics non authentifiés.\n",
    "\n",
    "#### *IAM policies, bucket policies, ACLs*\n",
    "Le contrôle de l'accès à un bucket peut se faire à l'aide d'IAM policies, de bucket policies ou une combinaison des deux: \n",
    "* Le principal dans les IAM policies est un IAM user, group ou role. Une IAM policy permet de définir potentiellement pour chaque service AWS (et pas seulement S3) l'ensemble des ressources auquel le principal a accès. Dans le cas de S3, l'IAM policy s'appliquera à tous les buckets.\n",
    "* Une bucket policy est attachée à un bucket particulier et s'applique à tous ses objets (il n'existe pas d'object policy, quoique le champ `Resource` d'une bucket policy permet de gérer les permissions au grain prefix). Une bucket policy attache des permissions sur des ressources du buckets à une liste de principals (entités IAM).\n",
    "\n",
    "L'IAM policy répond à la question \"qu'a le droit de faire tel user/membre de tel user group/user assumant tel role sur AWS?\" là où la bucket policy répond à \"qui a le droit de faire quoi sur ce bucket et ses objets?\". Les IAMs policies ont l'avantage d'avoir une gestion centralisée au niveau du service IAM (en évitant d'avoir des policies dispersées entre S3 et IAM), une policy pouvant éventuellement consolider de multiples buckets policies. A l'inverse, on peut préférer conserver les politiques d'accès à S3 au niveau de S3 dans un ensemble de bucket policies. Les documents JSON des buckets policies ont aussi l'avantage de pouvoir être plus gros que ceux des IAM policies.\n",
    "\n",
    "Remarque: Le JSON décrivant une IAM *bucket policy* ne peut pas dépasser 20KB contre 2KB, 5KB et 10KB pour les IAM policies attachées à des users, groups et roles respectivement.\n",
    "\n",
    "Quand utilisés en combinaison, la permission la plus restrictive de la réunion de toutes les permissions est appliquée sachant que prévaut un deny implicite, s'il y a un explicit deny, la requête est refusée, sinon et s'il existe au moins un explicit allow, la requête est acceptée. IAM et buckets policies sont écrites en JSON dans le AWS access policy language.\n",
    "\n",
    "Les Access Control Lists (ACLs) sont un mécanisme historique de contrôle d'accès sur S3 qui précèdent l'apparition des policies. Les permissions accordées sont moins fines que les policies et les principals auxquels sont accordés les permissions ne sont par définitions pas des IAM users/groups/roles mais des comptes AWS (AWS Account IDs) ou des \"groupes\" comme l'ensemble des comptes AWS, l'ensemble des utilisateurs AWS (pas forcément authentifiés), etc. Les ACLs sont écrites en XML et peuvent être attachés à des objets particuliers ou des buckets. A la création d'une ressource (bucket ou object), il est toujours créé une ACL par défaut qui donne à l'owner de la ressource un contrôle total sur celle-ci. Tout ce qui peut être fait avec des ACls peut être fait avec des policies mais l'inverse n'est pas vrai. Les policies ayant l'avantage de pouvoir consolider en une seule policy ce qui demandait plusieurs ACLs dispersées au niveau de chaque ressource.\n",
    "\n",
    "https://docs.aws.amazon.com/AmazonS3/latest/dev/how-s3-evaluates-access-control.html\n",
    "\n",
    "#### S3 Access Point \n",
    "Un S3 Access Point est un hostname unique (alias DNS) permettant d'accéder à des données stockées sur S3 et auquel est attaché un jeu de permissions. Ex d'usage: permet d'avoir un point d'accès (alias DNS) par groupe d'application: ventes, marketing. On y accède aussi bien depuis le réseau public (Internet, même si le bucket est privé, on peut y accéder via HTTPS via l'internet public) ou depuis un VPC (VLAN privé). \n",
    "\n",
    "Quand on utilise un VPC, l'accès à S3 peut aussi se contrôler au niveau du VPC S3 Endpoint si on en utilise un https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies-vpc-endpoint.html. Voir aussi ACLs.\n",
    "\n",
    "### Versioning\n",
    "Le versioning est une *bucket-level option* et influence la façon dont S3 gère les requêtes PUT et DELETE faite sur un même objet. Si le versioning est activé pour un bucket donné est activé, alors tous ses objets seront versionnés. \n",
    "\n",
    "Dans un *bucket* où le versioning est désactivé, une requete PUT faite sur un objet déjà existant va l'écraser, sa version antérieure ne ouvant être récupérée. Une requête DELETE faire sur un objet déjà existant va le supprimer de façon irréversible. permet entre autre de ne pas se retrouver sans rien si on supprime malencontreusement un fichier.\n",
    "\n",
    "Dans un *bucket* où le versioning est activé, une requete PUT faite sur un objet déjà existant va simplement en créer une nouvelle version (la `Version` étant un attribut de l'objet et permettant d'en identifier toutes les versions de façon unique). Une requête DELETE faite sur un objet déjà existant ne va pas l'effacer (mais un GET sur cet objet reverra quand même une erreur) mais introduire un objet spécial appelé *delete marker* comme dernière version de l'objet. Toutes les versions de l'objet sont ainsi préservées et une restauration est possible. Effacer une objet versionné de façon irréversible sous-entend d'en supprimer explicitement chaque version. Si un objet versionné expire dans le cadre de lifecycle management policies, l'objet n'est pas effacé, seul le *delete marker* est ajouté. L'expiration peut cependant se gérer au niveau des version (dites non-concurrentes) qui sont ensuite définitivement supprimées.\n",
    "\n",
    "Du point de vue du *versioning*, un *bucket* ne peut se trouver que dans 3 états: versioning activé, désactivé ou suspendu. Une fois le versioning activé, un bucket ne peut plus retourner dans l'état de versioning désactivé mais seulement dans l'état de versionning suspendu. L'activation comme la suspension du versioning m'impacte que la façon dont S3 gère les **futures** requêtes PUT et DELETE faites sur les objets du buckets, que ceux-ci existent déjà ou non au moment du changement de statut.\n",
    "\n",
    "### S3 Object Lock\n",
    "Le S3 Object Lock est une fonctionnalité s'activant au niveau du bucket mais le s'appliquant qu'aux objets de celui-ci pour lesquels on le décide. L'Object Lock permet de protéger un objet de la suppression ou de l'écrasement pour une durée fixe ou indéterminée (*retention period*). \n",
    "\n",
    "Les objets *lockés* deviennent alors immutables pendant la durée de la *retention period*. Cette fonctionnalité permet notamment d'implémenter le WORM model (Write Once Read Many). Cette fonctionnalité peut être requise par le type de donnnées pour des raisons légales (ex: un contrat de travail ne peut être modifié que par la signature d'un avenant).\n",
    "\n",
    "Il existe deux modes d'Object Lock: \n",
    "* Le *compliance mode* où personne pas même le *root user* ne peut modifier les paramètres du locking d'un objet, l'effacer ou l'écraser.\n",
    "* Le *governance mode* où certains utilisateurs peuvent encore se voir donner des privilèges leur permettant d'administrer des objets lockés. \n",
    "\n",
    "### Storage classes et lifecycle management\n",
    "Le cycle de vie des objets se gère au niveau du bucket à l'aide de lifecycle policies (mêmes s'il est possible de les décliner à la maille préfixe). Ces lifecyles policies permettent de confier à S3 la charge d'automatiquement faire passer un objet d'une storage class à une autre (*transition action*) et/ou de supprimer un objet au bout d'un certain temps (*expiration action*).\n",
    "\n",
    "Chaque objet se voit associé une storage class. Les différentes storages classes sont, par coût de stockage décroissant, coût et temps de requêtage croissant:\n",
    "* S3-Standard.\n",
    "* S3-Standard-IA / S3 One-Zone-IA: IA = *infrequent access*. Le coût de stockage est inférieur à S3-Standard mais le requêtage est plus cher, les données restant requêtables en quelques millisecondes comme pour S3-Standard. S3 One-Zone-IA est moins cher, les données n'étant stockées que sur une seule AZ, elles sont aussi potentiellement moins *available*/*durable*.\n",
    "* S3 Glacier: Les données doivent être restaurées avant de pouvoir être requêtées. Elles restent toutefois en cas de besoin accessibles en quelques minutes grace à la fonctionnalité d'*expedited retrieval*. \n",
    "* S3 Glacier Deep Archive: Encore moins cher que S3 Glacier pour le stockage, la restauration des données prend de l'ordre de 12h.\n",
    "\n",
    "Toutes ces classes sont conçues de façon à garantir une durability de 99.999999999% (on ne perd en moyenne chaque année que 0.000000001% des objets) et une availability de 99.9% pour les classes résilientes (multi-AZ) et 99.5% pour les classes mono-AZ.\n",
    "\n",
    "S3 offre une storage class S3-Intelligent-Tiering qui surveille les accès à l'objet et par défaut l'assigne soit à une class Frequently accessed (S3-Standard), classe par défaut à l'upload de l'objet et Infrequent Access (si par exemple 30 jours sans aucun accès. Il est possible d'activer deux archive access tiers, l'Archive Access tier (dans le quel un objet tombe au bout de 90 jours sans accès) et le Deep Archive Access tier (180 jours). Cette fonctionnalité est particulièrement utile si on ne connait pas suffisamment précisément les patterns d'accès aux données pour définir efficacement une lifecycle policy.\n",
    "\n",
    "### Cross-Origin Resource Sharing (CORS)\n",
    "Le CORS est une foctionnalité de sécurité proposée par la plupart des *web browser* modernes. Il s'agit de règles permettant de définir pour un *web server* donné (*domain*) quels *web servers* extérieurs (*origins*) peuvent, avec quels protocoles et pour quelles opérations, accéder à ses ressources ou ses services.\n",
    "\n",
    "Il peut arriver qu'une *web application* s'exécutant dans un browser ait besoin d'accéder à des ressources ou des services hébergés par un *web server* différent de son *web server* d'origine (on parle de domaines différents). Pour des raisons de sécurité, ces requêtes *cross-domain* sont souvent bloquées par le browser, c'est le cas par exemple pour un certain nombre d'opérations des requêtes HTTP.\n",
    "\n",
    "Si le serveur extérieur auquel on veut accéder autorise via des règles CORS, une application provenant d'un autre domaine à lui envoyer une requête pour une certaine opération avec un certain protocole, on va pouvoir la lui soumettre en ajoutant y un *header* CORS que le *browser* laissera passer. L'application s'enquiert de cette possibilité auprès du *web server* extérieur à l'aide d'une requête préliminaire appelée *pre-flight request*. \n",
    "\n",
    "S3 étant capable d'héberger des sites webs statiques, cette fonctionnalité devait être utilement complétée par la possibilité d'attacher des *CORS configurations* au *bucket* (concrètement il s'agit d'un document XML détaillant quelles origines peuvent accéder aux ressources du *bucket* avec quel type d'opération et de protocole). Avec les bonnes CORS *policies* en place, une *web application* s'exécutant dans un *browser* pourra accéder aux ressources du bucket sans que celui-ci soit son *domain* d'origine.\n",
    "\n",
    "### Authentification des requêtes REST\n",
    "L'interaction avec S3 se fait via une API REST par l'intermédiaire de requêtes HTTP. Quand un requête est réceptionnée, elle doit être authentifiée, c'est à dire rattachée à un utilisateur. En fonction des permissions attachées à cet utilisateur, la requête est ensuite traitée ou rejetée. \n",
    "\n",
    "L'authentification des requêtes HTTP s'appuie sur le *header* HTTP `Authorization` (en fait dédié à l'authentification). L'utilisateur concatène des éléments du header de la requête et génère une string appelée signature à leur appliquant l'algorithme HMAC-SHA1 avec son AWS Secret Key. La signature est jointe à la requête avec l'AWS Access Key de l'utilisateur. Le système identifie à l'aide de la l'Access Key la Secrey Key ayant servi à calculer la signature. S'il parvient à recalculer la même string que la signature, alors l'expéditeur de la requête aura démontré qu'il est en possession de cette Secret Key et la requête sera alors traitée sous l'identité et avec les permissions de cet utilisateur.\n",
    "\n",
    "On oppose requêtes signées (signed, authentifiées) et non-signées (unsigned, anonymes).\n",
    "\n",
    "L'ensemble des informations d'authentification peuvent être incluses non pas dans le header HTTP dédié mais directement dans l'URL. On parle alors que query string ou de presigned URL.\n",
    "\n",
    "Les presigned URL peuvent être un moyen de partager l'accès à des objets (par défaut privés) à d'autres utilisateurs qui ne sont pas des utilisateurs IAM, pour le téléchargement par exemple. L'utilisateur enregistré créé une presigned URL à l'aide se ses clés AWS et la partage aux utilisateurs auxlequels il souhaite donner l'accès aux objets. On peut assigner une durée de vie (*expiration date*) à une presigned URL permettant de limiter l'accès aux objets dans le temps.\n",
    "\n",
    "Les presigned URLs permettent de faire du third-party access without proxying the requests: c'est à dire sans avoir à mettre en place un service intermédiaire d'authentification des requêtes.\n",
    "\n",
    "### Autres solutions d'upload vers S3\n",
    "\n",
    "#### AWS S3 Multipart upload/download\n",
    "N'est pas une fonctionnalité mais une API. Permet d'uploader/download des fichiers au delà de la limite de taille d'upload/download de 5 GB qui est plus faible que la taille max autorisée d'un fichier sur S3 (5 TB). AWS S3 Multipart upload/download permet d'uploader/de downloader des fichiers de n'importe quelle taille avec comme limite la taille maximale qu'un objet peut avoir sur S3 (5 TB).\n",
    "\n",
    "#### AWS S3 Transfer acceleration\n",
    "AWS S3 Transfer acceleration est une fonctionnalité s'activant au niveau du bucket et s'appuyant sur AWS CloudFront permettant d'accélérer le transfert de données vers S3. Les données sont recueillies au niveau des *edges locations* au plus prêt de l'utilisateur et sont ensuite routées vers S3 au sein du réseau AWS suivant un chemin optimisé.\n",
    "\n",
    "#### AWS Transfer\n",
    "[AWS Transfer family official documentation](https://docs.aws.amazon.com/transfer/latest/userguide/what-is-aws-transfer-family.html)\n",
    "\n",
    "Le service AWS Transfer est un service fully-managed et highly available de transfert de fichiers vers S3 à l'aide d'un des protocoles suivants SFTP, FTPS ou FTP. IL s'agit simplement de créer un serveur AWS Transfer, le bucket S3 vers lequel les fichiers seront transférés, de créer les IAM policies permettant au serveur d'accéder au bucket et d'associer un identity provider au serveur permettant d'authentifier et de gérer les utilisateurs pouvant utiliser le service de transfert. Une fois le serveur créé, les utilisateurs autorisés peuvent s'y connecter depuis un client FTP à l'aide du endpoint hostname associé au serveur à sa création. L'utilisation de AWS Transfer peut aussi être rendue transparente à l'utilisateur qui peut continuer d'utiliser les noms de domaines (DNS) habituellement usités (ex: `ftps.accounting.example.com`) qu'on route à l'aide d'AWS Route 53 vers le serveur AWS Transfer.\n",
    "\n",
    "#### AWS Datasync\n",
    "[AWS Transfer family official documentation](https://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html)\n",
    "\n",
    "Le service AWS Datasync est un managed service de transfert de donnés entre systèmes de stockage on-premise (Network File System - NFS, Server Message Block - SMB, self-managed object storage) et services de stockages AWS (S3, S3 Glacier, Snowcone, EFS, FSx) ou entre services de stockage AWS.\n",
    "\n",
    "Un des éléments de l'architecture AWS Datasync est l'agent Datasync qui est une VM localisée au niveau du stockage sur lequel elle doit lire ou écrire (par exemple sur l'infrastructure on-premise du client, ou dans une instance EC2 localisée dans la même région que le moyen de stockage à synchroniser). L'agent Datasync n'est pas nécessaire pour transférer des données entre deux services de stockages au sein du même compte AWS (même si cross-region ?). Il est nécessaire pour de la synchronisation avec un stockage on-premise ou avec des services de stockage d'un autre compte AWS.\n",
    "\n",
    "AWS Datasync peut notamment être utilisé au sein d'un même compte pour des transferts de données cross-region entre deux buckets S3 par exemple (même si S3 peut déjà gérer cela) ou entre un EFS et un bucket S3 situés dans des régions différentes. \n",
    "\n",
    "### Résilience \n",
    "Sauf pour la storage class S3-One-Zone-IA, un objet est répliqué sur plusieurs AZ. S3 offre aussi une fonctionnalité S3 Cross-Region Replication permettant de faire des copies automatiques et assynchrones de données d'un bucket vers un ou plusieurs autres buckets situés dans des Regions différentes.\n",
    "\n",
    "### Encryption\n",
    "S3 supporte la server-side encryption (SSE) / encryption at rest: c'est S3 qui se charge de chiffrer les données à l'upload, celles-ci sont stockées chiffrées et sont déchiffrées au téléchargement. Le chiffrement est transparent pour l'utilisateur et entièrement géré par S3. Les clés de chiffrements peuvent être soit une S3-managed key (S3-SSE), soit une clé AWS KMS (SSE-KMS), soit une clé fournie par le client (SSE-C).\n",
    "\n",
    "Remarque: Pour ce qui est de l'encryption in transit, ce n'est pas explicite mais les données étant transférées de manière sécuriée via le protocole HTTPS dont encryptées SSL/TLS, peut-on considérer qu'il y a encryption in transit ?\n",
    "\n",
    "### *Data consistency*\n",
    "Sur le spectre allant de l'*eventual consistency* à la *strong consistency*, Amazon S3 proposait jusqu'à décembre 2020 une *eventual read-after-write consistency*. Depuis décembre 2020, Amazon S3 supporte la *strong read-after-write consistency* nativement sans surcoût ou perte de performance (rendant des dispositifs comme l'EMRFS pour Amazon EMR par exemple, caduques). \n",
    "\n",
    "Plusieurs répliques (*replicas*) des mêmes données étant stockés afin de garantir la *highly availability* et *durability*, toute opération modifiant (opérations PUT ou DELETE) des données doit être répercutée sur l'ensemble des répliques. La *read-after-write consistency* s'intéresse à ce qui se passe quand des requêtes en lecture (GET ou LIST) arrivent avant que les changements apportés à des données aient été propagés à l'ensemble des répliques. Schématiquement:\n",
    "* Dans la *strong read-after-write consistency*, toutes les opérations de lecture suivant immédiatement une modification réussie retourneront le nouvel état des données. Tout se passe comme si les opérations de *write* (modification de l'ensemble des *replicas* incluse) et de *read* avaient été linéarisées, rendues séquentielles. La *strong consistency* a l'avantage de diminuer la complexité de l'interaction avec le stockage souvent au détriment de la performance.\n",
    "* Dans l'*eventual read-after-write consistency*, l'ensemble des *replicas* finira par (*eventually*) se retrouver dans le même et nouvel état (leur données seront alors dites dans un état cohérent). La priorité est donnée au traitement des requêtes: on a pas à attendre que la cohérence soit atteinte pour lire mais les lectures réalisées entre l'écriture et l'atteinte de la cohérence peuvent donner des résultats incohérents ou inattendus qui dépendent de la réplique lue et de son état au moment de la lecture.\n",
    "\n",
    "Remarque: A une époque, il était recommandé pour des raisons de performance de \"randomiser\" les clés des objets qu'on écrivait dans son *bucket*. Les données stockées sur S3 étaient en effet partionnées, les partitions stockant physiquement les données étant déterminées sur la base de la clé. Avoir des clés trop proches bien qu'unique impliquait le risque que toutes les données se retrouvent sur la même partitions qui en cas de fortes sollicitations créerait un *bottleneck*. Ce n'est semble-t-il plus le cas aujourd'hui.\n",
    "\n",
    "### AWS S3 Glacier\n",
    "[AWS S3 Glacier official documentation](https://docs.aws.amazon.com/amazonglacier/latest/dev/introduction.html)\n",
    "\n",
    "AWS S3 Glacier est la solution sécurisée, low cost, durable et fully-managed d'AWS pour le stockage de long-terme et d'archivage de données. \n",
    "\n",
    "Pour une durabilité et disponibilité maximale, les données sont automatiquement stockées sur plusieurs AZ. \n",
    "\n",
    "Les principales ressources de S3 Glaciers sont: \n",
    "* Les vaults\n",
    "* Les archives\n",
    "* Les jobs\n",
    "* Les notification configurations\n",
    "\n",
    "Un vault est l'analogue du bucket pour S3, un vault est le conteneur renfermant les archives. Un vault est une ressource Region-based et a un nom unique à l'échelle de la région. Un vault est adressable avec des adresses du format suivant: `https://glacier.<region-name>.amazonaws.com/<account-id>/vaults/<vaultname>`. Les vaults supportent différents types d'opération comme create, delete, list, etc.\n",
    "\n",
    "Une archive est l'unité de base de stockage dans S3 Glacier et peut être n'importe quel type de fichier. L'archive est à S3 Glacier ce que l'objet est à S3. Une archive est une ressource Region-based et a un nom unique à l'échelle de la région. Une archive est adressable avec des adresses du format suivant: `https://glacier.<region-name>.amazonaws.com/<account-id>/vaults/<vaultname>/archives/<archive-id>`\n",
    "\n",
    "Remarque: Uploader des archives vers S3 Glacier ne peut se faire que via les SDK ou l'API REST mais pas depuis la *management console*. On peut uploader des archives allant jusqu'à 4GB en un seul upload mais AWS recommande d'utiliser le multipart upload dès 100MB. L'upload massif de fichiers vers S3 Glacier peut aussi se faire à l'aide d'AWS Snowball. Une archive ne peut pas être modifiée ou écrasée. Pour effectuer une telle opération l'ancienne archive doit être supprimée et la nouvelle uploadée (et elle le sera avec une ID différente de l'ancienne).\n",
    "\n",
    "Trois types d'opérations sont réalisées de façon asynchrones, ces opérations étant représentées par des ressouces appelées jobs: \n",
    "* `select`: Il est possible pour S3 Glacier d'exécuter une requête SELECT sur des archives contenant des fichiers CSV.\n",
    "* `archive-retrieval`: Les données archivées dans S3 Glacier ne sont pas directement accessibles, elles sont d'abord récupérées (retrieved) et mise à disposition pour le téléchargement (jusqu'à 24h après la terminaison du job). Les données récupérées correspondent à la sortie du job d'`archive-retrieval` et sont accessibles via une opération GET Job output. Il existe trois options de récupération d'archive, par rapidité décroissante (temps d'exécution du job) et coût au GB décroissant: \n",
    "    * Expedited: Les données sont accessibles en 1-5 minutes.\n",
    "    * Standard: Les données sont accessibles en 3-5 heures.\n",
    "    * Bulk: Les données sont accessibles en 5-12 heures.\n",
    "* `inventory-retrieval`: Permet de lister le contenu d'un vault.\n",
    "\n",
    "Les opérations réalisées par des jobs l'étant de façon asynchrone et pouvant prendre un certain temps, il peut être pratique de se faire notifier par AWS SNS. Les ressources notification configuration d'un vault permettent ainsi de spécifier au format JSON le nom du topic SNS (un par vault) et les types d'événements pour lesquels une notification doit être poussée (ex: `ArchiveRetrievalCompleted`).\n",
    "\n",
    "Remarque: S3 semble offrir un service permettant de [restaurer des objets archivés dans Glacier](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/restore-archived-objects.html) sans en passer par le SDK ou l'API REST. Il semble que des données stockées sur Glacier sont accessibles depuis S3 (toujours le cas ?, puisque Glacier correspond à un sous-ensemble de storage classes il est naturel qu'on ait une intégration particulière de Glacier à S3 ?)\n",
    "\n",
    "Il est possible de définir des resource-based policies dans S3 Glacier au niveau des vaults (vault policies). On parle plutôt de vault access policies. S3 Glacier se distingue en proposant deux types de vault policies: les vault access policies et les vault lock policies. Les secondes correspondent à des vault (access) policies immutables: une fois verrouillée (locked), une vault lock policy ne peut plus être modifiée. Ces policies sont utilisées pour satisfaire des besoins de compliance. \n",
    "\n",
    "Encryption:\n",
    "* Encryption in transit: Les données peuvent être transférées de façon sécurisée avec SSL ou si transfert vers S3 Glacier par recours à de la Client-Side Encryption (CSE).\n",
    "* Encryption at rest: Les données peuvent être stockées de façon chiffrées, voir S3 pour les différentes options de Server-Side Encryption (SSE): AWS-managed key, KMS, customer-provided key. \n",
    "\n",
    "### AWS Storage Gateway\n",
    "\n",
    "[AWS Storage Gateway official documentation](https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html)\n",
    "\n",
    "AWS Storage Gateway permet de connecter une infrastructure *on-premises* à un stockage cloud duquel des applications hébergées on-premise peuvent lire et écrire des données. Le service consiste en une *on-premises software appliance* qui est une VM hébergée sur l'infrastructure *on-premise* (la *gateway*) et un service de stockage cloud (AWS S3) où sont stockées les données. AWS Storage Gateway offre trois types de solutions de stockage:\n",
    "* ***File-based storage***\n",
    "* ***Volume-based storage***\n",
    "* ***Tape-based storage***\n",
    "\n",
    "Pour le ***file-based storage***, la *gateway* \"présente S3 comme un *file system*\" et est destinée à des clients utilisant les *file protocols* NFS (Network File System) ou  Server Message Block (SMB). La *gateway* permet d'interagir avec S3 comme avec un *file system* à l'aide des *file-system protocols* habituels. A chaque fichiers écrits dans le partage de fichier correspond un objet sur S3 dont la clé S3 correspond au chemin dans le *file system* et inversement. Dans le *file-based storage*, les données sont transférées (en HTTPS) vers S3 et y sont stockées. Seules les fichiers accédés récemment sont conservés en cache côté *gateway*. Le *file-based storage* permet à des applications *on-premises* de lire et d'écrire leurs données sur S3 avec en utilisant la même interface que pour un *file system*. C'est en cela qu'on dit que la *gateway* fournit une *file-system interface* à S3. La *gateway* (qu'on appelle aussi dans ce cas *file gateway*) peut se voir comme un point de montage sur S3.\n",
    "\n",
    "Pour le ***volume-based storage***, la *gateway* fournit des volumes de stockage (*storage volumes*) back-upés sur S3 qu'on peut monter sous forme de *devices* iSCSI sur les serveurs *on-premises* hébergeant les applications requérant ce stockage. La *gateway* supporte alors deux configurations:\n",
    "* **Cached volumes**: Toutes les données sont stockées sur S3 et seules les données fréquemment utilisées sont conservées en cache (sur disque en local) pour conserver un *low-latency access* à ces fichiers.  \n",
    "* **Stored volumes**: Toutes les données sont stockées sur l'infrastructure on-premise et les différents volumes sont simplement sauvegardés sur S3 sous forme de *snapshots* EBS périodiquement et de façon asynchrone. \n",
    "\n",
    "Pour le ***tape-based storage***, la *gateway* fournit une interface (sous forme de *devices* iSCSI) comme dans le cas du *file-based storage* mais pour du stockage sur bande magnétique, les données étant finalement persistées sur S3 ou S3 Glacier.\n",
    "\n",
    "### AWS Snow Family\n",
    "[AWS Snow Family official documentation](https://docs.aws.amazon.com/snowball/?id=docs_gateway)\n",
    "\n",
    "L'AWS Snow Family consistue un ensemble de services permettant de migrer de grandes quantités de données vers Amazon S3 mais où passer par le réseau serait trop long: on va physiquement télécharger les données sur des disques qui sont ensuite physiquement expédiés dans un *data center* AWS et dont le contenu est uploadé vers Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 3: Adding the compute layer\n",
    "\n",
    "### AWS EC2\n",
    "[AWS EC2 official documentation](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html)\n",
    "\n",
    "Amazon Elastic Compute Cloud (EC2) est le service fournissant des capacités de calcul à la demande, c'est-à-dire des VMs. AWS EC2 n'est pas un service *fully managed*. Là où un service *fully-managed* relève du PaaS, EC2 relève du IaaS: Amazon prend à sa charge le *hardware* et la virtualisation mais la configuration des VMs (OS, *runtimes*, data, applications) est à la charge de l'utilisateur. Amazon n'est ainsi pas responsable de la disponiblité d'un service hébergés sur des instances EC2. Si le service n'est plus disponible du fait qu'une instance EC2 a planté, c'est de la responsabilité de l'utilisateur d'Amazon EC2.\n",
    "\n",
    "Une VM est appelée instance EC2. Une instance EC2 est démarrée sur la base: \n",
    "* D'un *instance type* décrivant les capacités physiques voulues pour l'instance: RAM, vCPUs, GPUs, bande passante réseau, bande passante EBS, type de stockage disque local (*instance store*) etc.\n",
    "* D'une Amazon Machine Image (AMI) qui rassemble la configuration de la VM (OS, runtimes, etc.) de la même manière qu'une image Docker.\n",
    "\n",
    "Une instance EC2 est physiquement déployée dans tous les types de localisation proposés par AWS: une Availability Zone au sein d'une Region, une Local Zone si la Region en possède, un AWS Outpost ou une Wavelength Zone.\n",
    "\n",
    "Les principaux éléments à configurer d'une instance EC2 concernent:\n",
    "* Le *block storage*\n",
    "* Le réseau\n",
    "\n",
    "#### *Block storage*\n",
    "Les instances ECS supportent deux types de *block devices*:\n",
    "* Les *instance store volumes* qui sont des *virtual devices* dont le *hardware* sous-jacent est attaché à la machine hôte sur laquelle l'instance EC2 s'exécute.\n",
    "* Les volumes EBS qu'on peut voir comme des disques réseau à haute performance et qui ne sont donc pas physiquement rattachés à la machine machine hôte sur laquelle l'instance EC2 s'exécute.\n",
    "\n",
    "Une instance EC2 peut se voir attacher/monter plusieurs *instance store volumes* ou volumes EBS. Les *instance store volumes* ne peuvent être attachés qu'au démarrage de l'instance alors que des volumes EBS peuvent être attachés et déttachés d'une instance déjà en marche.\n",
    "\n",
    "L'ensemble des *block devices* à attacher à une instance est détaillé par une liste appelée *block device mapping* qui peut être contenu dans l'AMI ou passée au démarrage de l'instance (et elle a alors priorité de celle éventuellement spécifiée dans l'AMI). Le mapping contient: \n",
    "* Pour les *instance store volumes*, le nom du *virtual device* correspondant à chaque volume (en `ephemeral[0-24]`)\n",
    "* Pour les volumes EBS, la spécification de chaque volume à créer: *snapshot ID*, taille du volume, type de volume, etc.\n",
    "\n",
    "Ces deux types distinguent aussi deux types de stockage:\n",
    "* Les *instance store volumes* correspondent à du stockage temporaire: les données ne sont persistées que le temps de vie de l'instance. Les données des *instance store volumes* sont conservées en cas de *reboot* mais sont perdues si l'instance est stoppée, hibernée, terminée ou *fail*. Ce type de stockage est adapatée pour toutes formes de données temporaires, de caches, de *buffers* et de données qu'on sait présentes sur d'autres machines. L'ensemble *instance store volumes* d'un hôte est désigné sous le terme générique d'*instance store*. Il s'agit de l'ensemble des partitions disque allouée au stockage des données temporaires des différentes instances EC2 s'exécutant sur cet hôte.\n",
    "* Les données stockées sur volumes EBS sont persistées pour tous les types d'arrêt d'une instance. Cette solution est donc à privilégier pour le stockage de long-terme des données. D'autres services comme AWS S3 ou AWS EFS autorisent à la fois une persistence des données tout au long du cycle de vie d'une instance EC2 et un partage des données entre instances.\n",
    "\n",
    "Le type de stockage temporaire (*instance storage*) dépend de l'*instance type*: certains *types* n'autorisent que du stockage temporaire sous la forme de volumes EBS qui sont détruits si l'instance est stoppée, hibernée, terminée ou *fail*, d'autres recourrent à des *virtual devices* dont le *hardware* sous-jacent est attaché à la machine hôte sur laquelle l'instance EC2 s'exécute. Le type de *hardware* (SATA HDD, SATA SSD, NVMe SSD) dépend du type d'instance, les NVMe SSD offrant par exemple une faible latence et supportent un nombre élevé d'IOPS.\n",
    "\n",
    "Remarque: Les *instance store volumes* NVMe n'ont pas à être inclus dans le *block device mapping*, ils sont automatiquement listés et se voient automatiquement assignés un *device name* au démarrage de l'instance.\n",
    "\n",
    "Le type de *root device* est déterminé par le type d'AMI utilisée (`ebs` ou `instance store`). Cf. AMI.\n",
    "\n",
    "##### Amazon Elastic Block Storage (EBS)\n",
    "[AWS EBS official documentation](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html)\n",
    "\n",
    "Amazon EBS fournit des *block-level storage volumes* pouvant ensuite être attachés/montés à une instance EC2 (différent de S3 qui est un service de *blob storage*). Les principales ressources d'Amazon EBS sont les *volumes* et les *snapshot*, les seconds correspondant à une sauvegarde des premiers stockée sur Amazon S3. Un *snapshot* peut servir à créer de nouveau volumes dans la même Region. Un même volume ne peut être attaché qu'à une seule instance EC2 à la fois (avec des exceptions: cf. Amazon EBS Multi-attach) mais on peut attacher plusieurs volumes à une même instance EC2. Un volume attaché à une instance EC2 peut être utilisé comme n'importe quel disque dur externe et peut se voir comme un disque réseau. Les données écrites sur le volume sont persistées indépendamment du cycle de vie de l'instance à laquelle il est attaché.  \n",
    "\n",
    "Remarque: \n",
    "* Les *snapshots* sont des *backups* incrémentales: seuls les *blocks* ayant changé depuis le dernier *snapshot* sont sauvegardés.\n",
    "* Les *snapshots* sont stockés sur S3 mais dans un *AWS-managed* S3. Si on souhaite les rendre disponibles dans une autre Region, il ne nous est pas possible de le faire en recourrant à la CCR de S3. Nous devons nous même copier les *snapshots* vers une autre Region.\n",
    "\n",
    "Les volumes EBS sont particulièrement indiqués pour les cas où les données doivent être persistées sur le long-terme tout en restant accessible avec une faible latence. Ils sont notamment utilisés pour les instances hébergeant des bases de données.\n",
    "\n",
    "**Un volume EBS est créé pour une Availability Zone particulière**. Le rendre disponible dans une autre AZ requiert d'en faire un *snapshot* à partir duquel d'autres volumes peuvent être créés ailleurs dans la même Region. Pour le rendre disponible dans une autre Region, il faut là encore en créer un *snapshot* qui doit être copié dans les autres Region où il pourra être utilisé comme base pour la création de nouveau volumes.\n",
    "\n",
    "Il existe trois grands types de volumes EBS: \n",
    "* Les volumes SSD qui sont particulièrement adaptés aux charges transactionnelles impliquant un très grand nombre de petites opérations. L'attribut de performance dominant est le nombre d'IOPS. A noter que seul ce type de volume  peut être utilisé comme *boot volume*. Les volumes SSD ne peuvent dépasser 16 TiB. On distingue deux sous-types de volumes SSD: \n",
    "    * Les *General Purpose SSD volumes* qui offrent le meilleur rapport performance/prix et sont adaptés à une large palette d'usages: bases de données, applications requérant une faible latence. Ces volumes supportent jusqu'à 16000 IOPS.\n",
    "    * Les *Provisioned IOPS SSD volumes* sont plus performants et offrent jusqu'à 64000 IOPS pour les *workloads* les plus *I/O-intensives*.\n",
    "* Les volumes HDD qui sont particulièrement adaptés au fortes charges où l'attribut le plus important est le *throughput*. Les volumes HDD ne peuvent dépasser 16 TiB. On distingue deux sous-types de volumes HDD: \n",
    "    * Les *Throughput Optimized HDD volumes* qui sont particulièrement adaptés aux *workloads* pour lesquelles la performance se mesure davantage en *throughput* qu'en IOPS: ETL, *data warehouse*, etc.\n",
    "    * Les *Cold HDD volumes* qui fournissent une solution de *block storage* peu onéreuse pour des données peu fréquemment accédées.\n",
    "* Les *Previous generation* qui correspondent à des HDD d'ancienne génération à n'utiliser que sur de faibles volumétries où la performance n'est pas un enjeu. De tels volumes sont limités à 1 TiB.\n",
    "\n",
    "#### Configuration réseau\n",
    "Une instance EC2 se situe du point de vue du réseau toujours dans un VPC. Amazon EC2 et VPC utilisent par défaut le protocole d'adressage IPv4. L'instance EC2 se voit assigner à son démarrage une adresse IPv4 privée du bloc CIDR IPv4 de son VPC. Amazon EC2 et VPC supportant le protocole IPv6, une instance EC2 peut aussi se voir assigner une adresse IPv6 si son VPC définit un bloc CIDR IPv6 (ce bloc étant choisi par Amazon au sein de son *pool* d'adresses IPv6). Cette adresse étant globalement unique, elle est publique et adressable depuis Internet.\n",
    "\n",
    "L'adresse IPv4 privée est assignée à la carte réseau par défaut (*default network interface*) de l'instance (`eth0`) et le reste jusqu'à la terminaison de l'instance où elle est restituée et à nouveau disponible pour être assignée à une nouvelle instnace. On parle d'adresse primaire puisqu'il est possible d'assigner plusieurs adresses à une même interface réseau.\n",
    "\n",
    "A l'adresse IPv4 privée est aussi associé un alias DNS interne qui n'est résolu et donc ne peut être utilisé qu'à l'intérieur du VPC. Une adresse IP privée n'est par définition pas accessible depuis Internet. Une instance EC2 peut se voir assigner une adresse IPv4 publique et devenir accessible depuis Internet. L'instance à qui on assigne un adresse IPv4 publique se voit aussi assigner un alias DNS public. La résolution de l'alias DNS public retourne l'adresse IPv4 privée à l'intérieur du VPC et l'adresse IPv4 publique à l'extérieur du VPC. La correspondance entre adresse publique et privée est assurée par un service de Network Address Translation (NAT).\n",
    "\n",
    "Remarque: Le serveur DNS Amazon est situé à la base du bloc CIDR du VPC +2.\n",
    "\n",
    "Une instance lancée dans le *default VPC* reçoit une adresse IPv4 publique par défaut. Sinon, le fait de recevoir une adresse IPv4 publique ou pas est une propriété du *subnet* ou de l'instance, la seconde ayant priorité sur la première. L'adresse IPv4 publique provient du *pool* d'adresses IPv4 d'Amazon et n'est pas attachée au compte et retourne dans le *pool* dès qu'elle est dissociée de l'instance. Il n'est pas possible de manuellement dissocier une adresse IP publique d'une instance. Cette dissociation a lieu dans deux principaux cas:\n",
    "* Quand l'instance est stoppée, mise en hibernation ou terminée.\n",
    "* Quand on associe l'instance à une Elastic IP.\n",
    "\n",
    "L'instance recevra une nouvelle adresse IPv4 publique du *pool* d'adresses Amazon à son redérrage et à la dissociation de l'Elastic IP respectivement. Cette nouvelle adresse publique ne sera cependant pas la même. Si on souhaite une adresse IP publique statique utiliser une Elastic IP qui est une adresse IPv4 publique (il n'existe pas d'Elastic IP IPv6), attachée au compte (à la Region). Une Elastic IP est fixe tant qu'elle n'est pas restituée, tant qu'elle reste attachée au compte. Les adresses IPv4 étant des ressources rares, Amazon facture la non-utilisation d'une Elastic IP (si elle n'est pas attachée à une instance EC2 dans le statut \"Running\").\n",
    "\n",
    "Remarque: On accède à une Elastic IP via l'*internet gateway* d'un VPC. Si on a établi une connection VPN avec notre VPC, le trafic VPN passe par une *virtual private gateway* et non par l'*internet gateway* et ne peut donc accéder à une Elastic IP.\n",
    "\n",
    "Concernant les adresses IPv6, comme pour les adresses IPv4 publiques, le fait pour une instance d'en recevoir une ou pas au démarrage est une propriété du subnet ou de l'instance, la seconde ayant priorité sur la première (il est aussi possible s'assigner une adresse IPv6 à l'instance après démarrage ou de lui attacher une Elastic Network Interface à laquelle est attachée une adresse IPv6). L'adresse IPv4 ou IPv6 publique est par défaut assignée à la carte réseau par défaut (*default network interface*) de l'instance (`eth0`). Une instance conserve son adresse IPv6 après avoir été stoppée ou mise en hibernation et ne la restitue qu'à terminaison. Il n'y a cependant pas d'alias DNS associé à l'adresse IPv6.\n",
    "\n",
    "Il existe l'analogue des volumes EBS pour les cartes réseaux: l'Elastic Network Interface (ENI). Voir [cette partie](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html) de la documentation. Il s'agit d'un composant d'un VPC correspondant à une carte réseau virtuelle. Une ENI consiste en une MAC *address*, une *primary private IPv4 adress*, une *public IPv4 adress* et peut se voir assigner plusieurs adresses IPv4 privées et IPv6 ainsi qu'autant d'Elastic IPs que d'adresses IPv4 privées. Une ENI est attachée à une instance EC2. Le nombre de cartes réseau pouvant être attachées à une instance ainsi que le nombres d'IPs pouvant être assignées à une même carte dépend du type d'instance.\n",
    "\n",
    "#### Cycle de vie d'une instance EC2\n",
    "Les différentes actions faisant transitionner une instance EC2 d'un état à un autre: \n",
    "* ***Launch***: Action permettant de passer une instance démarrée dans l'état *Pending* qui prépare l'instance à passer dans l'état *Running* (recherche de l'hôte, configuration réseau, etc.) et entreprise sur une instance démarrée pour la première fois ou précédemmment dans l'état *Stopped*.\n",
    "* ***Reboot***: Le *Reboot* d'une instance à l'état *Running* est l'équivalent d'un *reboot* de l'OS. Le *reboot* préserve les données écrites dans les *instance store volumes*. De même, les alias DNS public, l'adresse IPv4 privée et l'éventuelle adresse IPv6 sont préservés lors de l'opération.\n",
    "* ***Terminate***: Passe l'instance dans l'état *Shutting down* où elle est préparée à être terminée. Au terme du process, elle passe dans l'état *Terminated*. Une instance terminée est définitivement supprimée et ne peut être redémarée même si son *root device* est *EBS-based*. Dans le cas où le *root device* est *EBS-based*, le volume EBS correspondant est par défaut supprimé à la terminaison. Les éventuels autres volumes EBS attaché à l'instance sont en revanche conservés. Une instance *Stopped* peut également être terminée.\n",
    "* ***Stop***: Passe l'instance de l'état *Running* à *Stopping* où elle est préparée à être arrêtée, processus au terme duquel elle passe à l'état *Stopped*. Seules les instances possédant un *EBS-based root device* peuvent être stoppées (au lieu d'ếtre terminées). Une instance stoppée peut être redémarrée ultérieurement. Les données stockées en RAM et dans les *instance store volumes* sont perdues, celles des *EBS volumes* sont préservées, ces derniers restant attachés à l'instance même une fois stoppée. L'adresse IPv4 privée et l'adresse IPv6 sont conservées de même qu'une éventuelle Elastic IP (qui est alors facturée car mobilisée mais non utilisée). L'adresse publique IPv4 est restituée (*released*), une nouvelle sera assignée au redémarrage.\n",
    "* ***Hibernate***: La mise en hibernation (qui exige un certain nombre de prérequis) revient à stopper l'instance La principale différence est que les données stockées en RAM sont persistées dans le *root device EBS volume* (les données des *instance store volumes* sont en revanche perdues). Redémarrer une instance mise en hibernation va la restituer dans son état pré-hibernation: les données sont rechargées en RAM et les processus interrompus reprennent leur exécution au point où ils s'étaient arrêtés.\n",
    "* ***Start***: Passe une instance stoppée ou mise en hibernation de l'état *Stopped* à *Pending* qui la prépare à passer à l'état *Running*.\n",
    "\n",
    "Voir notamment [cette page de la documentation](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-lifecycle.html).\n",
    "\n",
    "#### Options d'achat d'instances EC2\n",
    "Les principales options d'achat d'instances EC2 sont résumées [ici](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html). Les principales sont:\n",
    "* **On-demand**: Le prix d'usage de l'instance en USD/s est fixe par Region. L'utilisation se fait sans engagement de durée (*long-term commitment*) et les instances *on-demand* ne peuvent être interrompues.\n",
    "* **Spot**: L'instance est provisionnée depuis un pool d'instances disponibles et non-utilisées. Le prix est déterminé par l'équilibre offre-demande pour cet *instance type* et peut être significativement inférieur à l'*on-demand* price. L'instance n'est provisionnée que si le *spot price* est inférieur au prix maximum qu'on se donne (*on-demand price* par défaut). Une instance *spot* est interruptible. Elle sera en particulier interrompue si le prix *spot* passe au dessus de notre prix maximum.\n",
    "* **Reserved instances**: Les *reserved instances* sont plus des réductions qu'on achète pour des types d'instance que des instances elle même en échange d'un engagement sur la durée (1 an ou 3 ans). Similaire aux [Amazon Saving Plans](https://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html) où l'engagement se fait sur la durée et le montant horaire (USD/h).\n",
    "\n",
    "EC2 offre également:\n",
    "* **Dedicated host**: Provisionne non pas une VM mais une machine physique dédiée pouvant héberger des instances EC2. Utile pour maitrise les coûts de *hardware-bound licenses* (*per socket*/*per* CPU/*per* VM). Quand stoppées, les instances sont toujours redémarrées sur le même hôte physique. \n",
    "* **Dedicated Instances**: Les instances EC2 *Dedicated Instances* sont hébergés sur du *hardware* dédié au client. Seule des instances EC2 du même compte peuvent être hébergées sur cet hôte (qu'elles soient *Dedicated Instances* ou pas). Permet une *harware-level isolation*. \n",
    "\n",
    "Remarque: Un dernier outil, les *On-Demand Capacity Reservations* permet de réserver pour une certaines durées, un certain nombre d'*instance types* dans une AZ donnée.\n",
    "\n",
    "#### Amazon EC2 Auto Scaling\n",
    "Il est possible de créer des entités appelées *EC2 Auto Scaling groups* (cf. [documentation officielle](https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html)) qui permettent à Amazon de gérer le scaling, l'adéquation à la charge d'un ensemble d'instances. On spécifier une capacité minimale, désirée et maximale et Amazon se charge de créer et de terminer les instances à l'aide de *scaling policies* dans le but de s'adapter à la charge dirigée vers le groupe.\n",
    "\n",
    "Un *Auto Scaling group* (ASG) est une ressource de Region et s'étend sur l'ensemble de ses AZ. Cela a par exemple pour conséquences que si une AZ tombe, Amazon relance immédiatement d'autres instances EC2 dans les autres AZ afin de revenir à la capacité désirée ou encore, Amazon se charge de répartir les instances du groupe entre les différentes AZ. D'un point de vue réseau, les instances sont lancées dans les *subnets* précisés à la définition de l'ASG.\n",
    "\n",
    "Les paramètres de base de l'ASG sont une capacité minimale, maximale et désirée. Amazon commence par démarrer la quantité désirée d'instances. En absence de tout autre ***scaling policy***, Amazon se contente de maintenir le nombre d'instances au niveau désiré, en contrôlant l'état de santé des différentes instances, terminant celles jugées *unhealthy* pour en démarrer de nouvelles. Une autre façon de maintenir un nombre constant d'instances *healthy* à l'aide d'un ASG est d'égaliser capacités minimales, maximale et désirée.\n",
    "\n",
    "On appelle *dynamic scaling* le fait de faire varier l'effectif d'un ASG en réponse aux variations de la \"demande\". Concrètement, les règles suivies pour *scale up* ou *scale in* sont décrites dans une *scaling policy*. Une *scaling policy* consiste le plus souvent pour l'ASG à suivre une métrique CloudWatch (ex: utilisation CPU moyenne) et à prendre des décisions en fonctions des valeurs prises par celle-ci. Amazon EC2 Auto Scaling distingue trois types de *scaling policies*:\n",
    "* ***Target tracking scaling policy***: L'effectif de l'ASG est ajusté dans le but d'ajuster une métrique spécifique à une valeur cible (ex: utilisation CPU moyenne, nombre moyen de requêtes traitées par instance, etc.).\n",
    "* ***Step scaling policy***: L'effectif est ajusté à l'aide de règles (*scaling adjustment*) prédéfinies fonctions l'écart au seuil d'alarme de la métrique suivie.\n",
    "* ***Simple scaling policy***: L'effectif est ajusté basé sur une unique règle (*scaling adjustment*).\n",
    "\n",
    "Un *step adjustment* demande de choisir pour la métrique suivie une fourchette et la nature de l'ajustement à opérer quand la métrique tombe dans cette fourchette (ne rien faire, ajouter/retrancher un nombre absolu/relatif d'instance, établir la capacité à un nombre fixe d'instances. Une *step scaling policy* consiste à définir plusieurs de ces *step adjustments*, par exemple si la métrique est entre 0 et 10, ne rien faire, entre 10 et 20, +20%, entre -10 et 0, -10%, etc.\n",
    "\n",
    "Une seule *scaling policy* est la plupart du temps suffisante mais il est aussi possible d'en attacher plusieurs au même ASG.\n",
    "\n",
    "Remarque: L'*autoscaling* tient compte du *warm-up time* de l'instance qui démarre (paramètre qu'on peut ajuster) et du *shutdown time* d'une instance en terminaison. Par exemple, si une *scaling policy* a suscité le démarrage d'une instance en cours de *warm up* et que la valeur prise par la métrique fait que la policy en demande une plus, aucune nouvelle instance n'est démarrée. Si la *policy* avait demandé 3 instances supplémentaires du fait des nouvelles valeurs de la métrique, on en aurait lancé que 2. Idem dans l'autre sens, celà permet de ne pas lancer ou arrêter plus d'instance que nécessaire à cause de ces décalages entre ordre de démarrage/terminaison et disponibilité/terminaison effective.\n",
    "\n",
    "Pour les *simple scaling policy*, afin d'éviter qu'elles ne se redéclanchent avant que leur effets soient visibles, celles-ci doivent attendre l'écoulement d'une *cooldown period* (par défaut 300s mais ajustable). En *scale-out*, le *cooldown* démarre après l'entrée en service de la dernière instance demandée. En *scale-in*, il démarre après l'arrêt de la dernière instance demandée.\n",
    "\n",
    "Une *simple scaling policy* peut se voir comme une *step scaling policy* ne comportant qu'un seul *step adjustment* mais pas seulement. La principale différence entre les deux types de policy est que seule la *simple scaling policy* doit observer une *cooldown period* avant de pouvoir à nouveau réagir à des alarmes. \n",
    "\n",
    "Remarque: La *cooldown period* n'est pas observée si une instance s'avère *unhealthy* et exige d'être remplacée.\n",
    "\n",
    "Amazon EC2 Auto Scaling laisse la possibilité d'exécuter des tâches sur les instances entre la fin de leur démarrage et leur mise en service effective (ajout à l'effectif du groupe) en *scale out* ou entre l'ordre de terminaison et leur terminaison effective. Ces tâches sont passées à l'aide de *lifecycle hooks*. L'exécution de ces *lifecycle hooks* fait passer les instance dans un état intermédiaire `Pending:Wait` ou `Terminating:Wait` suivant les cas. Elles restent dans ce *waiting state* une durée (*timeout*) d'une heure par défaut où jusqu'à ce que le *lifecycle hook* notifie qu'il a terminé (`CompleteLifecycleAction`).\n",
    "\n",
    "Remarque: Pour les *simple scaling policy*, en *scale out*, la *cooldown period* ne commence qu'à la sortie de l'état `Pending:Wait` (passage à `In Service`). Idem en *scale in* avec la sortie de l'état `Terminating:Wait` sauf en cas d'utilisation avec un *load balancer* où on attend seulement la fin du *deregistration delay*.\n",
    "\n",
    "Amazon EC2 Auto Scaling s'intègre avec ELB: il est possible d'attacher un ou plusieurs *load balancers* à un Auto Scaling group. Le *load balancer* enregistre alors automatiquement toutes les instances du *group* et leur distribue le trafic qu'il reçoit.\n",
    "\n",
    "Quand la charge est prévisible, on peut planifier le *scaling*: *scheduled scaling*. On crée alors des *scheduled actions* qui spécifie les nouvelles capacités minimale, désirée et maximales et le calendrier avec lequel elles s'exécutent.\n",
    "\n",
    "Il est possible de définir pour une *scaling policy* une *termination policy* autre que celle prévalant par défaut. La *termination policy* correspond aux règles suivies par Amazon EC2 Auto Scaling pour déterminer quelles instances seront stoppées en cas de *scale-in* ou autres événements demandant d'arrêter des instances (*rebalancing across AZs*, se conformer à l'*allocation strategy* entre instances On-demand et Spot, etc.).\n",
    "\n",
    "Remarque: Compromis *scale* horizontal/vertical: Si notre application supporte le *scaling* horizontal, ne pas prendre de grosses VMs dimensionnées sur la charge en pointe qui seront sous-utilisées et qui coûteront cher. Privilégier des VMs plus petites et ajouter des VMs pour passer la pointe.\n",
    "\n",
    "#### Amazon Machine Image (AMI)\n",
    "Une AMI contient toute l'information requise pour démarrer une instance EC2. Il en existe de deux types qui déterminent le type du *root device* de l'instance EC2. Le *root device* d'une instance EC2 stocke en effet toutes les éléments décrits dans l'AMI et notamment l'OS et son type est déterminé par le type d'AMI. Les deux types d'AMI sont: \n",
    "* *EBS-backed*: L'AMI correspond physiquement à un *snapshot* EBS et le *root device* de l'instance lancée à partir de cette image sera une instance EBS. Le fait que l'AMI soit *EBS-backed* et que son *root volume* soit une instance EBS permet à l'instance EC2 de pouvoir être stoppée et redémarrée autant de fois que nécessaire. Le temps de *boot* d'une instance EC2 créée en utilisant une AMI *EBS-backed* est de l'ordre de 1min. La taille du *root device* est au plus de 16 TiB.\n",
    "* *Instance store-backed*: L'AMI est physiquement stockée sous forme de template sur S3 et le *root device* de l'instance lancée à partir de cette image sera un *instance store*, c'est-à-dire un volume de la machine physique sur laquelle l'instance EC2 sera lancée. Ce type d'instance est plus long à démarrer (5 min) puisque l'AMI doit être copiée de S3 vers l'*instance store volume* qui servira de *root device*. La taille maximale du *root device* est également plus petite: 10 GiB. Le fait que son *root volume* soit *instance store-backed* fait que l'instance EC2 ne peut pas être stoppée pour être redémarrée. Elle ne peut qu'être terminée. \n",
    "\n",
    "Pour pouvoir être utilisée (ie: pour pouvoir servir à créer des instances), une AMI doit être enregistrée (*registered*). Une fois désenregistrée (*deregistered*), une AMI ne peut plus servir à démarrer de nouvelles instances. Les instances démarrées avec l'image désenregistrée peuvent toutefois continuer à opérer normalement. \n",
    "\n",
    "Un utilisateur d'EC2 n'a pas forcément à se créer sa ou ses images AMI pour utiliser EC2: il existe déjà un grand nombre d'AMI publiques, payantes ou non pouvant être directement utilisées. Il est possible de créer ses propres AMIs à partir de l'état d'une instance EC2. Ces AMIs pouvant ensuite être partagée au public ou seulement à un cercle restreint d'utilisateurs.\n",
    "\n",
    "Remarque: A la création d'une instance EC2, il est possible de passer des *user data* qui sont globalement un ou plusieurs scripts qui sont exécutés sur la machine pour par exemple patcher l'AMI, télécharger ou updater des éléments manquants comme des *license keys* ou du *software* supplémentaire.\n",
    "\n",
    "#### Placement groups\n",
    "Amazon EC2 offre la possibilité de créer des ressources appelées *placement groups*, des instances EC2 pouvant ensuite être lancées à l'intérieur d'un *placement group*. Un placement group permet de définir une stratégie d'allocation des instances sur l'infrastructure Amazon à un ensemble d'instances EC2. Par défaut, Amazon fait en sorte que les différentes instances se répartissent sur l'ensemble de son *hardware* pour limiter l'impact de défaillances de celui-ci. On peut toutefois influencer la stratégie de placement des instances à l'aide d'un *placement group*. Il existe 3 types de stratégies pouvant être attachées à un placement group: \n",
    "* **Cluster**: Les instances sont placées proches des une des autres (sans pour autant être sur la même machine physique) au sein d'une même AZ ce qui permet d'obtenir de bonnes performances réseau (*low latency*). Cette stratégie est typiquement utilisée pour le HPC.\n",
    "* **Partition**: Les instances sont réparties en partitions, les instances d'une partitions ne partageant pas leur *hardware* avec des instances d'une autre partition. Cette stratégie est typiquement utilisée pour les applications distribuées dont les données sont répliquées comme Hadoop, Cassandra ou Kafka. \n",
    "* **Spread**: Aucune des instances du *group* ne partage son *hardware* avec les autres afin de réduire au maximum l'impact d'une *hardware failure*.\n",
    "\n",
    "### AWS Elastic File System (EFS)\n",
    "\n",
    "[AWS EFS official documentation](https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html)\n",
    "\n",
    "AWS Elastic File System (Amazon EFS) est un *fully-managed service* fournissant des *file systems* NFS pouvant être utilisé aussi bien par d'autres services *cloud* comme des instances EC2 ou des ressources *on-premises*. Le service est *fully-managed* et élastique: pas besoin de provisionner de la capacité, il *scale* automatiquement lors de l'ajout ou la suppression de fichiers. L'utilisateur n'a pas à sa charge de s'occuper de l'infrastructure de stockage, de la configuration des *file systems* etc.\n",
    "\n",
    "Le service est conçu pour être *highly available*, *highly scalable* et *highly durable* (pas de perte de données). \n",
    "\n",
    "Un *file system* EFS est une ressource *region-based* pouvant être montée directement sur une instance EC2 mais aussi sur un VPC. Le point de montage (*mount target*) sur le VPC est un *endpoint* NFSv4 et est attaché à une adresse IP. Cette *mount target* (une par AZ par *file system*) peut ensuite être utilisée (via son IP ou son alias DNS - car il s'agit d'une ressource AWS *highly available* à laquelle on peut associer un tel alias) pour monter le *file system* sur une ou plusieurs des instances EC2 présents dans cette AZ. Le fait de pouvoir monter un *file system* EFS sur un VPC par la création d'une *mount target* permet également de pouvoir monter ses *file systems* EFS sur son infrastructure *on-premises* si celle-ci est connectée au VPC via AWS Direct Connect ou AWS VPN.\n",
    "\n",
    "Un *file system* EFS peut évidemment être accédé par de multiples ressources simultanément (contrairement aux volumes EBS d'une instance EC2 par exemple) et peut ainsi servir de source de données commune/partagée à différentes applications ou services.\n",
    "\n",
    "Comme pour S3, EFS propose pour les fichiers qui y sont stockées différentes classes de stockage: Standard et Infrequent Access.\n",
    "\n",
    "Concernant la performance, EFS permet d'adapter la performance d'une ressource EFS aux cas d'usage à l'aide des:\n",
    "* **Performance mode**: Où on a à choisir entre:\n",
    "    * **General Purpose**: Offre une faible latence et est donc adapté aux *latency-sensitive use cases* comme les *content management systems*, le *file serving*, etc. C'est le *performance mode* pris par défaut.\n",
    "    * **Max I/O** qui offre un nombre d'IOPS plus élevé au prix d'une latence légèrement accrue. Ce *performance mode* davantage plus adapté aux *highly parallelized applications and workloads*.  \n",
    "* **Throughput mode**: \n",
    "    * *Bursting Throughput*: Permet au *throughput* du *file system* de *scaler* avec la quantité de données stockée. L'idée est de pouvoir délivrer un *throughput* élevé pendant une courte période de temps comme l'exigent beaucoup de *file-based workloads*: on lit/écrit un fichier puis plus rien. C'est le mode recommandé par défaut.\n",
    "    * *Provisioned Throughput*: Est plus adapté pour les applications exigeant des niveaux de *throughput* plus élevés que ce que peut délivrer *Bursting Throughput* ou avec relativement peu de données rapportées à la demande de *throughtput* (*high throughput to storage (MiB/s per TiB) ratios*) : on stocke assez peu de données mais elles sont très sollicitées (ex: *web serving*, *content management applications*).\n",
    "\n",
    "Remarque: Un *file system* EFS ne peut être monté que sur un VPC à la fois. Le VPC et le *file system* doivent appartenir à la même Region.\n",
    "\n",
    "### AWS FSx for Windows File Server / Lustre\n",
    "\n",
    "[AWS FSx official documentation](https://docs.aws.amazon.com/fsx/?id=docs_gateway)\n",
    "\n",
    "Amazon FSx for Windows File Server est un *fully managed service* fournissant des serveurs de fichiers (*file servers*) Windows backés par un *file system* Windows. Les fichiers peuvent être accédés sur le réseau à l'aide du protocole Server Message Block (SMB). Ce service est particulièrement adapté à la migration dans le cloud d'applications et d'outils Windows.\n",
    "\n",
    "Les principales ressources du service sont les *file systems*, les *back-ups* et les *file shares*. Les *file sytems* sont *highly available* et peuvent être *single* ou *multi-AZ* et consistent en un ou plusieurs *file servers* (plusieurs dans le cas *multi-AZ*). Question performance, comme pour d'autres services de stockage de fichiers, AWS permet de choisir capacité et *throughput* séparémment. Entre autres, un *file system* peut se baser sur les HDD ou des SSD. Les *back-ups* d'un *file system* sont incrémentale, automatiques par défaut et *highly durable*. Un *file share* est en fait un répertoire du *file system* qui peut être accédé par n'importe quelle instance de *compute* via SMB. C'est par l'intermédiaire d'un *file share* que se monte un *file system* FSx sur une instance Windows. La seule condition d'accès au *file share* étant le protocole SMB, on peut tout à fait y accéder depuis une machine Linux. Un *file share* peut également être accessible depuis une infrastructure *on-premises* pour les instances utilisant AWS VPN ou AWS Direct Connect.\n",
    "\n",
    "Amazon FSx for Lustre fournit le même type de service mais pour des *file system* Lustre, un *file system* (Linux) distribué. Comme pour Windows, un *file system* FSx for Lustre peut être monté sur une instance *compute* située dans le même *subnet* ou le cas échéant, VPC (et peut être accessible depuis une instance *on-premises*). Les *file systems* FSx for Lustre peuvent être notamment utilisés pour backer les *persistent volumes* de conteneurs exécutés sur Kubernetes (AWS EKS). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4: Adding a database layer\n",
    "\n",
    "### Amazon Relational Database Service\n",
    "[Amazon RDS official documentation](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html)\n",
    "\n",
    "Amazon Relational Database Service (RDS) est un *managed relational database service* permettant de mettre en place, d'opérer et de scaler des bases de données relationnelles dans le *cloud* AWS. AWS prend notamment à sa charge la gestion des tâches administratives de base, la gestion automatique des *backups*, le *patching* du *software*, la détection de défaillances et le *recovery*. \n",
    "\n",
    "La ressource principale d'Amazon RDS est l'instance RDS qui correspond globalement à une instance EC2 hébergeant un *database engine* spécifié par l'utilisateur. Les *database engines* actuellement supportés sont MySQL, MariaDB, PostgreSQL, Oracle, Microsoft SQL Server et Amazon Aurora.\n",
    "\n",
    "Une instance RDS étant un cas particulier d'instance EC2, il faut donc choisir un *instance type*. Les *instances types* RDS se regroupent en 3 catégories: \n",
    "* **Standard**: Ce type propose un ensemble équilibré de performance ce calcul, mémoire et réseau qui sied à une large gamme de cas d'usage.\n",
    "* **Memory Optimized**: Ce type propose des capacités mémoire plus importantes (avec par exemple des fréquences de bus mémoire élevées) associées à d'importantes capacités de calcul pour les applications *memory-intensives*.\n",
    "* **Burstable Performance**: Ce type propose des capacités de calcul plus importantes avec la capacité de monter rapidement au maximum des capacités CPUs et des optimisations permettant de dégager le plus de capacité de calcul possible (*dedicated hardware*, *lightweight hypervisor*, etc.).\n",
    "\n",
    "Tous les *database engines* sauf Aurora stockent leur données ainsi que leurs *transaction logs* sur des volumes EBS. Chaque *database engine* impose une limite maximale au stockage (64TiB sauf pour Microsoft SQL Server avec 16TiB). Les trois principaux types de volumes EBS proposés pour les instances RDS sont: \n",
    "* **General purpose SSD**: Ce type offre le meilleur rapport performance/prix pour une large gamme d'usage. Il délivre une latence de l'ordre de la milliseconde et est capable de piquer à 3000 IOPS.\n",
    "* **Provisioned IOPS SSD**: Ce type est dédié aux *I/O-intensive workloads*.\n",
    "* **Magnetic**: Ce type est principalement proposé pour des raisons de rétro-compatibilité. Même s'il est le moins onéreux et le moins performant, il est recommandé de s'orienter vers du General purpose SSD.\n",
    "\n",
    "Remarque: Si on recourt à des *read replicas*, il est possible de choisir un type de volume différent de celui de l'instance primaire.\n",
    "\n",
    "Remarque: A la création d'une nouvelle instance, RDS lui alloue automatiquement un *DNS endpoint* de la forme `db-identifier.fixed-db-identifier.region-name.rds.amazonaws.com`.\n",
    "\n",
    "Remarques: Toutes les données (des *databases*, des *backups*, des *read replicas*, des *snapshots*, etc.) sont *encrypted at rest*. L'*encryption in transit* est possible avec SSL (au prix d'une latence accrue).\n",
    "\n",
    "#### *High availability*\n",
    "Par défaut, une instance RDS n'est exécutée que dans une seule AZ mais une option permet d'activer le déploiement multi-AZ. Dans ce dernier cas, Amazon provisionne et maintient une instance secondaire dite de *standby* dans une autre AZ. L'instance primaire est répliquée de façon synchrone dans la où les autres AZ. Le fait que la *data replication* soit synchrone fait que les instances déployées en multi-AZ ont une latence de *write* et de *commit* légèrement supérieure. Cette approche fournit une redondance des données et un *failover support* contribuant à la *high availability*. Elle permet aussi de minimiser les pics de latence lors des *backups* qui sont réalisées au niveau de l'instance secondaire, l'instance primaire restant disponible. \n",
    "\n",
    "#### *Backups*\n",
    "Amazon RDS créé par défaut automatiquement un *backup* de l'instance RDS à chaque *backup window* (une fois par jour par défaut). Si le *backup* demande plus de temps qu'alloué par la *backup window*, il se pousuit jusqu'à complétion. **Les *backups* automatiques consistent précisément à créer un *snapshot* du *storage volume* de l'instance.** On *back up* ainsi l'ensemble de l'instance (fichiers temporaires compris par ex.) et pas seulement la *database*. Ces *backups* sont gérés par Region par RDS et conservées pour une durée de 1 jour par défaut et pouvant aller jusqu'à 35 jours (*backup retention period*). Les *transactions logs* sont persistés toutes les 5min. *Transaction logs* et *backups* automatiques sont persistées dans un *bucket* S3 *AWS-managed*.\n",
    "\n",
    "La réalisation d'un *backup* occasionne la suspension du *storage I/O* pendant l'initialisation du processus sauf dans le cas d'un déploiement multi-AZ où seule la latence est plus élevée pendant quelques minutes. Il n'y a pas d'interruption du *storage I/O* (sauf pour Microsoft SQL Server) car le *backup* est réalisé sur la *standby instance*.\n",
    "\n",
    "Il est aussi possible de créer des *snapshots* de l'instance manuellement (qui ne sont pas soumis à une *retention period*). Seul le premier *snapshot* contient l'ensemble des données de l'instance, les suivants étant incrémentaux.\n",
    "\n",
    "Pour une meilleure *disaster recovery*, il est possible de copier un *snapshot* vers une autre Region. Il est possible de configurer l'instance RDS de façon à ce qu'elle crée des copies de ses *snapshots* et de ses *transaction logs* vers d'autres Regions (*backup replication*).\n",
    "\n",
    "Les *backups* sont stockées sur Amazon S3.\n",
    "\n",
    "#### *Read replicas*\n",
    "Tous les *database engines* autorisent la création de *read replicas* qui sont instance RDS séparée, liée à l'instance dont elle est la réplique appelée *primary DB instance* (concrètement, une *read replica* est créée à partir d'un *snapshot* de la *primary DB instance*). On peut avoir pour une même *primary DB instance* une ou plusieurs *read replicas*. Les *read replicas* peuvent être déployées en mode *multi-AZ* voire *cross-Region* (sauf pour Microsoft SQL Server). Les *read replicas* peuvent se voir comme des versions *read-only* de la *primary instance*. On peut rediriger tout ou partie du trafic en lecture vers les *read replicas*, soulageant d'autant la *primary instance*. Les *read replicas* sont une solution pour faire scaler facilement des *read-heavy database workloads*. La *primary instance* et ses *read replicas* sont synchronisées de façon asynchrones: cela pose donc des problèmes de *stale data*/*data consistency* (on a pas forcément la dernière version des données / le résultat d'une requête peut dépendre de la *read replica* l'ayant exécutée). \n",
    "\n",
    "### Amazon Aurora\n",
    "[Amazon Aurora official documentation](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_AuroraOverview.html)\n",
    "\n",
    "Amazon Aurora fait partie du service RDS dans lequel on peut le voir comme un *database engine* particulier. Aurora est en fait un service de RDBMS distribué *fully managed*. Le *database engine* d'Aurora est compatible MySQL et PostgreSQL (mais est jusqu'à respectivement 5 et 3 fois plus rapide) est peut être utilisée par des applications utilisant ces *database engines* quasiment sans aucune modification.\n",
    "\n",
    "La ressource principale du service est un cluster Aurora. Un cluster est déployé dans un VPC. Le concept du cluster Aurora a la particularité de séparer *compute* et *storage*. Les données du cluster sont stockées dans un unique volume virtuel (jusqu'à 128 TiB) réparti sur toutes les AZ de la Region dans laquelle le cluster est déployé (un cluster Aurora peut aussi être déployé sur plusieurs Regions, cf. *Aurora global database*). Toute donnée écrite dans le cluster est écrite dans ce volume virtuel et par construction automatiquement répliquée sur plusieurs AZ (*durability*, *high availability*). Les données sont répliquées de façon synchrones (donc *full data consistency* ?). \n",
    "\n",
    "Côté *compute*, un cluster Aurora a une structure hiérarchique. Un cluster Aurora comporte toujours au moins une *primary instance* et jusqu'à 15 (*read*) *replicas* automatiquement réparties sur les différentes AZ. La décorrélation du *storage* et du *compute* permet de démarrer et d'arrêter des *replicas* très rapidement et facilement: on à pas à copier des données dans le premier cas, on ne perd pas de données dans le second cas. En cas d'indisponibilité de la *primary instance*, Aurora soit en démarre une nouvelle soit promeut (*promote*) une des *replicas* nouvelle *primary instance* (*failover mechanism*, *fault tolerance*). En mode *multi-masters*, il n'y a plus de distinction *primary instance* vs. *replicas*.     \n",
    "\n",
    "La connection à un cluster Aurora se fait par l'intermédiaire d'*endpoints* qui jouent le rôle d'interfaces. On distingue différents types d'*endpoints* parmi lesquels:\n",
    "* Le *cluster (writer) endpoint* qui renvoit toujours vers l'instance jouant le rôle de *primary instance* (sans que le client ait à savoir son adresse précise).\n",
    "* Le *reader endpoint* qui est associé à un service de *load balancing* qui répartit les requêtes sur les différentes *replicas*.\n",
    "\n",
    "Un cluster Aurora peut être créé dans deux configurations différentes: \n",
    "* *Provisionned*: Il appartient à l'utilisateur de choisir la taille de son cluster (ex: nombre d'instances et *instance type*).\n",
    "* *Serverless*: Dans ce cas, le cluster Aurora démarre, scale et s'arrête automatiquement. La capacité est adaptée automatiquement à la charge. Cette solution est présentée comme particulièrement adaptée aux *workloads* intermittent, peu fréquents ou imprévisibles. On spécifie cependant une fourchette de capacité du cluster en *Aurora capacity units* (ACUs) à l'intérieur de laquelle le *scaling* est géré automatiquement.\n",
    "\n",
    "Remarque: Le *replica lag* (temps au bout duquel une *update* écrite au niveau de la *primary* instance a été propagée aux *read replicas*) d'Aurora a l'air particulièrement faible (bien inférieur à 100ms).\n",
    "\n",
    "### Amazon DynamoDB\n",
    "[Amazon DynamoDB official documentation](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html)\n",
    "\n",
    "Consistency dans DynamoDB\n",
    "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html\n",
    "\n",
    "DynamoDB: region-based, fully-managed. Utilisé pour les hot-data, les données qui arrivent vite. Solution très performante mais la plus chère. Pas forcément la meilleure pour faire un stockage permanent, plutôt à utiliser commme buffer.\n",
    "\n",
    "Remarques: \n",
    "* Les opérations de lecture dans DynamoDB sont par défaut *eventually consistent*. Les principales opérations de lecture (`GetItem`, `Scan` et `Query`) peuvent cependant être rendues *strongly consistent* (paramètre `StrongConsistency` au prix notamment d'une possible perte de performance. Avec la *strong consistency*, on est sûr que le résultat de la requête sera le produit des dernières *successful write operations*.\n",
    "* Si notre *business workflow* le requiert, il est possible de grouper plusieurs opérations de lecture ou d'écriture d'*items* sur une ou plusieurs tables (dans la même Region) dans une même requête *all-or-nothing* appelée transaction. Une transaction DynamoDB est conçue pour être ACID. En particulier, une transaction est conçu pour s'exécuter de façon atomique: soit l'ensemble des éléments de la transaction réussissent, soit la transaction échoue. \n",
    "* DynamoDB fournit une solution managée permettant de déployer une base de données multi-Region: les *global tables*. Une *global table* est en fait une collection de tables situées dans des Regions différentes et de notre choix. DynamoDB se charge de propager les changements apportés dans une Regions aux autres tables (appelées *replicas*) situées dans les autres Regions.\n",
    "* Dans l'opposition, NoSQL vs RDBMS, il y a un compromis entre consistence (RDBMS) et performance (NoSQL). Le NoSQL scale (horizontalement) plus facilement: il suffit en général de rajouter des serveurs. Le SQL à l'opposé de scale que verticallement d'où une limite intrinsèque en termes de capacité. La performance en termes de *throughput*/*read-write capacity* plafonne ou devient très onéreuse côté RDBMS à partir d'un certain seuil, le NoSQL est en général indiqué pour des besoins de performance extrême.\n",
    "* Les données sont *encrypted at rest*. Les communications avec DynamoDB se faisant par défaut en HTTPS, les données sont donc *encrypted in transit* avec SSL.\n",
    "\n",
    "#### DynamoDB Streams\n",
    "DynamoDB est capable d'offrir des *streams* sur les changement de données intervenant sur ses tables à la granularité de l'*item*. Des applications peuvent ensuite consommer ces *streams* et prendre des décisions ou faire de recommandations basées sur leur contenu. On parle de *Change Data Capture*.\n",
    "\n",
    "DynamoDB offre deux *streaming models*: Kinesis Data Streams for DynamoDB et DynamoDB Streams.\n",
    "\n",
    "#### DynamoDB Accelerator (DAX)\n",
    "DAX est un service permettant de réduire des temps de réponse de DynamoDB de la *single-digit millisecond* à la mircroseconde. DAX est en fait un service de cache *in-memory* compatible avec DynamoDB: un service utilisant déjà DynamoDB peut utiliser DAX quasiment sans aucune modification les deux services utilisent en particulier la même API mais utiliser DAX demande à l'application d'en passer un composant spécifique appelé *DAX client* qui tournera sur la même VM de l'application. En plus d'être une source de gains de performance, DAX permet aussi d'économiser des *read capacities*: si jamais les application lisent très fréquemment les mêmes données, la majeure partie des données sont lues depuis DAX et non DynamoDB. La principale ressource de DAX est le DAX cluster (le fait d'avoir un cluster contribue à la *high availability* du cache, on recommande minimum un *node* par AZ) qui est déployé dans un VPC. Les applications voulant utiliser DAX tournent sur des instances EC2 sur chacune desquelles est installé le *DAX client*. Le *DAX client* traite toutes les requêtes DynamoDB et les envoie vers le *DAX cluster*. Si le cluster ne dispose pas des données pour exécuter la requête (*cache miss*), elle est passée à DynamoDB.\n",
    "\n",
    "Comme tout système de cache, DAX est sujet au problème de *stale data* (partiellement géré par le paramètre de TTL - *time-to-live*) et de possible *data inconsistencies* d'autant que les données peuvent être updatées côté DynamoDB sans passer par DAX (en utilisant un client DynamoDB classique et non un client DAX) et donc sans que DAX le sache. Les opérations de lecture avec DAX sont par défaut *eventually consistent*. Voir [cette page de la documentation](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.consistency.html) pour en savoir plus.\n",
    "\n",
    "### Outils de migration\n",
    "\n",
    "#### AWS Database Migration Service\n",
    "[AWS Database Migration Service official documentation](https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html)\n",
    "\n",
    "AWS Database Migration Service est un outil permettant de migrer des bases de données relationnelles ou NoSQL, des *data warehouses* vers le *cloud*. On peut en même temps migrer d'un *database engine* vers un autre grace à AWS Schema Conversion Tool.\n",
    "\n",
    "#### AWS Schema Conversion Tool\n",
    "[AWS Schema Conversion Tool official documentation](https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_Welcome.html)\n",
    "\n",
    "AWS Schema Conversion Tool est un outil permettant de convertir le schéma d'une base de données d'un *database engine* vers un autre. Par exemple: convertir le schéma d'une DB MySQL vers un schémé de DB Aurora. AWS Schema Conversion Tool permet aussi de convertir les schémas de *data warehouses* comme Teradata par exemple vers Amazon Redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 5: Networking in AWS\n",
    "[AWS VPC official documentation](https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html)\n",
    "\n",
    "VPC = VLAN = Réseau dans le cloud AWS. Permet d'isoler les ressources.\n",
    "VPC créé comme privé par défaut: \n",
    "* Aucune communication avec le réseau publique (internet): si on le souhaire, c'est à nous de créer cette connection.\n",
    "* Aucune communication avec les autres VPC: si on le souhaite, c'est à nous de connecter les VPC.\n",
    "\n",
    "Est-ce qu'un seul VPC suffit? Souvent non, ne serait-ce que pour avoir plusieurs environnements.\n",
    "Multi-account: couche d'isolation supplémentaire mais au prix d'une complexité supplémentaire (on doit gérer les droits d'accès des autres comptes aux ressources de son compte)\n",
    "\n",
    "Remarque: On est limité à 5 VPC/compte/région. Il existe de telles limites pour de nombreux services, elles sont consultables dans [AWS Service Quotas](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html?id=docs_gateway).\n",
    "\n",
    "Bloc CIDR = subnet mask. Une adresse IPv4 se décrit sur 32 bits répartis en 4 blocs de 8 bits. Un bloc CIDR représente une plage d'IP en donnant un *network prefix* (le début de la plage d'adresses) et le nombre de bits fixes, les bits fixes étant forcément les bits de gauche. Dans le bloc 172.31.0.0/24 par exemple, le *network prefix* est 172.31.0.0 et seuls les 8 derniers bits sont libres, la plage d'IPs va donc de 172.31.0.0 à 172.31.0.255. Le bloc 172.31.0.0/25 décrit l'ensemble des adresses de 172.31.0.0 à 172.31.0.127. Dans A.B.C.D/X, le nombre d'adresses de la plage est égal à $2^{32-X}$. Ainsi, 172.31.125.12/32 correspond à une seule ($2^0=1$) adresse (172.31.125.12). A l'opposé, l'ensemble des adresses possibles est décrit par le bloc 0.0.0.0/0.\n",
    "\n",
    "AWS réserve 5 IP par subnet: les 4 premières (les adresses *network prefix* +1 et +2 étant en particulier le routeur et le serveur DNS du VPC) et la dernière (?, la 255?) (pas de broadcast).\n",
    "\n",
    "Tout compte AWS se voit créer un Default VPC dans chaque région en 172.31.0.0/16. Chaque Default VPC possède un Default subnet par AZ de sa région. Ces Default subnets sont tous publics. Il est là principalement pour faire des tests. Permet de lancer directement des services comme EC2 sans se préoccuper du réseau: elle se lancera dans un Default subnet d'un Default VPC. Mais attention à ne pas oublier que ces réseaux par défaut qui nous permettent de prototyper rapidement sont publics!.\n",
    "\n",
    "Chaque VPC est créé avec une route table par défaut qui permet de définir le trafic. Chaque route table est créée avec une route locale qu'on ne peut pas supprimer et qui permet à toutes les ressources d'un même VPC de communiquer entre elles. Bonne pratique: associer à chaque subnet une custom route table (sinon par défault ils sont associés à la main route table). Un public subnet inclut une entrée vers une internet gateway. C'est la table de routage qui détermine si un subnet est privé ou public.\n",
    "\n",
    "AWS internet gateway: fully managed, scalable, redundant, highly available => ce n'est pas un SPOF.\n",
    "\n",
    "Pour être visible sur le réseau public, une resource doit avoir une IP publique qui lui est assignée automatiquement.\n",
    "\n",
    "Trois conditions pour qu'une ressource accède au réseau public:\n",
    "* Avoir créé une Internet gateway pour son VPC\n",
    "* Créer une entrée vers l'internet gateway pour le subnet qu'on souhaite rendre public\n",
    "* Les ressources du subnet public\n",
    "\n",
    "Imaginons qu'on a une resource qu'on ne souhaite pas exposer au réseau public, qui reste dans un réseau privé mais qui doit pouvoir sortir sur le réseau public, accéder à internet (ex: réaliser des updates, accéder à un repo Git). On utilise une NAT qui doit forcément être dans un subnet public pour avoir une IP publique. Idem, pour accéder à des ressources à l'échelle de la Région (bucket S3, table DynamoDB, avec lesquels on interagit avec des requêtes HTTP), celles-ci ne sont pas dans mon VPC, une solution est d'y accéder via le réseau privé. \n",
    "\n",
    "Remarque: \n",
    "* NAT gateway: juste pour sortir du privé vers le public. NAT instance: EC2 instance dans lequel on fait du NAT (Attention: si intance EC2 => non-managé => c'est un SPOF. Pour la high availability, toujours choisir le service, ie: la NAT gateway plutôt que la NAT instance. Globalement NAT instance n'est pas une pratique recommandée).\n",
    "* Bastion: software qui permet de rediriger du trafic public vers du privé. Pas de service AWS, il faudrait le démarrer sur une instance EC2.\n",
    "\n",
    "La NAT masque l'adresse IP privée de la ressource privée, la connection à l'extérieur est faite par la NAT, la réponse lui sera adressée et elle redirigera la réponse vers la ressource privée.\n",
    "\n",
    "Data store instance: ex: EC2 instances dans lesquelles on stocke des données => privé\n",
    "Batch processing instances: dépend du type de donnée processées.\n",
    "Back-end: privé le plus souvent\n",
    "Web-app: privé ou public.\n",
    "\n",
    "Network interface = carte réseau. Quand une instance EC2 est créée, une network interface (`eth0`) est créée en même temps mais elle est détruite avec l'instance. On peut vouloir garder fixe l'adresse (IP publique/privée, MAC) de cette instance. On peut ainsi comme pour EBS pour le disque, créer une ENI (Elastic Network Interface), sorte de carte réseau flottante, qu'on peut rattacher à une instance EC2 à sa création.\n",
    "\n",
    "Elastic IP Address: IP fixe. On paye si on ne l'utilise pas.\n",
    "\n",
    "De manière générale, ne vont dans les subnets publics que les composant ayant à être face à l'internet public: NAT *gateways*, *load balancers* (même les *web apps* peuvent être placées dans des réseaux privés, seul leur *load balancer* se trouvant dans le réseau public), *bastions hosts*, etc. Le reste pouvant être mis dans un ou plusieurs *subnets* privés. Un suffit le plus souvent, celà permet notamment d'avoir un \"gros\" *subnet*: pas de risque de tomber à cours d'adresses pour des applications scalant horizontalement automatiquement (Ex: EC2 Auto Scaling). Certaines application stockant des données sensibles peuvent devoir n'avoir accès à internet ni de façon directe ni indirecte. Ces applications seraient alors placées dans un *subnet* privé séparé et dédié appelé *subnet* protégé (*protected*). De manière générale une infrastructure comporte beaucoup plus de composants privés que publics: les *subnets* privés comportent (ou les dimensioner comme tels) beaucoup plus d'adresses que les subnets publics.\n",
    "\n",
    "### Securité\n",
    "Contrôle du trafic réalisé avec un firewall virtuel qu'on appelle un security group. Un SG autorise par défaut toutes les sorties, aucune entrée.\n",
    "Stateful: si je crée une règle d'entrée, pas besoin de créer l'équivalent en sortie.\n",
    "\n",
    "Default Network ACL créé avec le subnet qui autorise tout.\n",
    "Là où le SG fait que du allow, le Network ACL fonctionne en faisant du allow et du deny, les règles allow ayant la priorité (on peut tout interdire puis définir des allow au compte goutte comme pour un SG). Stateless: pour chaque règle de trafic sortant, il faut définir une règle pour le trafic entrant.\n",
    "\n",
    "### Connexion de VPC\n",
    "\n",
    "Route 53: Hors région car implémenté côté edge location.\n",
    "Load balancer: Au niveau de la région, redirige le trafic entrant vers les instances, qu'elles se trouvent dans un public ou private subnet.\n",
    "\n",
    "#### Amazon Virtual Private Network (VPN)\n",
    "Amazon VPN est un service permettant d'établir une connection sécurisée entre le réseau d'une infrastructure *on-premise* et un ou plusieurs VPC de notre infrastructure AWS. Amazon VPN se décline en deux principaux services: \n",
    "* Amazon Site-to-Site VPN qui permet de connecter des VPCs Amazon au réseau d'une infrastructure on-premise.\n",
    "* Amazon Client VPN qui permet à une application cliente non hébergée sur l'infrastructure AWS d'établir une connection VPN avec un VPN *endpoint* depuis lequel l'application peut ensuite accéder à des ressources Amazon choisies. La principale ressource de ce service est le *Client VPN endpoint* avec lequel l'application cliente établie une connection VPN. Les clients pouvant se connecter au *endpoint* et les ressources auxquelles ils peuvent accéder est contrôlé au niveau de la configuration du *Client VPN endpoint*.\n",
    "\n",
    "On ne discute ici plus que du Amazon Site-to-Site VPN: \n",
    "* [Amazon Site-to-Site VPN documentation](https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html)\n",
    "* [Amazon Client VPN official documentation](https://docs.aws.amazon.com/vpn/latest/clientvpn-user/client-vpn-user-what-is.html)\n",
    "\n",
    "Une Site-to-Site VPN connection consiste en deux *gateways*, une côté Amazon et une côté client *on-premise*. La connection VPN est établie entre les deux *gateways* et consiste pour la *high availability*, en deux tunnels VPN. Côté client, la *gateway* consiste en une application ou une machine dédiée appelée *Customer Gateway device*. Celle-ci est décrite côté AWS par une ressource appelée *Customer Gateway*. Côté Amazon, on a deux types de *gateway* possibles:\n",
    "* *Virtual Private gateway* qui est une ressource AWS attachée à un VPC.\n",
    "* *Transit gateway* qui est une ressource de Region à laquelle on peut attacher plusieurs VPCs mais aussi une *peering* ou *VPN connection* à d'autres *Transit gateways* ou une Amazon Direct Connect *gateway*. Une *transit gateway* agit comme un routeur virtuel régional. Cette ressource possède une [documentation dédiée](https://docs.aws.amazon.com/vpc/latest/tgw/what-is-transit-gateway.html).\n",
    "\n",
    "Remarques:\n",
    "* Une *Transit gateway* n'est pas utilisée uniquement pour faire communiquer de façon sécurisée infrastructures *on-premise* et *AWS*. Une *Transit gateway* peut aussi juste servir à connecter différents VPCs Region entre eux. \n",
    "* Quand on attache un VPC à une *Transit gateway*, il faut pour que du trafic d'une AZ puisse accéder à la *gateway* et inversement, pour que du trafic puisse être dirigé vers cette AZ, sélectionner un *subnet* de cette AZ. La *Transit gateway* place alors une *network interface* au sein de ce *subnet* en consommant une IP de son bloc CIDR. Tous les autres subnets de l'AZ pourront alors accéder à cette *Transit gateway* via cette *network interface*.\n",
    "* Il est aussi possible d'établir une connection sécurisée avec son VPC en passant par un *VPN software* tournant sur une instance EC2 mais l'utilisateur est responsable de toute sa gestion. \n",
    "\n",
    "Comme toutes les *gateways* (internet, NAT, private), on est facturé au trafic. Chaque ressource correspondant à un service managé, elles sont *highly available* et leur capacité *scale* automatiquement en fonction du trafic.\n",
    "\n",
    "#### AWS Direct Connect\n",
    "[AWS Direct Connect official documentation](https://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html)\n",
    "\n",
    "AWS Direct Connect consiste à établir une connection physique directe entre notre réseau d'entreprise *on-premise* et une *location* AWS Direct Connect (*data centers* partenaires où la connectivité haut débit redondée existe déjà). Le routeur *on-premise* est physiquement connecté par cable Ethernet à un routeur AWS Direct Connect. On peut alors accéder à ses services AWS sans passer par le réseau (public) des fournisseurs d'accès. On peut alors accéder via des ressources appelées *virtual interfaces* à des VPCs (*private virtual interface*), des services publics mais sans passer par l'Internet public (*public virtual interface*) ou des *Transit gateways* auxquelles ont été associées une *Direct Connect gateway* (*transit virtual interface*).\n",
    "\n",
    "#### Amazon VPC Peering\n",
    "[AWS VPC Peering official documentation](https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html)\n",
    "\n",
    "Le VPC Peering consiste à établir une connection réseau entre deux VPCs. Les deux VPCs à lier doivent avoir des blocs CIDR disjoints, peuvent appartenir à des comptes AWS différents et peuvent se trouver dans des Regions différentes. Une fois connectées, les instances se trouvant dans un VPC peuvent communiquer avec les instances situées dans l'autre VPC comme si elles appartenaient au même réseau: un VPC Peering n'est ni une *gateway*, ni une connexion VPN, ni ne repose sur un composant *hardware* particulier. Il est conçu pour ne pas représenter de SPoF ou de *bandwidth bottleneck* et offrir l'expérience d'échanger au sein d'un seul et même réseau. \n",
    "\n",
    "Remarque: Un VPC Peering créé une *one-to-one relationship* entre deux VPC et cette relation n'est pas transitive: si un réseau B est liée par VPC Peering à deux autres VPC A et C, on ne peut accéder à C depuis A, il faut pour cela créer un VPC Peering entre les deux.\n",
    "\n",
    "Remarque: La table de routage de chaque VPC doit être modifiée en conséquence afin de pouvoir envoyer et recevoir du trafic vers/de l'autre VPC.\n",
    "\n",
    "#### Amazon VPC NAT Gateway\n",
    "[Amazon VPC NAT Gateway official documentation](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html)\n",
    "\n",
    "Une NAT (Network Address Translation) Gateway est une ressource permettant à des instances situées dans un réseau privé d'accéder à Internet mais qui empêche n'importe quel utilisateur sur Internet d'initier une connection à ces instances. Une NAT Gateway est ainsi toujours placée dans un *subnet* public. La table de routage des *subnets* privés est modifiées pour envouyer tout le trafic *Internet-bound* vers la NAT Gateway qui de charge ensuite de l'envoyer vers l'Internet Gateway. Une Elastic IP à associer à la NAT Gateway doit être spécifiée à sa création. Il n'est pas possible de détacher une Elastic IP attachée à une NAT Gateway sans supprimer cette dernière.\n",
    "\n",
    "Une NAT Gateway est créée dans une AZ donnée et est conçue comme *highly available*. Il est tout à fait possible si on souhaite davatage de contrôle d'utiliser une instance EC2 comme NAT Gateway. On parle alors de NAT *instance*. Des services comme la *high availability* sont alors à la charge de l'utilisateur. \n",
    "\n",
    "Afin d'éviter que le trafic destiné à S3, DynamoDB ou d'autres *Region-level services* ne passe par l'Internet public, on peut définir des *endpoints* dédiés qui permettent au trafic destiné ou en provenance de ces services de ne transiter que par le réseau interne AWS. Les ressources associées sont appelées: \n",
    "* *Gateway VPC endpoints* dans les cas de Amazon S3 et DynamoDB.\n",
    "* *Interface VPC endpoints* (Amazon PrivateLink) dans le cas des autres services.\n",
    "\n",
    "Voir notamment [cette section](https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html) de la documentation.\n",
    "\n",
    "Remarque: Dans le cas de deux *peered* VPCs, il faut créer une NAT Gateway par VPC si chacune comporte un *subnet* privé ayant besoin d'accéder à Internet: un VPC ne peut pas utiliser la NAT Gateway d'un autre VPC.\n",
    "\n",
    "#### AWS PrivateLink\n",
    "[AWS PrivateLink official documentation](https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-privatelink.html)\n",
    "\n",
    "Imaginons que nous développions un service dans un VPC. Ce service est privé, personne ne peut y accéder de l'exéterieur, en particulier depuis l'internet public. Si une application localisée dans un autre VPC dans un autre compte souhaite y avoir accès, on peut établir une connection privée (qui ne transite que par le réseau Amazon) à l'aide d'AWS PrivateLink avec ce VPC pour lui donner accès à ce service en particulier. Un VPC Peering est différent, il permet à partir de deux réseau de n'en faire plus qu'un, ici il n'est pas question que tout le trafic du VPC client soit accepté. PrivateLink permet seulement de partager de façon privée un service hébergé sur AWS. En particulier, AWS s'appuie sur sa propre technologie dans l'implémentation de ses *service endpoints* comme les VPC S3 *endpoints*. Concrètement, on va créer un *endpoint* dans le VPC client qui consiste en une carte réseau. Le trafic à destination du service peut ainsi être décrit dans la table de routage du VPC: on le redirige vers l*endpoint*. De l'autre côté, on place un *network load balancer* qui répartit le trafic vers les différentes machines constituant le service.\n",
    "\n",
    "Le consommateur de service a alors directement accès depuis son VPC via une IP privée aux services du *service provider*. L'ensemble suppose la création de VPC *endpoints* de chaque côté qui correspondent à une *network interface* placée dans le *subnet* choisi et consommant une IP (privée) du bloc CIDR de ce *subnet*. Voir aussi cette [partie de la documentation](https://docs.aws.amazon.com/vpc/latest/userguide/endpoint-services-overview.html) de Amazon VPC a propos de ces *endpoints*. L'idée d'AWS PrivateLink est 1) que des services hébergés sur AWS soient accessibles depuis un *subnet* sans que le propriétaire du service ait à l'exposer via une IP publique / que le trafic transite par l'Internet public et par conséquent 2) pas besoin d'utiliser de NAT et d'Internet Gateway ou d'en passer par un VPN ou du DirectConnect.\n",
    "\n",
    "### Amazon Elastic Load Balancing (ELB)\n",
    "[AWS Elastic Load Balancer official documentation](https://docs.aws.amazon.com/elasticloadbalancing/?id=docs_gateway)\n",
    "\n",
    "Un Elastic Load Balancer (ELB) distribue automatiquement le traffic entrant sur un ensemble de cibles (*registered targets*) comme des instances EC2, des conteneurs, des IPs, etc. réparties sur une ou plusieurs AZ. L'ELB réalise des *health checks* sur ses *targets* et n'envoie le trafic que vers les *targets* saines (qui répondent au *health check*). Quand une *target* est jugée *unhealthy*, l'ELB arrête automatiquement de lui rediriger du trafic et lui renverra du trafic s'il détecte qu'elle est à nouveau *healthy*. Les ELB sont des services *fully managed*, *highly available* et *scalent* automatiquement.\n",
    "\n",
    "Un *load balancer* est une ressource de Region qui répartit le trafic sur des subnets situés dans plusieurs AZ (les Application Load Balancers exigent au moins 2 AZ). Les AZ doivent être enregistrées auprès du *load balancer* pour que du trafic y soit redirigé: si un *target group* possède des instances situées dans une AZ non enregistrée, ces instances ne recevront pas de trafic su l'AZ n'a pas été enregistrée.\n",
    "\n",
    "Dans le cas des Application Load Balancers, un *load balancer node* est créé dans chaque *subnet* consommant une IP privée du bloc CIDR du subnet. AWS recommande d'avoir 8 IPs de disponibles par subnet pour permettre au *load balancer* de scaler (en précisant que le processus de *scaling* pour le conduire à en consommer jusqu'à un maximum de 100). \n",
    "\n",
    "Les *load balancers* sont les ressources vers lesquelles des services comme Route 53 redirigent le trafic.\n",
    "\n",
    "En distribuant le trafic sur un ensemble de moyens de calculs potentiellement répartis sur plusieurs AZ, un ELB contribue à la disponibilité et la *fault tolerance* d'une application. \n",
    "\n",
    "Il existe trois grands types d'ELB: \n",
    "* Les Classic Load Balancers: Opère aussi bien au niveau de la requête (couche 7 - HTTP) que connection (couche 4 - TCP). Ressource *legacy*, privilégier les deux autres. Une des principales différences avec les autres types est que les instances vers lesquelles rediriger le trafic sont directement enregistrées auprès du *load balancer* alors que les autres redirigent vers des *target groups* au sein desquels sont enregistrés les instances. \n",
    "* Les Application Load Balancers qui opèrent au niveau de la couche OSI 7 (redirigent du trafic HTTP(S)). Ils peuvent notamment rediriger du trafic vers des fonctions Lambda contrairement au Network Load Balancer. Check aussi l'état de santé des cibles, qu'elles soient des instances EC2 ou des conteneurs. **Exige au moins 2 AZ.**\n",
    "* Les Network Load Balancers qui opèrent au niveau de la couche OSI 4 (redirigent du trafic TCP, UDP, TLS). Ils ont une performance supérieure aux ALB en termes de capacité de traitment et sont appropriés dans les cas de trafics extrêmes ou de pointes de trafic. Ils sont capables de rediriger vers des instances EC2 comme des conteneurs.\n",
    "\n",
    "Remarque: Les ELB offrent une solution de gestion des certificats et de décryptage SSL: ils permettent de centraliser la configuration SSL à leur niveau et de décharger nos applications de la tâche *CPU-intensive* du décryptage.\n",
    "\n",
    "Dans le cas des ALB et NLB, les *targets* sont enregistrées dans des *target groups*. Le trafic est redirigé vers les *target groups* qui sont des entitées intermédiaires chargées de faire les *health checks* de leurs membres et de répartir le trafic entre eux. Dans le cas des CLB, les *targets* sont directement enregistrées auprès du *load balancer*.\n",
    "\n",
    "Un des avantages de l'ELB est que le client n'a pas à connaître l'IP de la machine qui traitera sa requête. L'IP/l'alias DNS de l'ELB est la seule adresse à connaître, ce dernier se chargeant du dispatch. \n",
    "\n",
    "Remarque: Dans le cas des instances EC2, il existe un paramètre appelé *deregistration delay* permettant d'arrêter d'envoyer du trafic à une instance sur le point d'être stoppé (utile quand un Auto Scaling Group *downscale*). L'instance est d'abord labellisée *deregistered*, l'ELB va alors arrêter de lui envoyer du nouveau trafic mais les connexions existantes ne sont pas fermées comme en cas de *missed health check*. Ces connexions disposent du *deregistration delay* pour se fermer. On a donc un délai afin de finir de satisfaire la requête d'un client qui ne connait pas d'interruption de service: on stoppe la VM \"proprement\".\n",
    "\n",
    "### Amazon Route 53\n",
    "[AWS Route 53 official documentation](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Welcome.html)\n",
    "\n",
    "Amazon Route 53 est un DNS (*Domain Name System*) *service highly available* et *scalable*. Route 53 a 3 principaux usages:\n",
    "* *Domain registration*: Amazon Route 53 permet grace qu'Amazon soit aussi un *domain reseller*, d'enregistrer un nouveau domaine. L'enregistrement du domaine auprès des *domain registries* est prise en charge par Amazon.\n",
    "* *DNS routing*: Amazon Route 53 associe à chaque *registrered domain* des *name servers* capables d'assurer le DNS *service* pour ce domaine. Une requête DNS pour un domaine enregistré avec Route 53 sera satisfaite et routée vers les ressources avec la *policy* choisie, l'ensemble étant enregistré dans un objet dédié utilisé par les *name servers* appelé *hosted zone*.\n",
    "* *Health checking*: Amazon Route 53 est capable de d'envoyer des requêtes à n'importe quelle ressource sur Internet pour s'enquérir de son état et notifier des utilisateurs suivant la situation.\n",
    "\n",
    "Remarques:\n",
    "* Amazon Route 53 peut aussi être utilisé pour faire du *private DNS*, qui est une forme de DNS local permettant de router du trafic à l'intérieur d'un VPC.\n",
    "* Amazon Route 53 est implémenté côté *edge location*. Il est donc hors Region et permet de faire, de construire de la *multi-region availability*.\n",
    "\n",
    "Une fois un domaine enregistré avec Amazon Route 53, le domaine se voit associer 4 *name servers* pour prendre répondre à toutes les requêtes DNS dont il ferait l'objet. Au domaine est également associé une *hosted zone* qui est un registre rassemblant des objets appelés DNS *records* (ou simplement *records*). Un DNS *record* est un objet décrivant comment le trafic a destination du domaine ou d'un ou plusieurs de ses sous-domaines doit être routé. Par exemple, un *record* peut servir à décrire que le traffic à destination de www.accounting.example.com, sous-domaine de www.example.com doit être routé vers une ressource de type *web server* adressé en IPv4 et d'adresse 192.0.30.125. A un DNS *record* est aussi associé une *routing policy* qui précise comment est réalisé le routage entre le client et la ressource qui traitera sa requête: \n",
    "* **Simple routing policy**: *Policy* de base routant le trafic vers une unique ressource (*web server*, *mail server*, etc.) dite primaire fournissant des services pour notre domaine.\n",
    "* **Failover routing policy**: Contrairement à la *Simple routing policy*, on a deux ressources, une primaire et une secondaire, le trafic étant redirigé vers la ressource secondaire si la primaire s'avérait *unhealthy*.\n",
    "* **Geolocation routing policy**: A utiliser pour réaliser le routage basé sur la localisation des utilisateurs.\n",
    "* **Geoproximity routing policy**: A utiliser pour réaliser le routage basé sur la localisation des ressources (ou le cas échéant pour rediriger le trafic de ressources situées à un endroit vers des ressources situées à un autre emplacement.\n",
    "* **Latency routing policy**: A utiliser pour router le trafic vers les ressources à plus faible latence.\n",
    "* **Multivalue answer routing policy**: A utiliser quand on souhaite que Amazon Route 53 réponde à la requête DNS avec les adresse de ressources (jusqu'à 8 sélectionnées au hasard) contrôlées comme *healthy*.\n",
    "* **Weighted routing policy**: A utiliser pour router le trafic vers différentes ressources dans des proportions qu'on choisit.\n",
    "\n",
    "Voir [cette  page de la documentation](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html) pour la liste des *routing policies* disponibles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 7: Identity and Access Managmeent (IAM)\n",
    "[AWS IAM official documentation](https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html)\n",
    "\n",
    "Compte root: Pour être safe et faire en sorte que personne puisse y acceder programmatiquement (CLI ou SDK), on supprime son access key. On crée ensuite autant de comptes admins que nécessaires avec les seuls droits requis.\n",
    "IAM n'est pas un annuaire LDAP. IAM c'est pour ceux qui vont avoir accès à l'infra: les devs, les applis, etc. Si on est juste un utilisateur d'une appli de l'entreprise tournant sur AWS, pas besoin d'avoir un IAM user. Ce n'est pas à IAM de s'assurer que cet utilisateur est bien un membre de l'entreprise. \n",
    "\n",
    "Possible d'accorder des permissions per user mais pas recommandé. Préférer gérer les permissions au niveau de groupes et ajouter les users au groupe.\n",
    "\n",
    "Permissions représentées par des policies.Les policies contrôlent l'accès aux services AWS et sont évaluées au traitement de la requête. Policies = JSON object (ce qui permet de les versionner):\n",
    "* Effect: Allow/Denied\n",
    "* Quel type d'action? Depuis la CLI/console?\n",
    "* Resource: S'applique à toutes les ressources ? Ou pas. Ex: une policy permet de cibler certaines ressources: ex: appliquer la policy qu'à certains buckets.\n",
    "\n",
    "IAM policies permettent seulement de contrôler l'accès aux services. Evaluées au moment de la requête. Denied par défaut: si pas de allow explicit, c'est deny.\n",
    "\n",
    "Managed policies vs inlines policies: Les deux sont des *identity-based policies*. Les *managed policies* sont des *policies* autonomes (*standalone*) qu'on peut attacher à n'importe quel *principal* et gérer indépendemment de ceux-ci. On distingue les AWS managed policies et les *customer managed policies*. Les premières peuvent se voir comme des policies par défaut, des jeux de permissions gérés par AWS regroupant les permissions requises pour l'accomplissement d'une tâche précise. Elles sont notamment recommandées quand on débute avec AWS. Les secondes sont simplement créées et gérées par l'administrateur du compte. Le terme *inlines policies* sert à désigner les *policies* faisant directement partie (*embedded*) de la définition d'un rôle, groupe ou utilisateur. Les *ressource-based policies* sont toujours des *inline policies*, en particulier, il n'existe pas d'AWS managed resource-based policies.\n",
    "\n",
    "Resource-based policy: ex: sur cette table, les users de ce groupe ne sont pas autorisés à faire telle opération. Policy attaché à une resource (à confirmer).\n",
    "\n",
    "User identifie une personne physique\n",
    "Role: regroupement de permissions nécessaire à un utilisateur ou un service pour accéder à des ressources. Ces permissions ne sont attachées ni à un utilisateur, ni à un groupe d'utilisateurs. On donne (l'admin) la permission à un user d'assumer un rôle. Dès lors, le user se déleste de ses permissions de groupe et travaille avec les permissions du rôle. Attaché à une session: les rôles permettent entre autre use case de proprement d'implémenter des permissions temporaires. \n",
    "\n",
    "Un exemple simplifié de use case: un rôle IAM peut permettre d'encadrer un accès extérieur aux ressources du compte et permet de ne pas avoir à créer et à gérer des comptes pour ces entités extérieures à notre organisation (ou au moins authentifiées à l'extérieur du compte). Si on ne souhaite plus leur donner accès, on supprime ou modifie le rôle.\n",
    "\n",
    "Autre use case: permet de donner à des ressources d'accéder à des services AWS. Rôle rattaché à une ressource par forcément à sa création. Les rôles sont rattachés à un service: quand on cherche à rattacher un rôle à une resource d'un service, on ne nous propose que les rôles attachés à ce service. Ex: rôle EC2 permettant de lire dans S3 et d'écrire dans DynamoDB. Dans ce use case, le rôle IAM joue le rôle d'un compte de service.\n",
    "\n",
    "Remarque: Puisque le contrôle d'accès avec une ressource peut se faire à l'aide de policies, pas besoin d'embarquer des credentials dans le code.\n",
    "\n",
    "Attention: Security group: sécurise l'accès à la ressource au niveau du trafic réseau. Un rôle IAM sont l'ensemble des permissions accordé à la resource pour qu'elle effectue ses tâches.\n",
    "\n",
    "Fédération d'identité/single sign-on (SSO): Pas besoin pour l'utilisateur de s'identifier puisqu'il s'est déjà identifié auprès d'une trusted entity. \n",
    "\n",
    "### AWS Cognito\n",
    "[AWS Cognito official documentation](https://docs.aws.amazon.com/cognito/?id=docs_gateway)\n",
    "\n",
    "AWS Cognito est un service d'authentification, d'autorisation et de gestion d'utilisateurs à destination des applications web et mobiles. Les principales ressources d'AWS Cognito (*user pools* et *identity pools*) sont *Region-based*. AWS supporte aussi l'authentification auprès de tiers de confiance (*identity providers*) comme Facebook, Amazon, Google ou Apple et fournit ainsi aux applications l'utilisant une solution de single sign-on (SSO). \n",
    "\n",
    "Les deux principales ressources d'AWS Cognito sont:\n",
    "* Les *user pools*: un *user pool* est un *user directory* (annuaire d'utilisateurs) fournissant des fonctionnalités d'inscription (sign up) et d'authentification (sign in). Chaque *user pool* stocke des informations utilisateur (*user attributes*) accessibles depuis le SDK, quelle que soit la façon dont ils se sont identifiés. Attention à la légalité du stockage de certains attributs (CNIL, RGPD).\n",
    "* Les *identity pools* qui servent à donner accès à des ressources AWS à des utilisateurs. Permet de donner à un utilisateur des *credentials* temporaires pour accéder à des ressources AWS ainsi qu'un identifiant unique permettant de tracer son utilisation de ces ressources. Les *identity pools* supportent les *anonymous guest users* tout comme les utilisateurs authentifiés d'un *user pool* Cognito, d'un IdP, d'un OIDC ou SAML *identity provider*.  \n",
    "\n",
    "Your web and mobile app users can sign in through social identity providers (IdP) like Facebook, Google, Amazon, and Apple. With the built-in hosted web UI, Amazon Cognito provides token handling and management for all authenticated users, so your backend systems can standardize on one set of user pool tokens.\n",
    "\n",
    "You can enable your web and mobile app users to sign in through a SAML identity provider (IdP) such as Microsoft Active Directory Federation Services (ADFS), or Shibboleth. Choose a SAML identity provider that supports the SAML 2.0 standard.\n",
    "\n",
    "With the built-in hosted web UI, Amazon Cognito provides token handling and management for all authenticated users, so your backend systems can standardize on one set of user pool tokens. \n",
    "\n",
    "You can enable your users who already have accounts with OpenID Connect (OIDC) identity providers (IdPs) (like Salesforce or Ping Identity) to skip the sign-up step—and sign in to your application using an existing account. With the built-in hosted web UI, Amazon Cognito provides token handling and management for all authenticated users, so your backend systems can standardize on one set of user pool tokens.\n",
    "\n",
    "AWS STS = SSO\n",
    "\n",
    "https://docs.aws.amazon.com/STS/latest/APIReference/welcome.html\n",
    "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html\n",
    "https://jayendrapatil.com/tag/iam-role/\n",
    "https://iam.harvard.edu/glossary\n",
    "https://pramanaconseil.medium.com/saml-oauth-openid-quel-standard-pour-quelle-utilisation-27ecd6acc0fa\n",
    "\n",
    "Identity pool: sert à identifier l'application.\n",
    "Peut aussi faire de la fédération d'identité\n",
    "Third-party sign-in = Federation\n",
    "\n",
    "AWS Cognito est un service permettant d'identifier les utilisateurs d'applications web et mobile. Le service permet de créer des magasins de profils utilisateurs (*user pools*) pour chaque application en ayant besoin. A distinguer des IAM users qui correspondent aux profils des utilisateurs ayant accès à un compte AWS donné qui diffèrent donc des utilisateurs des applications hébergés sur ce même compte. \n",
    "\n",
    "Remarque: AWS Cognito n'est pas un annuaire LDAP. AWS Directory Service semble plus se rapprocher de ce type de service et semble être un outil de migration pour les entreprises faisant du Windows Active Directory (AD) *on-premise*. \n",
    "\n",
    "### AWS Key Management Service \n",
    "[AWS Key Management Service official documentation](https://docs.aws.amazon.com/kms/latest/developerguide/overview.html)\n",
    "\n",
    "AWS KMS est un ***region-based managed service*** qui permet de créer et gérer facilement des ***Customer Master Keys*** (CMKs). La CMK est une ressource AWS (doté d'un ARN) et est la principale ressource du service KMS. Une CMK est une *logical representation* d'une *master key*: c'est une entité qui contient des métadonnées comme l'ID de la clé, sa date de création, sa description, son état, etc. et bien sûr du matériel cryptographique utilisé pour encrypter ou décrypter des données. Le matériel cryptographique d'une CMK n'est jamais accessible. Les CMKs sont protégées (sauf dans les Regions chinoises) par des *hardware security modules* (HSMs). AWS KMS est intégré avec AWS CloudTrail où on garde trace de l'utilisation de chaque clé pour des besoins d'audits et de *compliance*.\n",
    "\n",
    "Remarque: Un HSM est un composant physique réputé inviolable et dédié à la protection et la gestion de clés utilisées pour de l'encryptage ou de décryptage dans des opérations de signature, d'authentification, etc. Ces composants prennent la forme de cartes qu'on vient directement rattacher à un ordinateur ou un serveur.    \n",
    "\n",
    "Une CMK peut représenter aussi bien une clé symétrique (notamment utilisée pour les opérations d'*encryption at rest*) qu'asymétrique. Elle consiste dans ce dernier cas en une paire de clés RSA privée et publique notamment utilisée pour de la signature.\n",
    "\n",
    "Remarque: La rotation automatique (*fully managed*) du matériel cryptographique des CMK par AWS est une option à activer au niveau de la clé.\n",
    "\n",
    "L'identité du propriétaire (*key owner*) et le gestionnaire (*key manager*) des clés permet d'en distinguer trois types:\n",
    "* ***Customer managed CMK***: CMK créée et gérée (alias, IAM *policies*, rotation, etc.) par l'administrateur ou l'utilisateur du compte.\n",
    "* ***AWS managed CMK***: Ces clés sont créées, gérées et utilisées au nom de l'utilisateur par des services AWS intégrés avec AWS KMS. Un utilisateur ne peut ni les modifier ni les utiliser directement. Ces clés sont toutefois visibles dans la console KMS. \n",
    "* ***AWS owned CMK***: Ces clés sont créées, gérées et utilisées par un service AWS pour ses besoins internes et potentiellement pour plusieurs comptes. Ces clés ne sont pas visibles dans la console KMS.\n",
    "\n",
    "En dehors de l'encryption/décryption, une CMK peut être utilisée pour générer des clés appelées ***data keys*** (symétriques ou asymétriques) elles-mêmes principalement destinées à être utilisées directement par des applications (et non des services AWS qui utilisent des CMK). Elle peuvent être générées en clair (déconseillé) ou encryptées (par la CMK). Un exemple d'application consiste à décrypter une *data key* à l'aide de la CMK l'ayant généré, encrypter des données, recrypter la *data key* à l'aide de la CMK et la stocker à côté des données qu'elle a servi à encrypter. \n",
    "\n",
    "Les CMKs étant des ressources, le contrôle de leur accès se fait à l'aide de *policies*. Il est possible de définir pour les CMKs des *resource-based policies* appelées *key policies* à utiliser seules ou en combinaison avec des *identity-based policies* (IAM *policies*) ou d'autres mécanismes complémentaires de contrôle d'accès (comme les *grants* dans le cas des CMKs). \n",
    "\n",
    "Remarque: Pour la plupart des resources, les *identity-based policies* (IAM *policies*) peuvent suffire (voire sont le seul moyen) de contrôler l'accès à celles-ci, des *resource-based policies* pouvant être utilisées en complément si elles peuvent être définies pour le type de ressource visé. Les CMKs sont des exceptions dans le sens où leur contrôle d'accès exige au minimum de définir des *resource-based policies* éventuellement complétées par des *identity-based policies* (IAM *policies*) ou d'autres mécanismes complémentaires. Attention, le JSON décrivant une *key policy* ne peut dépasser 32KB.\n",
    "\n",
    "### AWS Organizations \n",
    "[AWS Organizations official documentation](https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html)\n",
    "\n",
    "AWS Organizations est un service permettant d'administrer de façon centralisée de multiples comptes AWS. Les comptes sont organisés en groupes et sous groupes appelés *Organization Units* (OUs). AWS Organization permet notamment de gérer les *policies* de façon centralisées. AWS Organization permet en effet de créer des ressources spéciales appelées *Service Control Policies* (SCP) qui permettent de centraliser le contrôle de l'utilisation des ressources dans différents comptes AWS.\n",
    "\n",
    "AWS Organizations fournit également des facilités de création automatique de comptes AWS permettant par exemple de créer facilement des comptes *sandbox*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 8: Elasticity, availability and monitoring\n",
    "* ***Fault tolerance***: On garantit le niveau de service, implique de la redondance. Si notre service demande 6 VMs pour bien fonctionner: on prend 6 VMs x 2AZ. Version cost-effective: 3 VMs x 3 AZ. \n",
    "* ***High availability*** : Continuité de service en acceptant le mode dégradé: 3 VMs x 2AZ suffit. La *fault tolerance* sera plus chère que la *high availability*, elle est plus restrictive. L'idée est que le service soit toujours disponible même dans l'éventualité d'une défaillance plus ou moins grave.\n",
    "\n",
    "* ***Scalability***: Capacité à s'accomoder d'une augmentation de capacité sans changement de design.\n",
    "\n",
    "Bases de données:\n",
    "* Read replicas: Permette de faire scaler horizontalement la lecture de bases de données. Peuvent être créées jusque dans une autre région.\n",
    "* Sharding: fait pour les grosses bases. Permet d'améliorer les perfs notamment en écriture (comparé à la situation où on a qu'un seul master qui prend toutes les écritures). AWS ne propose pas de soft de sharding, c'est à nous de l'intégrer niveau applicatif.\n",
    "* DynamoDB: Veiller à choisir sa partition key de manière à éviter l'apparition de hot partition qui concentrent beaucoup plus d'écriture que les autres.\n",
    "\n",
    "### Amazon CloudWatch \n",
    "[Amazon CloudWatch official documentation](https://docs.aws.amazon.com/cloudwatch/?id=docs_gateway)\n",
    "\n",
    "Amazon CloudWatch est un service de *monitoring*. CloudWatch permet de monitorer à la fois les ressources AWS utilisées sur le compte mais aussi nos applications s'exécutant sur AWS. CloudWatch peut être vu comme un *metric repository* qui est utilisé pour collecter et suivre (*track*) des métriques (*logs* compris). CloudWatch permet également de réaliser un ensemble de statistiques simples sur ces métriques. \n",
    "\n",
    "CloudWatch permet d'associer des alertes à des métriques. Ces alertes peuvent être paramétrées pour envoyer des notifications ou automatiquement ajuster les ressources en cours d'utilisation (ex: ajouter des instances EC2 face à une charge qui augmente ou à l'inverse *scale down* face à une charge qui diminue). \n",
    "\n",
    "Les métriques sont stockées au niveau des Regions mais il est possible d'utiliser CloudWatch cross-Region pour agréger des statistiques produites dans différentes Regions. \n",
    "\n",
    "Certaines métriques ne sont pas collectées par défaut (ex: conso RAM pour les instances EC2). Il faut alors créer une *custom metric* qui sera collectée par un agent CloudWatch directement installé au niveau de la ressource (service payant).\n",
    "\n",
    "### AWS CloudTrail\n",
    "[AWS CloudTrail official documentation](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html)\n",
    "\n",
    "AWS CloudTrail permet de logger comme événements (*events*) toutes les actions entreprises par un utilisateur, un rôle ou un service AWS sur un compte AWS donné. AWS CloudTrail enregistre en particulier tous les *API calls* faits sur le compte. Les actions enregistrées incluent les actions exécutées via la console web, la CLI AWS et les SDKs/APIs AWS.\n",
    "\n",
    "L'enregistrement d'une activité ou action réalisée sur le compte AWS par CloudTrail est appelé un CloudTrail *event*. Il existe une grande variété de CloudTrail *events* regroupés en trois grande catégories:\n",
    "* Les *management events*: Comme la création d'un *security group*, la modification d'une table de routage, etc.\n",
    "* Les *data events*: Comme les *object operations* sur S3, les invocations de fonctions Lambda, etc. Ce sont les plus nombreux.\n",
    "* Les *Insights events*: Ils ne sont collectés que si CloudTrail Insights est activé, ce sont des événements reflétant une activité inhabituelle sur le compte, par exemple notre compte ne compte que rarement plus de 20 *calls* à une certaines API par minute mais qu'on en observe 100.\n",
    "\n",
    "CloudTrail permet de consulter et de télécharger les *events* des 90 derniers jours (CloudTrail *event history*). On peut vouloir stocker certains *events* plus longtemps dans un espace de stockage permettant ensuite des analyses plus approfondies comme S3. On peut alors créer ce qu'on appelle des CloudTrail *trails*. Un *trail* est simplement une configuration permettant de filtrer les *events*, d'éventuellement les encryptés et de les stocker dans S3 et/ou CloudWatch. La configuration d'un *trail* peut inclure celle d'un *topic* SNS nous notifiant de la livraison de ces *logs*. Il existre deux grands types de *trail*, les *trails* ne s'appliquant aux *events* cibles d'une seule Region et ceux s'appliquant aux *events* cibles quelle que soit la Region, les futures Regions étant automatiquement ajoutées. \n",
    "\n",
    "C'est un service à destination de l'administration du compte et est notamment utilisé pour les audits de sécurité et la *compliance*.\n",
    "\n",
    "Remarque: VPC Flow Logs est une *feature* d'Amazon VPC qui permet de collecter toutes les informations se rapportant au trafic IP en provenance ou à destination des *network interfaces* d'un VPC. Les données produites par VPC Flow Logs sont stockées dans AWS CloudWatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 9: Automation\n",
    "\n",
    "### AWS CloudFormation\n",
    "[AWS CloudFormation official documentation](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html)\n",
    "\n",
    "AWS CloudFormation est un service similaire d'*infrastructure as code* similaire à Terraform (qui est multi-cloud). AWS CloudFormation se repose sur trois principales resources/concepts:\n",
    "* Les *templates*: Un template est un fichiers texte (YAML ou JSON) qui sert de *blueprint* à la construction d'une *stack*. C'est dans un fichier *template* qu'on va décrire l'ensemble des ressources qui vont constituer notre infrastructure, notre application, notre *stack*.\n",
    "* Les *stacks*. Une *stack* correspond à l'ensemble des resources AWS créées à partir d'un *template*. AWS CloudFormation se charge de provisioner et de configurer ces ressources pour nous à partir des informations rassemblées dans le *template*. Une *stack* est une collection de ressources qui peut être créée, updatée ou détruite. \n",
    "* Les *change-sets*. Un *change-set* est un objet créé à partir d'un template et d'une version updatée de ce même template. Il permet de visualiser le détails des opérations qu'AWS CloudFormation exécuterait lors de l'update d'une *stack* basée sur la première version du template. Un *change-set* permet de faire un *dry run* avant d'updater une stack et d'anticiper par exemple des événements qu'un *rollback* ne suffiera pas à annuler (ex: suppression d'une base de données).  \n",
    "\n",
    "AWS CloudFormation vient aider à:\n",
    "* Simplifier le management de l'infrastructure: On n'a plus à créer les différents éléments manuellement et à faire la navette entre les différentes ressources pour les faire marcher ensemble. On gère tout au niveau d'un *template*. Idem, toutes les ressources étant rassemblées sous une *stack*, les supprimer ou les updater est aussi simple que de supprimer ou updater la stack. C'est moins de complexité, de temps passé à configurer et de risque d'erreur. \n",
    "* Être capable de répliquer l'infrastructure dans une autre Region (si celle où on est hébergée devient indisponible).\n",
    "* Contrôler et suivre les changements apportés à l'infrastructure. Lors d'une *update*, AWS CloudFormation garde notamment trace de l'ancien état de l'infrastructure et permet de *rollback* très facilement. \n",
    "\n",
    "Concernant les *templates*, AWS CloudFormation offre ne nombreuses fonctionnalités:\n",
    "* On décrit les composants de sa *stack* dans une section \"Resources\".\n",
    "* Il existe une section Conditions qui permet de conditionner la stack finalement déployée. Ex: On fait du multi-AZ en prod mais pas en dev.\n",
    "* Un *template* peut retourner des valeurs (ex: les Id de ressources créées) qui peuvent être utilisées par des *templates* exécutés après. \n",
    "* On peut gérer des contraintes de précédence (cf. `Ref`) pour éviter par exemple que la création d'une instance EC2 à l'aide du *template* plante à cause du simple fait que le *security group* dont son paramétrage dépend n'est créé que plus loin dans le *template*.\n",
    "\n",
    "Remarques : \n",
    "* Il n'est pas recommandé de faire un énorme *template* qui fait tout, les mises à jours en seront d'autant plus faciles. Décomposer par exemple en Frontend, Backend, Network, Security, etc.\n",
    "* AWS CloudFormation vient avec une fonctionnalité de *drift detection*: on compare l'état de l'infrastructure avec celle qui serait créée à partir du *template*. Cela permet notamment de détecter des modification introduites manuellement (ressources en plus ou en moins, modifications de configuration). Attention cependant sur les *drifts* de configuration: AWS CloudFormation ne contrôle les écarts qu'avec ce qui a été défini explicitement dans le *template*. En particulier, passer une configuration de sa valeur par défaut à une autre valeur n'est pas détecté à moins que cette valeur par défaut ait été rendue explicite dans le *template*. Dit autrement, c'est le *template* qui décrit les écarts qu'on souhaite suivre.\n",
    "\n",
    "#### [AWS Quick Starts](https://aws.amazon.com/quickstart/?nc1=h_ls&quickstart-all.sort-by=item.additionalFields.sortDate&quickstart-all.sort-order=desc) \n",
    "AWS Quick Starts met à disposition de nombreux templates créés par des partenaires AWS et validés par les ingénieurs AWS.\n",
    "\n",
    "#### [AWS Architecture Center](https://aws.amazon.com/architecture/?nc1=h_ls)\n",
    "L'AWS Architecture Center rassemble un ensemble d'architecture génériques éprouvées pour une grande variété de cas d'usages ainsi que des ressources détaillant les principes à la base d'une bonne architecture.\n",
    "\n",
    "Pour faire de beaux dessins d'architecture: \n",
    "* [Cloudcraft](https://www.cloudcraft.co/)\n",
    "* [DrawIO](https://app.diagrams.net/).\n",
    "\n",
    "### AWS Systems Manager\n",
    "[AWS Systems Manager](https://docs.aws.amazon.com/systems-manager/latest/userguide/what-is-systems-manager.html)\n",
    "\n",
    "Destiné à l'équipe Infra, fully-managed service permettant de gérer nos flottes de VMs que ce soient des instances EC2 ou des VMs *on-premise*. Permet de facilement gérer l'écosystème *software* installé sur chaque machine, de patcher l'OS des machines, de configurer l'OS, d'exécuter des commandes sur les machines sans avoir à se connecter individuellement sur chacune d'entre elles.\n",
    "\n",
    "### AWS OpsWorks\n",
    "[AWS OpsWorks](https://docs.aws.amazon.com/opsworks/latest/userguide/welcome.html)\n",
    "\n",
    "*Fully-managed configuration management service* qui permet d'utiliser Chef, Puppet et un jour Ansible pour gérer la configuration de son infrastructure AWS. Là où AWS CloudFormation nous construit l'infrastructure, AWS OpsWorks vient déployer l'ensemble de l'applicatif sur cette infra à l'aide d'AWS OpsWorks Stacks.\n",
    "\n",
    "### AWS Elastic Beanstalk\n",
    "[AWS Elastic Beanstalk official documentation](https://docs.aws.amazon.com/elastic-beanstalk/?id=docs_gateway)\n",
    "\n",
    "AWS Elastic Beanstalk est un service permettant le déploiement aisé d'applications, notamment web. L'idée est que le développeur n'a à fournir que l'ensemble des sources de son application (sous une forme dépendant de la plateforme destinée à l'héberger, un fichier .war dans le cas d'une application Java par exemple), Elastic Beanstalk se charge de provisionner l'environnement adapté. On peut déployer une application sans avoir eu à préalablement architecturer l'infrastructure sur laquelle on la déploie. On se concentre sur le code, Elastic Beanstalk se charge de faire le travail qu'on aurait eu à faire avec AWS CloudFormation et/ou AWS OpsWorks par exemple. Elastic Beanstalk est donc un service de plus haut niveau, plus managé. Elastic Beanstalk se charge de provisioner un ensemble de ressources AWS formant un environnement résilient, *scalable*, monitoré, etc. Une fois un environnement lancé, l'utilisateur peut encore le modifier et y déploie les différentes versions de son application. Elastic Beanstalk fournit aussi différentes solutions de déploiement pour réaliser la transition d'une version à une autre (*rolling*, *blue-green*, etc.).\n",
    "\n",
    "Elastic Beanstalk se repose sur quelques concepts/abstractions clés:\n",
    "* **Application**: Une application Elastic Beanstalk est une collection de composants Elastic Beanstalk incluant les environnements, les versions et les configurations d'environnements. On peut se visualiser une application Elastic Beanstalk comme un répertoire (*folder*).\n",
    "* **Application version**: Une *application version* correspond à une \"*labeled iteration of deployable code for a web application*\". Concrètement, une *application version* pointe vers objet S3 correspondant à un artifact de code déployable. Une *application version* est un composant d'une application Elastic Beanstalk. Une application peut inclure plusieurs versions, chacune unique. On peut déployer toute application version ayant déjà été uploadée vers l'application Elastic Beanstalk.\n",
    "* **Environment**: Un environnement est une collection de ressources AWS faisant tourner une *application version*. Un environnement n'exécute qu'une seule *application version* à la fois, mais plusieurs *application versions* potentiellement différentes peuvent être exécutées simultanément dans plusieurs environnements.\n",
    "* **Environment tier**: L'*environment tier* détermine les ressources finalement provisionnées par AWS et correspond finalement au type d'application à faire tourner. Il en existe principalement deux: le *web server environment tier* et le *worker server environment tier*. Dans le premier destiné aux applications web, Elastic Beanstalk va provisionner un ELB, un Auto Scaling group et une ou plusieurs instances EC2. Tout *web environment* se voit associer une URL (ex: `myapp.us-west-2.elasticbeanstalk.com`) qui au niveau de Route 53 est un *alias* pour l'URL du *load balancer*. Pour le second, Elastic Beanstalk va provisionner un Auto Scaling group, une ou plusieurs instances EC2 et le cas échéant, une queue SQS. Ces environnements sont destinés à des applications *backend* récupérant leurs tâches à exécuter dans une queue. \n",
    "* **Platform**:  Quand on développe son application, on la construit à destination d'une ou plusieurs plateformes. Une plateforme est l'ensemble constitué d'un OS, des *runtimes* d'un language (Go, Java, Node.js, PHP, Python, Ruby) et de serveurs webs ou applicatifs (Tomcat, Passenger, Puma)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 10: Caching\n",
    "On met en cache aussi bien du contenu statique (images, vidéos) que du dynamique (champ \"Pierre's account\") tant qu'ils sont lus/requêtés fréquemment ou mis à jour peu souvent pour le contenu dynamique. Quand le contenu est récupéré d'une DB le cache peut permettre d'économiser l'exécution de multiples requêtes coûteuses et/ou contribuer à soulager la DB si ce sont souvent les mêmes données qui sont requêtées.\n",
    "\n",
    "### Amazon CloudFront\n",
    "[Amazon CloudFront official documentation](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html)\n",
    "\n",
    "Amazon CloudFront est un service de Content Delivery Network destiné à produire les plus faibles latences possibles dans la distribution de contenus webs statiques ou dynamiques. L'idée consiste à mettre en cache les fichiers (même volumineux comme de la vidéo) à servir dans sur des machines localisées le plus proche possible des clients (emplacements physiques appelés CloudFront PoP (*Point of Presence*) ou plus couramment *edge locations*, distincts des *datacenters* formant les AZs). On parle pour Amazon CloudFront de *edge caching*. Les requêtes adressées à CloudFront sont routées de façon optimisées à l'intérieur de l'infrastructure réseau Amazon. Le nombre de rebonds à effectuer par la requête / de réseaux à traverser jusqu'à atteindre l'*edge location* est minimisé, contribuant à diminuer encore la latence. \n",
    "\n",
    "Côté développeur, l'utilisation de CloudFront pour le service de fichier se fait via la création d'une \"distribution CloudFront\" qui décrit à CloudFront l'ensemble des fichiers qu'il peut avoir à servir et auprès de quels serveurs les obtenirs, ces serveurs étant appelés *origin servers* et peuvent par exemple être un *bucket* S3, un serveur web, etc. \n",
    "\n",
    "Une distribution CloudFront est associée à un nom de domaine (ex: `d111111abcdef8.cloudfront.net`, l'URL du fichier `logo.jpg` situé à la racine de l'*origin server* sera alors `http://d111111abcdef8.cloudfront.net/logo.jpg.`). On peut aussi paramétrer une distribution CloudFront de façon à lui faire utiliser notre propre *domain name* (Ex: http://www.example.com) pour notre distribution (on accède alors à `logo.jpg` servi par CloudFront simplement avec l'URL `http://www.example.com/logo.jpg`)\n",
    "\n",
    "Dans le cas d'une requête faite pour un fichier faisant partie d'une distribution CloudFront, si ce fichier ce trouve déjà au niveau de l'*edge location* vers laquelle la requête a été routée, alors celui-ci est servi immédiatement et avec la plus faible latence atteignable. Dans le cas contraire, CloudFront récupère le fichier auprès de l'*origin server* indiqué dans la distribution.\n",
    "\n",
    "La meilleure façon d'invalider le cache est de fixer un *time-to-live* (TTL) au bout duquel le contenu est automatiquement invalidé. Un changement du nom de l'objet mis en cache au niveau de l'*origin server* occasionne aussi une invalidation du cache. Une invalidation forcée est possible mais peu recommandée car coûteuse et peu efficace.\n",
    "\n",
    "### AWS WAF\n",
    "[AWS WAF official documentation](https://docs.aws.amazon.com/waf/latest/developerguide/waf-chapter.html)\n",
    "\n",
    "AWS WAF est un *firewall* applicatif (*web application firewall*) qui permet de contrôler les requêtes HTTP(S) adressées à Amazon CloudFront, Amazon API Gateway ou Amazon Elastic Load Balancer notamment. AWS WAF permet de contrôler l'accès à du contenu hébergé sur AWS. En particulier, il permet à un site public de se protéger contre un ensemble d'attaques communes. AWS WAF va en gros contrôler le contenu des requêtes HTTP(S): leur IP d'origine, leur taille, les valeurs du *header*, si elles contiennent une requête SQL (possible attaque *sql injection*) ou un script potentiellement malicieux (*cross-site scripting*), etc. AWS WAF agit donc au niveau de la couche OSI 7.\n",
    "\n",
    "### AWS Shield\n",
    "[AWS Shield official documentation](https://docs.aws.amazon.com/waf/latest/developerguide/shield-chapter.html)\n",
    "\n",
    "AWS Shield vient en complément de AWS WAF et offre une protection plus spécifiquement axée contre les *DDoS attacks*. Les services protégées sont globalement les mêmes, on peut ajouter Route 53 à la liste des services pris en charge par rapport à AWS WAF.\n",
    "\n",
    "### Amazon ElastiCache\n",
    "[Amazon ElastiCache official documentation](https://docs.aws.amazon.com/elasticache/?id=docs_gateway)\n",
    "\n",
    "Amazon ElastiCache est un service *fully-managed* (les cluster ElastiCache sont notamment *highly available* grace à un déploiement multi-AZ) fournissant un cache applicatif basé soit sur Redis ou Memcached. C'est un service managé, *scalable*, permettant de déployer et de gérer facilement un *distributed cache environment*, en l'espèce, un cluster Redis/Memcached (pouvant scaler jusqu'à 90 et 20 noeuds respectivement). La mise en cache est gérée au niveau de l'application: ce n'est pas le service de *caching* qui décide de mettre en cache ou non. Les interfaces d'Amazon ElastiCache sont conçues de façon à ce que des applications utilisant déjà Redis/Memcached puissent être migrées vers AWS presque sans aucune modification.\n",
    "\n",
    "La mise en cache se fait le plus souvent sur le mode du *lazy loading*: on ne met en cache que ce qui est lu. Celà évite de peupler le cache de données peu sollicitées mais vient avec une *cache miss penalty*: pour toute donnée absente du cache, il faut aller la chercher dans la base de données.\n",
    "\n",
    "Plus généralement, on a intérêt à recourir à de la mise en cache pour des données:\n",
    "* Qui changent relativement peu (l'idée n'est pas que le cache remplace la DB ou de saturer le cache avec plus de données que nécéssaire). \n",
    "* Qui sont accédées souvent (sinon des données peu fréquemment accédées prennent de la place dans le cache qui est un service couteux).\n",
    "* Qui sont couteuse à récupérer.\n",
    "\n",
    "Attention également au rafraichissement des données: si une entrée de la DB est mise à jour, son ancienne version est encore présente dans le cache et reste la valeur récupérée par l'utilisateur. On doit se préoccuper:\n",
    "* De la sensibilité des utilisateurs au données périmées (*stale data*).\n",
    "* D'invalider le cache en cas d'*update* des données (se fait au niveau applicatif).\n",
    "\n",
    "Remarque: Le *write through* permet de résoudre en partie le problème des *stale data*: une écriture est faite dans le cache et dans la base, les données sont donc toujours à jour. Cette méthode vient cependant avec des désavantages: chaque écriture implique deux opérations, il est possible de remplir le cache avec des données finalement peu lues (problème qu'on peut résoudre avec un TTL, ce qui permet d'avoir le meilleur du *write through* et du *lazy loading*), si des données sont updatées souvent, le cache est beaucoup sollicité en écriture alors que ce n'est pas son rôle (*cache churn*).\n",
    "\n",
    "### Autres solution de caching: DynamoDB, DynamoDB Accelerator\n",
    "Dans le cas de la gestion de sessions, celles-ci sont le plus souvent soit managées par un serveur dédié avec l'éventualité de problèmes de latences (il faut à chaque nouvelle requête s'enquérir auprès du serveur d'une éventuelle session déjà ouverte), soit managées à l'aide de *sticky sessions* où l'information de l'état de la session est stocké sur le serveur web (la validité de la session étant déterminée à l'aide de *client-side cookies*, de paramètres settés au niveau du *load balancer*, etc.). L'avantage de la dernière méthode est qu'on a plus à recourir à des serveurs extérieurs, l'information de session étant directement disponible sur le *web server*. Cette méthode a cependant deux inconvénients:\n",
    "* Si une machine tombe, on perd l'information relative aux sessions qu'elle traitait,\n",
    "* Elle vient perturber la répartition de la charge en cas de *scaling*: les sessions actives empêchent la machine de récupérer du trafic additionnel. \n",
    "\n",
    "Remarque: Dans le cas précis de la gestion de session, il s'agit d'un *use case* de *caching* où ElastiCache comme DynamoDB peuvent être utilisés.\n",
    "\n",
    "Une solution est de revenir à une gestion externalisée du cache mais avec une solution à très faible latence: DynamoDB. On rappelle que la latence de DynamoDB se compte en *singe-digit milliseconds* qu'on peut faire descendre en microsecondes avec DynamoDB Accelerator (DAX)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 11: Building decoupled architectures\n",
    "\n",
    "Exemple introductif d'une application web découpée en trois *tiers*: un *web tier* situé dont toute les machines sont enregistrées dans le DNS de Route53, un *app tier* et une *database layer*. Une première approche pour découpler les différentes couches serait d'introduire un *load balacer* entre Route 53 et le *web tier* puis entre le *web tier* et l'*app tier*. Cependant on garde une vulnérabilité: si l'*app tier* est à fond, toutes les requêtes envoyées par le *web tier* à l'ELB faisant l'interface risquent de *time-out* et on risque de perdre leur contenu. Solution: une file (*queue*) qui permet de découpler davantage en rendant le traitement des requêtes totalement asynchrone. Le *web tier* n'est plus dépendant de la capacité à traiter de l'*app tier*.\n",
    "\n",
    "### Amazon Simple Queue Service\n",
    "[Amazon SQS official documentation](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html)\n",
    "\n",
    "Amazon SQS est un service de *queuing fully managed*, *highly available*, *scalabe* et *durable* (les données stockées dans la *queue*/les serveurs SQS sont redondées sur plusieurs AZ). Le contenu d'un message placé dans une *queue* SQS **ne peut dépasser 256kB** en taille. Il existe deux principaux types de *queue*:\n",
    "* **Standard *queue***: La *standard queue* se caractérise par un *throughput* virtuellement illimité, par une *At-Least-Once Delivery* (il peut rarement arriver qu'un serveur stockant une des répliques d'un message soit indisponible lors de la suppression d'un message, celui-ci n'est alors pas supprimé et peut être traité une seconde fois) et un *Best-Effort Ordering* (l'ordre d'arrivée des message peut occasionnellement ne pas être respecté). \n",
    "* **FIFO *queue***: Par opposition à la *standard queue*, la FIFO *queue* garantit l'ordre d'arrivée des messages et offre un *Exactly-Once Processing* (on ne peut pas introduire deux fois - supposément par erreur - le même message dans la *queue* durant un intervalle de temps appelé le *deduplication interval*). Ces garanties viennent au prix d'une capacité plus limitée en termes de transactions par seconde.\n",
    "\n",
    "**Un consommateur ayant reçu et traité un message d'une *queue* SQS doit l'effacer lui même de la *queue*.**\n",
    "\n",
    "Les *standard *queue* offrent un *throughput* en termes de transactions par secondes virtuellement illité. A l'opposé, les FIFO *queues* sont limitées à 300 messages/opérations *send*/*receive*/*delete* par seconde. Au maximum autorisé pour les *batch operations* (10 messages par opérations), cela porte le *throughput* maximal des FIFO *queues* à 3000 messages/s.\n",
    "\n",
    "Remarque: Amazon SQS est *Region-scoped*, une *queue* SQS n'est donc pas une ressource se plaçant dans un VPC. On peut toutefois créer un *VPC endpoint* dédié pour pouvoir y accéder sans en passer par le réseau public.\n",
    "\n",
    "Amazon SQS propose aussi une option appelée *dead-letter queue* qui recueille les messages qui auront généré une erreur lors de leur traitement. Cela permet d'éviter que des messages générant des erreurs restent indéfiniement dans la *queue* (car générant une erreur, ils n'en sont pas retirés) et de faciliter le débuggage.\n",
    "\n",
    "Autres paramètres importants:\n",
    "* **Visibility timeout**: Quand un consommateur commence à traiter un message, le message reste dans la queue mais est invisible aux autres consommateurs. Il ne le reste que le temps du visibility timeout qui doit être suffisant pour que le traitement et le retrait définitif du message par l'application qui l'a traité puissent s'effectuer sous peine que des messages soient traités plusieurs fois. \n",
    "* **Long polling**: quand un consommateur essaye de récupérer un message, il attend un certain temps de se voir attribué un message si aucun n'est immédiatement disponible. Si aucun n'est disponible au bout du time-out, la queue renvoit un message vide. Inconvénient, on a eu deux requêtes d'échangées et si la queue est souvent vide, cela peut faire beaucoup de requêtes improductives qui entrent dans la factuation. Allonger le temps de polling permet d'en faire diminuer ce nombre. Ce mécanisme de *polling* fait qu'une *queue* n'est pas adaptée aux *use cases* où les données arrivent irrégulièrement (la *queue* est potentiellement souvent vide comme les nombreuses requêtes qui lui sont périodiquement adressées par les consommateurs).\n",
    "* **Batch actions**: Certaines actions de base peuvent être effectuées par *batch*, réduisant d'autant le nombre de requêtes échangées et le montant finalement facturé.\n",
    "\n",
    "### Amazon Simple Notification Service\n",
    "[Amazon SNS official documentation](https://docs.aws.amazon.com/sns/latest/dg/welcome.html)\n",
    "\n",
    "Amazon SNS est un service managé permettant de transmettre des messages de *publishers* vers des *subscribers*: pas besoin pour le *publisher* de joindre au message les adresses de tous les *subscribers*, le service se charge de passer le message à l'ensemble *subscribers* ayant souscrit au *topic*. Les *publishers* publient leurs messages auprès du *topic* SNS qui propage ensuite le message à l'ensemble des *subscribers* du *topic*. Amazon SNS offre ainsi un moyen de communication asynchrone entre *publishers* et *subscribers*. Les *subscribers* peuvent aussi bien être d'autres services Amazon (SQS, Kinesis Firehose, Lambda, etc.) que des personnes physiques qui sont notifiées par email, SMS ou notification *push*. \n",
    "\n",
    "Remarque: Comme Amazon SQS, un *topic* SNS est une ressource de Region, les données envoyées sur un *topic*/vers les serveurs SNS sont redondées sur plusieurs AZ (*durability*).\n",
    "\n",
    "Comme pour Amazon SQS, la taille de messages publiés auprès d'un *topic* SNS ne peuvent excéder 256 kB. Le *throughput* d'un *topic* est virtuellement illimité.\n",
    "\n",
    "Si un *subscriber* n'est pas disponible, Amazon SNS propose des politiques de *retry*. Les messages publiés sur un *topic* SQS ne sont pas persistés: un message délivré avec succès à l'ensemble des *subscribers* ou pour lequel le nombre de *retry attemps* est épuisé est irréversiblement supprimé à moins d'avoir attaché une *dead-letter queue* au *topic*.\n",
    "\n",
    "Remarque: Il existe deux types de topic, *standard* et FIFO, le second garantissant que les messages seront propagés dans l'ordre dans lequel ils ont été publiés. Seule une FIFO *queue* SQS peut souscrire à un FIFO *topic*.\n",
    "\n",
    "Un *topic* SNS délivre ses messages selon un modèle *one-to-many* et de façon passive (les *subscribers* n'ont rien à faire pour recevoir le message, en revanche et contrairement à une *queue* SQS, le message est délivré au *subscriber* sans se préoccuper de sa disponibilité). Une *queue* SQS délivre ses messages selon un modèle *one-to-one* et de façon active: pour recevoir un message, un client de la *queue* doit en faire activement la requête et lui seul recevra le message qui lui est assigné.\n",
    "\n",
    "### Amazon MQ\n",
    "[Amazon MQ official documentation](https://docs.aws.amazon.com/amazon-mq/latest/developer-guide/welcome.html)\n",
    "\n",
    "Amazon MQ est un *managed message broker service*. Un *message broker* permet à différentes applications et composants de communiquer même si ceux-ci ne reposent pas sur les mêmes OS, languages de programmation, protocoles, etc.\n",
    "\n",
    "Toute application reposant sur les protocoles et APIs de messageries supportés par Amazon MQ est compatible et peut travailler avec un *message broker* Amazon MQ (qui se repose au choix sur Apache ActiveMQ ou RabbitMQ). Amazon MQ est donc particulièrement approprié pour migrer vers un *message broker cloud* dont la gestion est déléguée au *cloud provider*.\n",
    "\n",
    "Amazon SQS et Amazon SNS sont respectivement des *queues* et *topic services* *highly scalable* et simples d'utilisation qui en particulier n'exige pas de leur utilisateur la configuration d'un *message broker* (même si elles se reposent sur ce genre de composants). L'utilisation de Amazon SQS et Amazon SNS est donc recommandée pour le design de nouvelles applications, Amazon MQ sert à migrer vers le *cloud* des applications *legacy* se reposant sur l'utilisation de *messages brokers* compatibles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 12: Microservices and Serverless Architectures\n",
    "\n",
    "### Amazon Elastic Container Service (ECS) & AWS Fargate \n",
    "* [Amazon ECS official documentation](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html)\n",
    "* [AWS Fargate official documentation](https://docs.aws.amazon.com/AmazonECS/latest/userguide/what-is-fargate.html)\n",
    "\n",
    "Amazon Elastic Container Service (Amazon ECS) est un service permettant d'exécuter, stopper et gérer des conteneurs d'exécutant sur un cluster. Un préalable pour exécuter une application avec Amazon ECS est que ses différents composants aient été architecturés de façon à s'exécuter dans un conteneur. On doit en particulier pouvoir disposer d'images pour chacun de ces composants, images stockés dans un *image registry*, celui-ci pouvant être par exemple privé et *self managed*, privé et managé (Amazon ECR) ou public (DockerHub).\n",
    "\n",
    "Amazon ECS exécute des *tasks* sur des clusters aptent à exécuter des conteneurs. Un cluster ECS est une ressource régionale qui vit dans un VPC. On distingue deux types de cluster ECS: \n",
    "* Les cluster non-managés: au développeur de créer depuis ECS un cluster d'instances EC2. Il est possible de créer des Auto Scaling group et de faire en sorte que le *scaling* soit managé par ECS.\n",
    "* Les clusters managés: les conteneurs sont exécutés sur AWS Fargate qui manage une infrastructure *serverless* (le développeur ne s'occupe à aucun moment de la spécification, du *provisioning* et de la gestion des ressources sur lesquelles s'exécutent les conteneurs).\n",
    "\n",
    "Amazon ECS répartit ses clusters sur plusieurs AZ afin d'assurer une *high availability* \n",
    "\n",
    "Amazon ECS exécute des *tasks* sur ses clusters. Une *task* est une instance de *task definition* sur un cluster. Une *task definition* est un fichier texte au format JSON décrivant le ou les conteneurs constituant l'application (et les ports à ouvrir, les volumes à utiliser, le type de cluster à utiliser, roles IAM, etc.). Le *scheduler* d'Amazon ECS (qui peut être un *third-party scheduler* comme Apache Mesos) est reponsable du placement des *tasks* à exécuter sur le cluster. Avec Amazon ECS, on le lance pas directement et simplement des conteneurs mais des *tasks*.\n",
    "\n",
    "Quelque soit le type de cluster, on trouve sur chaque machine un *ECS agent* qui reçoit ses instructions (*scheduled tasks*) de la part du *scheduler* ECS, récupère les images, lance les conteneurs et monitore leur progression et leur utilisation des ressources qui leur sont allouées.\n",
    "\n",
    "AWS Fargate s'utilise avec Amazon ECS et permet d'exécuter des conteneurs sans avoir à se préoccuper des instances EC2 constituant le cluster. AWS Fargate est *highly available*, *scalable* et surtout *fully managed*: on a plus à choisir d'*instance type*, provisionner des instances EC2, les configuer, décider de la politique de *scaling*, de la répartition géographique des instances, etc. On a simplement à spécifier dans la *task definition* les *ressources requirements* en termes de CPU/RAM, la configuration réseau et les roles IAM.\n",
    "\n",
    "Remarque: AWS Fargate garantit l'isolation de deux *tasks* s'exécutant sur un cluster Fargate: les deux *tasks* ne partagent ni le même *kernel*, ni les mêmes ressources CPU/RAM si la même Elastic Network Interface.\n",
    "\n",
    "### Amazon Elastic Kubernetes Service\n",
    "[Amazon EKS official documentation](https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html)\n",
    "\n",
    "Amazon Elastic Kubernetes Service (EKS) est un service managé permettant d'utiliser Kubernetes sur AWS sans avoir à installer, opérer et maintenir nos propres noeuds Kubernetes. Amazon EKS garantit une *high availability* en répartissant pour un même cluster Kubernetes différents *control planes instances* sur différentes AZ. EKS détecte et remplace automatiquement les *control planes instances* trouvées *unhealthy*.\n",
    "\n",
    "Un cluster EKS est hébergé dans un VPC. Amzaon EKS s'intègre avec de nombreux services Amazon dont en particulier: Amazon ECR, VPC, ELB et IAM.\n",
    "\n",
    "Les données sont persistées au choix sur des volumes EBS, un *file system* EFS ou FSx for Lustre.\n",
    "\n",
    "### Amazon Elastic Container Registry\n",
    "[Amazon ECR official documentation](https://docs.aws.amazon.com/ecr/?id=docs_gateway)\n",
    "Amazon Elastic Container Registry (ECR) est un service de *container image registry* sécurisé, *scalable*, et * highly available* offrant la possibilité d'héberger et de récupérer des images Docker ou tout autre *artifact* OCI (Open Container Initiatives) au sein d'*image repositories*. Les permissions sont gérées avec des *resource-based policies* IAM et qui gèrent les accès des utilisateurs ou d'instances EC2 aux images. Un *repository* ECR est une *Region-based resource* mais peut être répliquée *across Regions* ou *across accounts*.\n",
    "\n",
    "### Amazon Lambda\n",
    "[Amazon Lambda official documentation](https://docs.aws.amazon.com/lambda/latest/dg/welcome.html)\n",
    "\n",
    "Fully managed.\n",
    "\n",
    "Remarque: Une fonction Lambda ne s'exécute pas dans un VPC mais la problématique apparaît quand on souhaite que la fonction se connecte à des ressources hébergées dans un *private subnet*. Amazon Lambda va alors créer une Elastic Network Interface par *private subnet* (ce qui prend un peu de temps la première fois). La fonction se connectera au *subnet* via cette ENI. L'ENI dédiée à un *private subnet* en particulier peut être utilisée par plusieurs fonctions.\n",
    "\n",
    "### Amazon API Gateway\n",
    "[Amazon API Gateway official documentation](https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html)\n",
    "\n",
    "Amazon API Gateway est un service managé permettant de créer, publier, maintenir, monitorer et sécuriser différents types d'APIs *at scale* (capable de gérer des centaines de milliers de *calls*). L'avantage d'utiliser une API est principalement de contrôler les services et les données auxquelles ont accès les utilisateurs de l'API. En particulier et du point de vue de la sécurité, on n'expose pas directement les *endpoints* de notre application (mais seulement l'API). Un autre avantage est que les développeurs utilisant l'API n'ont pas à faire dépendre leur code des SDK AWS: ils ont juste à utiliser l'API. L'API fait aussi office d'interface: si on change les services ou la façon d'utiliser les services situés derrière l'API, les clients de l'API n'ont pas ou presque pas à modifier leur code. L'utilisation d'API contribuent à un meilleur découplage entre les applications serveur et clientes et autorisent une plus grande portabilité.\n",
    "\n",
    "Amazon API Gateway propose trois types d'APIs:\n",
    "* HTTP API qui est une RESTful API mais plus légère et à plus faible latence qu'une API REST.\n",
    "* REST API.\n",
    "* WebSocket API.\n",
    "\n",
    "### AWS Step Functions\n",
    "[AWS Step Functions official documentation](https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html)\n",
    "\n",
    "AWS Step Functions est un service *fully managed* d'orchestration *serverless* qui permet de combiner différents services AWS (notamment Lambda mais aussi SageMaker, Amazon EMR, Amazon ECS, etc.) pour constuire des applications dont on peut se représenter le flot d'exécution (*workflow*) comme une serie d'*event-driven steps*. \n",
    "\n",
    "Une application construite avec AWS Step Functions se base sur la notion de *state machine*. Un *workflow* est représenté par une *state machine*. A chaque *step* du *workflow* correspond un *state* de la *state machine*. Une *task* correspond à un *state* particulier dans lequel un autre service AWS réalise une unité de travail.\n",
    "\n",
    "Une des principales utilisations d'AWS Step Functions est l'ordonnancement de fonctions Lambda en *workflow*. Une *state machine* est décrite à l'aide d'un langage *JSON-based* dédié appelé l'Amazon State Language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 13: RTO/RPO and Backup Recovery Setup\n",
    "\n",
    "AWS et plus généralement les *clouds* permettent de ne pas avoir à gérer un double de son infrastructure (qui sera sous-utilisée) pour la *disaster recovery*.\n",
    "\n",
    "RPO = *Recovery Point Objective*: Il s'agit de la quantité de données perdues qu'on se donne comme acceptable exprimée en temps. Par exemple: 1h. Au RPO est corrélé la fréquence des *backups*.\n",
    "RTO = *Recovery Time Objective*: Il s'agit de la durée mesurée à partir du début de l'incident, qu'on se donne pour retrouver un niveau de service acceptable tel que défini dans un *Operational Level Agreement* (OLA). Dit autrement: c'est la durée maximale d'indisponibilité du service qu'on se fixe.\n",
    "\n",
    "AWS Backup est un service permettant de centraliser et d'automatiser les *data backups* des ressources de différents services comme les volumes EBS, instances EC2, bases RDS, tables DynamoDB, *filesystems* EFS, volumes Storage Gateway, etc.\n",
    "\n",
    "Il faut faire l'inventaire des service supportant une *data migration* et le *durable storage* ou plus généralement contribuant à la *disaster recovery*.\n",
    "\n",
    "Par exemple:\n",
    "* **Amazon S3** supporte la *cross-region replication* (CRR) qui permet de dupliquer de manière asynchrone et automatique des objets entre *buckets* situés dans des Regions différentes. On rappelle que Amazon S3 est *highly durable*. S3 Glacier ne supporte pas la CRR mais il est possible de faire de la CRR directement vers Glacier: un objet stocké dans S3 peut être dupliqué dans un *vault* Glacier situé dans une autre Region.\n",
    "* **Amazon EBS** permet de persister le contenu d'un volume EBS dans des (*point-in-time*) *snapshots* stockés sur S3. Une fois le *snapshot* réalisé, il peut être dupliqué vers une ou plusieurs autres Region via la CRR de S3.\n",
    "* **Amazon Route 53** offre des possibilités intéressantes pour la *disaster recovery* comme les DNS *endpoints health checks* associés à une capacité de *global load balancing*: en cas de désastre localisé, Route 53 est capable de rediriger le trafic vers d'autres *endpoints* potentiellement situés dans d'autres Regions ou un site statique hébergé sur S3.\n",
    "* Les **ELB** fournissent des capacités similaires de *health check* et de redirection automatique du trafic vers des entités saines.\n",
    "* **Amazon RDS** offre la possiblité de créer des *snapshots* des données vers d'autres Regions. De même, Amazon RDS offre la possibilité de créer des *read replicas* dans d'autres AZ voire d'autres Regions avec la possibilité de *promote* ces *read replicas* au rang de *master instance*.\n",
    "* **Amazon DynamoDB** offre la possibilité de dupliquer rapidement des tables vers d'autres Regions ou de persister les données d'une table vers S3. L'utilisation de Global Tables permet facilement d'avoir un accès aux données indépendant des Regions.\n",
    "* **AWS CloudFormation** permet via ses *templates* de redéployer rapidement et à l'identique une infrastructure complète.\n",
    "* **AWS Elastic Beanstalk** et **AWS OpsWorks** offre des possibilités de *rollback* rapidement ou de déployer de façon ordonnée un *patch* sur l'ensemble de l'infrastructure.\n",
    "\n",
    "On peut aussi penser à des solutions hybrides *on-premise*/*cloud* où les données sont persistées dans le *cloud* ce qui permet d'économiser un site de *backup offsite* et surtout qui permet un accès plus rapide aux données (via le réseau vs. données stockées sur bandes (*tape*) dans un autre site) en cas de *disaster recovery*. Les services non disponibles peuvent également être relancés côté *cloud*. C'est là que des services de migration (AWS Snowball) ou de réplication (AWS Storage Gateway) sont particulièrement appropriés. Il existe différents degrés de solution hybride allant de la simple réplication de données dans le cloud (la moins chère avec les RTO les plus élevés) à une solution tournant à la fois complètement *on-premise* et côté *cloud*, la charge étant répartie entre les deux (la plus chère car avec le plus de ressources utilisées côté cloud mais celle avec le RTO le plus faible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annexes\n",
    "\n",
    "### Amazon Redshift\n",
    "[Amazon Redshift official documentation](https://docs.aws.amazon.com/redshift/index.html)\n",
    "\n",
    "Amazon Redshift est un service *fully managed*, *highly available* et *scalable* de *data warehousing*. \n",
    "\n",
    "La principale ressource d'Amazon Redshift est le cluster Redshift. Un cluster Redshift consiste en une collection de ressources de calcul appelées noeuds (*nodes*). Un cluster héberge une ou plusieurs bases de données, celles-ci étant hébergées sur les *compute nodes*. Un cluster a une structure hiérarchique et consiste en un *leader node* et d'un ou plusieurs *compute nodes*. Le *leader node* reçoit les requête des clients SQL, les parse et élabore un *query execution plan*. Le *leader node* coordonne ensuite l'exécution parallèle de ce plan sur les *compute nodes* avant d'en retourner le résultat au client. Voir notamment [cette page](https://docs.aws.amazon.com/redshift/latest/dg/c_high_level_system_architecture.html) pour avoir une idée d'ensemble de l'architecture d'un cluster Redshift.\n",
    "\n",
    "Redshift est un *relational database management system* (RDBMS) basé sur PostgreSQL (bien qu'il existe un certain nombre de différences entre les deux systèmes dont il faut avoir conscience). Le service est donc compatible avec la plupart des applications RDMS/SQL ou avec un nombre limité de modifications.\n",
    "\n",
    "Remarques: \n",
    "* Un cluster Redshift consiste concrètement en une collection d'instances EC2 hébergées dans un VPC.\n",
    "* Si les données doivent être accédées via un *VPC endpoint* S3, alors il est nécessaire d'activer l'Enhanced VPC Routing côté Redshift.\n",
    "\n",
    "### Amazon Kinesis\n",
    "[Amazon Kinesis official documentation](https://docs.aws.amazon.com/kinesis/?id=docs_gateway)\n",
    "\n",
    "Amazon Kinesis est une plateforme proposant différents services dédiés au traitement de *streaming data*. Les deux principaux services d'Amazon Kinesis sont:\n",
    "* **Kinesis Data Streams**: Kinesis Data Streams est un service permettant de collecter dans un *data stream* des données produites par des centaines, voire des milliers de *producers*. Ces données peuvent ensuite être consommées par des data-processing applications typiquement hébergées sur des instnaces EC2 et se reposant sur un des *stream processing frameworks* supportés comme Kinesis Client Library (KCL), Apache Storm, and Apache Spark Streaming.\n",
    "* **Kinesis Data Firehose**: Kinesis Data Firehose est un service *fully managed* permettant de collecter et de charger des données produites en continu par des centaines, voire des milliers de *producers* dans des services comme Amazon S3, Redshift, Elasticsearch Service ou tout autre *endpoint* HTTP géré par un *third-party solution provider* comme Splunk, Datadog, MongoDB, etc.\n",
    "\n",
    "Remarque: Data Firehose est *fully managed* là où Data Streams n'est que partiellement managé et requiert davantage de configuration. Le *scaling* n'est pas exemple pas géré par le service dans le second, contrairement au premier.\n",
    "\n",
    "https://jayendrapatil.com/aws-kinesis-data-streams-vs-kinesis-firehose/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda Python 3.6.12",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
